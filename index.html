<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>  
	  
  	Redmagic
  	
	</title>

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	<link href="atom.xml" rel="alternate" title="Redmagic" type="application/atom+xml">

	<link href="asset/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link href="asset/stylesheets/font-awesome.min.css" media="screen, projection" rel="stylesheet" type="text/css">
	<script src="asset/javascripts/jquery.min.js"></script>
	<script src="asset/highlightjs/highlight.pack.js"></script>
	<link href="asset/highlightjs/styles/solarized_dark.css" media="screen, projection" rel="stylesheet" type="text/css">
<script>hljs.initHighlightingOnLoad();</script>

	<!--[if lt IE 9]><script src="asset/javascripts/html5.js"></script><![endif]-->
	<!-- <link href='http://fonts.googleapis.com/css?family=Nunito:400,300,700' rel='stylesheet' type='text/css'> -->
	<style type="text/css">
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 300;
  src: local('Nunito-Light'), url(asset/font/1TiHc9yag0wq3lDO9cw0voX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 400;
  src: local('Nunito-Regular'), url(asset/font/6TbRXKWJjpj6V2v_WyRbMX-_kf6ByYO6CLYdB4HQE-Y.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 700;
  src: local('Nunito-Bold'), url(asset/font/TttUCfJ272GBgSKaOaD7KoX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
	</style>
	
	<style type="text/css">
	.container .left-col{ opacity: 1;}
	#pagenavi a{ font-size: 1.3em;}
	#pagenavi .next:before{ top: 3px;}
	#pagenavi .prev:before{ top: 3px;}
	.container .mid-col .mid-col-container #content .archives .title{ font-size: 1.5em;}
	.container .mid-col .mid-col-container #content article{ padding: 15px 0px;}
	#header .subtitle {
		line-height: 1.2em;
		padding-top: 8px;
	}
	article pre{ background: none; border: none; padding: 0;}
	article .entry-content{text-align: left;}
	.share-comment{ padding: 25px 0px; clear: both;}
	hr{ margin: 20px 0px;border: 0; border-top:solid 1px #ddd;}
	</style>
  

</head>


<body>
	<div class="container">
		<div class="left-col">
			<div class="intrude-less">
				<header id="header" class="inner">
				 
					
					<h1><a href="index.html">Redmagic</a></h1>
					<p class="subtitle">FFFF</p>
					<nav id="main-nav">
						<ul class="main">
						
						  <li id=""><a target="self" href="index.html">Home</a></li>
						
						  <li id=""><a target="_self" href="archives.html">Archives</a></li>
						
						</ul>
					</nav>

					<nav id="sub-nav">
						<div class="social">













								

								<a class="rss" href="atom.xml" title="RSS">RSS</a>
							
						</div>
					</nav>
				</header>				
			</div>
		</div>	
		<div class="mid-col">
			<div class="mid-col-container"> <div id="content" class="inner">
<div itemscope itemtype="http://schema.org/Blog">


	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2018-11-10T10:00:52+08:00" itemprop="datePublished">2018/11/10</time>
			</div>
			
			 
			
		</div>
		<h1 class="title" itemprop="name"><a href="15418152524286.html" itemprop="url">
		大数据平台的数据同步服务实践</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<h2 id="toc_0">引言</h2>

<p>在大数据系统中，我们往往无法直接对在线系统中的数据直接进行检索和计算。在线系统所使用关系型数据库，缓存存储数据的方式都非常不同，很多系统不适合 OLAP 式的查询，也不允许 OLAP 查询影响到在线业务的稳定性。从数仓建设的角度思考，稳定规范的数仓必定依赖于稳定和规范的数据源，数据需要经过采集加工后才能真正被数仓所使用。推动数据同步服务的平台化，才有可能从源头规范数据的产出。数据同步服务不像数据挖掘一样可以直接产生价值，但它更像是连接不同存储的高速公路，好的同步工具可以很大程度上提升数据开发的效率。</p>

<p>本文主要介绍知乎在数据同步这方面的建设，工具选型和平台化的实践。</p>

<h2 id="toc_1">业务场景及架构</h2>

<p>在线业务的数据库在知乎内部还是以 MySQL 和 HBase 为主，所以数据源方面主要考虑 MySQL 和 Hive 的互相同步，后续可以支持 HBase。早期数据同步使用 Oozie + Sqoop 来完成，基本满足业务需求。但是随着数仓任务不断变多，出现了很多重复同步的例子，对同步任务的负载管理也是空白。凌晨同步数据导致 MySQL 不断报警，DBA 苦不堪言。对于业务来说，哪些表已经被同步了，哪些表还没有也是一个黑盒子，依赖其他业务方的数据都只能靠口头的约定。为了解决这些问题，决定对数据同步做一个统一的平台，简化同步任务的配置，调度平衡负载，管理元信息等等。</p>

<h2 id="toc_2">技术选型</h2>

<p>数据同步工具市面上有很多解决方案，面向批的主要有 <a href="http://sqoop.apache.org/">Apache Sqoop</a> 和阿里开源的 <a href="https://github.com/alibaba/DataX">DataX</a>，下面主要对比这两种数据同步工具。</p>

<h5 id="toc_3">Sqoop</h5>

<p>Pros：</p>

<ul>
<li>基于 MapReduce 实现，容易并行和利用现有集群的计算资源</li>
<li>和 Hive 兼容性好，支持 Parquet，ORC 等格式</li>
<li>支持自动迁移 Schema</li>
<li>社区强大，遇到的问题容易解决</li>
</ul>

<p>Cons：</p>

<ul>
<li>支持的数据源不算太丰富（比如 ES），扩展难度大</li>
<li>不支持限速，容易对 MySQL 造成压力</li>
</ul>

<h5 id="toc_4">DataX</h5>

<p>Pros：</p>

<ul>
<li>支持的数据源丰富尤其是支持从非关系型数据库到关系型数据库的同步</li>
<li>支持限速</li>
<li>扩展方便，插件开发难度低</li>
</ul>

<p>Cons：</p>

<ul>
<li>需要额外的运行资源，当任务比较多的时候费机器</li>
<li>没有原生支持导出到 Hive，需要做很多额外的工作才能满足需求</li>
</ul>

<p>考虑到同步本身要消耗不少的计算和带宽资源，Sqoop 可以更好的利用 Hadoop 集群的资源，而且和 Hive 适配的更好，最终选择了 Sqoop 作为数据同步的工具。</p>

<h2 id="toc_5">平台化及实践</h2>

<p>平台化的目标是构建一个相对通用的数据同步平台，更好的支持新业务的接入，和公司内部的系统集成，满足业务需求。平台初期设计的目标有以下几个：</p>

<ul>
<li>简单的任务配置界面，方便新的任务接入</li>
<li>监控和报警</li>
<li>屏蔽 MySQL DDL 造成的影响</li>
<li>可扩展新数据源</li>
</ul>

<h3 id="toc_6">简化任务接入</h3>

<p>平台不应该要求每个用户都理解底层数据同步的原理，对用户而言，应该是描述数据源 (Source) 和目标存储(Sink)，还有同步周期等配置。所有提供的同步任务应该经过审核，防止未经许可的数据被同步，或者同步配置不合理，增加平台负担。最后暴露给用户的 UI 大概如下图。</p>

<p><img src="media/15418152524286/15421169494010.jpg" alt="15421169494010" style="width:600px;"/></p>

<h3 id="toc_7">增量同步</h3>

<p>对于数据量非常大的数据源，如果每次同步都是全量，对于 MySQL 的压力会特别大，同步需要的时间也会很长。因此需要一种可以每次只同步新增数据的机制，减少对于 MySQL 端的压力。但是增量同步不是没有代价的，它要求业务在设计表结构的时候，满足一些约束：</p>

<ul>
<li>业务对数据没有物理的删除操作，而是采用类似标记删除的机制</li>
<li>数据没有 UPDATE （类似日志） 或者有 UPDATE 但是提供 updated_at 来标记每一行最后一次更新的时间</li>
</ul>

<p>对于满足上面条件，数据量比较大的表就可以采用增量同步的方式拉取。小数据量的表不需要考虑增量同步，因为数据和合并也需要时间，如果收益不大就不应该引入额外的复杂性。一个经验值是行数 &lt;= 2000w 的都属于数据量比较小的表，具体还取决于存储的数据内容（比如有很多 Text 类型的字段）。</p>

<h3 id="toc_8">处理 Schema 变更</h3>

<p>做数据同步永远回避不掉的一个问题就是 Schema 的变更，对 MySQL 来说，Schema 变更就是数据库的 DDL 操作。数据同步平台应该尽可能屏蔽 MySQL DDL 对同步任务的影响，并且对兼容的变更，及时变更推送到目标存储。</p>

<p>数据同步平台会定时的扫描每个同步任务上游的数据源，保存当前 Schema 的快照，如果发现 Schema 发生变化，就通知下游做出一样的变更。绝大部分的 DDL 还是增加字段，对于这种情况数据同步平台可以很好屏蔽变更对数仓的影响。对于删除字段的操作原则上禁止的，如果一定要做，需要走变更流程，通知到依赖该表的业务方，进行 Schema 同步的调整。</p>

<h3 id="toc_9">存储格式</h3>

<p>Hive 默认的格式是 Textfile，这是一种类似 CSV 的存储方式，但是对于 OLAP 查询来说性能并不友好。通常我们会选择一些列式存储提高存储和检索的效率。Hive 中比较成熟的列式存储格式有 Parquet 和 ORC。这两个存储的查询性能相差不大，但是 ORC 和 Hive 集成更好而且对于非嵌套数据结构查询性能是优于 Parquet 的。但是知乎内部因为也用了 Impala，早期的 Impala 版本不支持 ORC 格式的文件，为了兼容 Impala 最终选择了 Parquet 作为默认的存储格式。</p>

<p>关于列式存储的原理和 Benchmark，可以参考这个 <a href="https://www.slideshare.net/oom65/file-format-benchmarks-avro-json-orc-parquet">Slide</a></p>

<h3 id="toc_10">负载管理</h3>

<p>当同步任务越来越多时，单纯的按照任务启动时间来触发同步任务已经不能满足需求。数据同步应该保证对于线上业务没有影响，在此基础上速度越快越好。本质上是让 Sqoop 充分利用 MySQL 节点的 iops。要避免对线上服务的影响，对于需要数据同步的库单独建立一个从节点，隔离线上流量。初次之外，需要一个调度策略来决定一个任务何时执行。由于任务的总数量并不多，但是每个任务可能会执行非常长的时间，最终决定采用一个中央式的调度器，调度器的状态都持久化在数据库中，方便重启或者故障恢复。最终架构图如下</p>

<p><img src="media/15418152524286/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E8%B0%83%E5%BA%A6%E5%99%A8.jpg" alt="数据同步调度器" style="width:336px;"/></p>

<p>最终任务的调度流程如下：</p>

<ol>
<li>每个 MySQL 实例是调度器的一个队列，根据同步的元信息决定该任务属于哪个队列</li>
<li>根据要同步数据量预估资源消耗，向调度器申请资源</li>
<li>调度器将任务提交到执行队列，没有意外的话会立刻开始执行</li>
<li>Monitor 定时向调度器汇报 MySQL 节点的负载，如果负载过高就停止向该队列提交新的任务</li>
<li>任务结束后从调度器归还资源</li>
</ol>

<h3 id="toc_11">性能优化</h3>

<h5 id="toc_12">针对不同的数据源选择合适的并发数</h5>

<p>Sqoop 是基于 MapReduce 实现的，提交任务前先会生成 MapReduce 代码，然后提交到 Hadoop 集群。Job 整体的并发度就取决于 Mapper 的个数。Sqoop 默认的并发数是 4，对于数据量比较大的表的同步显然是不够的，对于数据量比较小的任务又太多了，这个参数一定要在运行时根据数据源的元信息去动态决定。</p>

<h5 id="toc_13">优化 <a href="https://community.hortonworks.com/questions/79556/what-is-distributed-cache-in-hadoop.html">Distributed Cache</a> 避免任务启动对 HDFS 的压力</h5>

<p>在平台上线后，随着任务越来越多，发现如果 HDFS 的性能出现抖动，对同步任务整体的执行时间影响非常大，导致夜间的很多后继任务受到影响。开始推测是数据写入 HDFS 性能慢导致同步出现延时，但是任务大多数会卡在提交阶段。随着进一步排查，发现 MapReduce 为了解决不同作业依赖问题，引入了 Distributed Cache 机制可以将 Job 依赖的 lib 上传到 HDFS，然后再启动作业。Sqoop 也使用了类似的机制，会依赖 Hive 的相关 lib，这些依赖加起来有好几十个文件，总大小接近 150MB，虽然对于 HDFS 来说是很小数字，但是当同步任务非常多的时候，集群一点点的性能抖动都会导致调度器的吞吐大幅度下降，最终同步的产出会有严重延时。最后的解决方法是将 Sqoop 安装到集群中，然后通过 Sqoop 的参数 <code>--skip-distcache</code> 避免在任务提交阶段上传依赖的 jar。</p>

<h5 id="toc_14">关闭推测执行（Speculative Execution）</h5>

<p>所谓推测执行是这样一种机制：在集群环境下运行 MapReduce，一个 job 下的多个 task 执行速度不一致，比如有的任务已经完成，但是有些任务可能只跑了10%，这些任务将会成为整个 job 的短板。推测执行会对运行慢的 task 启动备份任务，然后以先运行完成的 task 的结果为准，kill 掉另外一个 task。这个策略可以提升 job 的稳定性，在一些极端情况下加快 job 的执行速度。</p>

<p>Sqoop 默认的分片策略是按照数据库的主键和 Mapper 数量来决定每个分片拉取的数据量。如果主键不是单调递增或者递增的步长有大幅波动，分片就会出现数据倾斜。对于一个数据量较大的表来说，适度的数据倾斜是一定会存在的情况，当 Mapper 结束时间不均而触发推测执行机制时，MySQL 的数据被重复且并发的读取，占用了大量 io 资源，也会影响到其他同步的任务。在一个 Hadoop 集群中，我们仍然认为一个节点不可用导致整个 MapReduce 失败仍然是小概率事件，对这种错误，在调度器上增加重试就可以很好的解决问题而不是依赖推测执行机制。</p>

<h3 id="toc_15">监控和报警</h3>

<p>根据 <a href="http://ylzheng.com/2018/02/02/monitor-best-praticase4-golden-signals/"><strong>USE</strong></a> 原则，大概整理出下面几个需要监控的指标：</p>

<ul>
<li>MySQL 机器的负载，IOPS，出入带宽</li>
<li>调度队列长度，Yarn 提交队列长度</li>
<li>任务执行错误数</li>
</ul>

<p>报警更多是针对队列饱和度和同步错误进行的</p>

<h3 id="toc_16">和离线作业调度器集成</h3>

<h2 id="toc_17">展望</h2>

<p>数据同步发展到比较多的任务后，新增的同步任务越来越多，删除的速度远远跟不上新增的速度，总体来说同步的压力会越来越大，需要一个更好的机制去发现无用的同步任务并通知业务删除，减轻平台的压力。</p>

<p>另外就是数据源的支持不够，Hive 和 HBase、ElasticSearch 互通已经成了一个呼声很强烈的需求。Hive 虽然可以通过挂外部表用 SQL 的方式写入数据，但是效率不高有很难控制并发，很容易影响到线上集群，需要有更好的实现方案才能在生产环境真正的运行起来。</p>

<p>另外这里没有谈到的一个话题就是流式数据如何做同步，一个典型的场景就是 Kafka 的日志实时落地然后实时进行 OLAP 的查询，或者通过 MySQL binlog 实时更新 ElasticSearch 的索引。关于这块的基础设置知乎也在快速建设中，非常欢迎感兴趣同学投简历到 <a href="mailto:ck@zhihu.com">ck@zhihu.com</a> ，加入知乎大数据计算平台组。</p>

<h2 id="toc_18">参考资料</h2>

<ul>
<li><a href="https://chu888chu888.gitbooks.io/hadoopstudy/content/Content/9/datax/datax.html">Datax 性能对比</a></li>
<li><a href="https://blog.csdn.net/zhaodedong/article/details/54177686">漫谈数据仓库之拉链表</a></li>
<li><a href="https://orc.apache.org/">Apache ORC</a></li>
<li><a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html">Apache Sqoop</a></li>
</ul>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2018-07-05T21:00:20+08:00" itemprop="datePublished">2018/7/5</time>
			</div>
			
			 
			
		</div>
		<h1 class="title" itemprop="name"><a href="15307956203332.html" itemprop="url">
		Sqoop 使用指南</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p>Sqoop 是一个数据同步工具，用于关系型数据库和各种大数据存储比如 Hive 之间的数据相互同步。Sqoop 因为它的使用便利得到了广泛使用。类似的工具还有阿里开源的 <a href="https://github.com/alibaba/DataX">DataX</a> 和其他商业工具。</p>

<p><a href="http://sqoop.apache.org/docs/1.99.7/index.html">Sqoop 2.0</a> 主要解决 Sqoop 1.x 扩展难的问题，提出的 Server-Client 模型，具体用的不是特别多。本文主要介绍的还是 Sqoop 1.x，最新的 Sqoop 版本是 1.4.7</p>

<h3 id="toc_0">安装</h3>

<p>Sqoop 安装需要依赖 Hadoop 和 Hive，以 Debain 为例，安装 Sqoop 也比较简单。</p>

<pre><code class="language-bash">apt-get install hadoop hive hive-hbase hive-hcatalog sqoop
</code></pre>

<p>除此之外，针对不同的数据源，需要不同的 JDBC Driver，这个是 Sqoop 默认没有自带的库，需要自行安装。比如 MySQL 的 Driver 是 <code>mysql-connector-java-5.1.13-bin.jar</code> ，确保 Jar 包在 Sqoop 的 classpath 内就行。</p>

<h3 id="toc_1">数据源</h3>

<p>Sqoop 支持非常多的数据源，理论上所有支持 JDBC 的数据源都可以作为 Sqoop 的数据源。最常见的场景还是从关系型数据（RDBMS）导入到 Hive, HBase 或者 HDFS。</p>

<p>Sqoop 的扩展性没有想象中的那么好，但是因为大部分企业的数据仓库还是构建在传统的 Hive 和 HBase 之上的，Sqoop 还是可以满足 80% 的数据同步需求的。</p>

<p>一个简单以 MySQL 作为上游数据源的同步：</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
  --username dbuser --password &quot;&quot; 
</code></pre>

<p>Sqoop 支持将数据同步到 HDFS 或者直接到 Hive：</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
  --username dbuser --password &quot;&quot; --table employees \
  --hive-import --hive-overwrite \
  --hive-database employees --hive-table employees
</code></pre>

<h3 id="toc_2">存储格式</h3>

<p>存储格式主要是 Hive 的概念，但是对于数据同步来讲，格式的选择会影响同步数据，类型系统的兼容性等等，我们必须予以关注。参考下面的表格：</p>

<table>
<thead>
<tr>
<th></th>
<th>压缩比</th>
<th>预计算</th>
<th>类型兼容性</th>
</tr>
</thead>

<tbody>
<tr>
<td>TextFile</td>
<td>无</td>
<td>否</td>
<td>一般</td>
</tr>
<tr>
<td>SequenceFile</td>
<td>中</td>
<td>否</td>
<td>一般</td>
</tr>
<tr>
<td>Parquet</td>
<td>高</td>
<td>是（sqoop 依赖的版本 feature 不完整）</td>
<td>好</td>
</tr>
<tr>
<td>ORC</td>
<td>高</td>
<td>是</td>
<td>好</td>
</tr>
</tbody>
</table>

<p>Hive 默认的存储格式是 TextFile，TextFile 类似一个 CSV 文件，使用不可见服务分割列，同步后的数据可读性比较好。但是因为所有数据都是按文本存储的，对于某些类型（比如 blob/bit ）无法支持。</p>

<p>Parquet/ORC 都是列式存储格式，这里不多介绍。在生产环境中更倾向于选择 Parquet/ORC ，节省空间的同时在 Hive 上的查询速度也更快。</p>

<p>同步为 Parquet 格式：</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
 --username dbuser --password &quot;&quot; --table employees \
 --hive-import --hive-overwrite \
 --hive-database employees --hive-table employees \
 --as-parquetfile
</code></pre>

<p>如果要导出为 ORC 格式，需要借助 Hive 提供的一个组件 HCatalog，同步语法也稍稍不太一样</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
 --username dbuser --password &quot;&quot; --table employees \
 --drop-and-create-hcatalog-table \
 --hcatalog-database employees --hcatalog-table employees \
 --hcatalog-storage-stanza &quot;STORED AS ORC&quot;
</code></pre>

<p>Parquet 理论上也可以通过这种方式同步，不过实测当前 Sqoop 版本 (1.4.7) 还是有 BUG，还是等等吧。</p>

<h3 id="toc_3">类型的兼容性</h3>

<p>由于数据源支持的类型和 Hive 本身可能不太一样，所以必然存在类型转换的问题。实际在使用过程中也是非常头疼的一件事。对于 Hive 来说，支持的类型取决于采用的存储格式。以 MySQL 为例，当存储格式为 Hive 时，基本的类型映射如下：</p>

<pre><code>MySQL(bigint) --&gt; Hive(bigint) 
MySQL(tinyint) --&gt; Hive(tinyint) 
MySQL(int) --&gt; Hive(int) 
MySQL(double) --&gt; Hive(double) 
MySQL(bit) --&gt; Hive(boolean) 
MySQL(varchar) --&gt; Hive(string) 
MySQL(decimal) --&gt; Hive(double) 
MySQL(date/timestamp) --&gt; Hive(string)
</code></pre>

<p>这里的类型映射并不完全准确，因为还取决于目标存储格式支持的类型。</p>

<p>由于 Text 格式非常类似 CSV，使用文本存储所有数据，对于 <code>Binary/Blob</code> 这样的类型就无法支持。Parquet/ORC/Avro 因为引入了序列化协议，本身存储是基于二进制的，所以可以支持绝大部分类型。</p>

<p>如果你在使用 TextFile 需要注意下面的问题：</p>

<ul>
<li>上游数据源中的 <code>NULL</code> 会被转化为字符串的 <code>NULL</code>, Hive 中的 <code>NULL</code> 用 <code>\N</code> 表示</li>
<li>如果内容中含有换行符，同步到 Hive 中会被当做独立的两行来处理，造成查询结果和实际数据不相符</li>
</ul>

<p>处理方法比较简单</p>

<p>如果在使用 Parquet，要注意 Sqoop 自带的 Parquet 库版本比较旧，不支持 DateTime/Timestamp 类型的数据，而是会用一个表示 ms 的 BIGINT 来代替，分析数据的时候应该注意这点。</p>

<h3 id="toc_4">数据校验</h3>

<p>Sqoop 内建有 validate 机制，只能验证单表的 row count: <a href="https://sqoop.apache.org/docs/1.4.3/SqoopUserGuide.html#validation">Sqoop User Guide (v1.4.3)</a></p>

<h3 id="toc_5">增量导入</h3>

<p>对于数据量很大的库，全量同步会非常痛，但是如果可以选择还是尽可能的选择全量同步，这种同步模式对数据一致性的保证最好，没有状态。如果不得不进行增量同步，可以继续往后看。</p>

<p>增量导入对业务是有一定侵入的，Schema 的设计和数据写入模式需要遵守一定的规范：</p>

<ul>
<li>增量同步表，最好有一个 Primary Key ，最好是单调递增的 ID</li>
<li>数据的写入模式满足下面两种情形之一

<ul>
<li>（Append）表的内容类似日志，一次写入不做修改和删除</li>
<li>（LastModified）表的内容有修改和删除，但是删除操作是逻辑删除，比如用 <code>is_deleted</code> 字段标识，并且有一个最后更新的时间戳比如 <code>updated_at</code>，<code>updated_at</code> 上有索引。</li>
</ul></li>
</ul>

<p>增量的数据同步大致分为 2 个阶段：读取增量数据和合并数据。对 Sqoop 来说，增量同步需要 <a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_literal_sqoop_metastore_literal">sqoop-metastore</a> 的支持，用于保存上次同步的位置。</p>

<p>比如对于 Append 模式，假设我们有一张表叫 <code>employees</code>，Primary Key 是 <code>id</code>，上一次同步到 <code>id &lt;= 10000</code> 的数据：</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
  --username dbuser --password &quot;&quot; --table employees \
  --target-dir &lt;path/to/hive/table/location&gt; \
  --incremental append --check-column id --last-value 10000
</code></pre>

<p>我们直接将数据 load 到了 Hive 的表空间里，Hive 可以直接查询到最新增量的数据。<br/>
对 LastModified 模式会稍微复杂一些，除了加载增量数据，还涉及数据合并的问题，这里唯一的主键就特别重要了。</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
  --username dbuser --password &quot;&quot; --table employees \
  --target-dir &lt;path/to/hive/table/location&gt; \
  --incremental lastmodified --check-column updated_at --last-value &#39;2018-07-05 00:00:00&#39;
</code></pre>

<p>Sqoop 会在同步结束后再启动一个 merge 任务对数据去重，如果表太小，可能 merge 的代价比全量同步的还要高，我们就要慎重考虑全量同步是不是值得了。</p>

<blockquote>
<p>由于 HDFS 不支持修改文件，sqoop 的 <code>--incremental</code> 和 <code>--hive-import</code> 不能同时使用</p>
</blockquote>

<p>Sqoop 也提供了单独的 <code>sqoop merge</code> 工具，我们也可以分开进行 import 和 merge 这两个步骤。</p>

<h3 id="toc_6">加速同步</h3>

<p>这个小节讨论一下如何加快 Sqoop 的同步速度，Sqoop 同步速度大致取决于下面的几个因素：</p>

<ul>
<li>数据源的读取速度</li>
<li>HDFS 写入速度</li>
<li>数据倾斜程度</li>
</ul>

<h5 id="toc_7">数据源的读取速度</h5>

<p>如果上游数据源是 MySQL，可以考虑更换 SSD，保证 MySQL 实例的负载不要太高。除此之外，Sqoop 可以通过参数控制并发读取的 Mapper 个数加快读取速度。</p>

<pre><code class="language-bash">sqoop -m &lt;mapper_num&gt; ......
</code></pre>

<p>注意 <code>-m</code> 并不是越大越高，并发数过高会把数据库实例打死，同步速度反而变慢。</p>

<p>Sqoop 默认会通过 jdbc 的 API 来读取数据，但是可以通过参数控制使用 MySQL 自己的 <code>mysqldump</code> 来导出数据，这种方式比 jdbc 快一些，缺点是你不能选择要同步的列。另外只能支持目标格式为 Textfile。比较局限但是特定情况下还是很好使的。</p>

<h5 id="toc_8">HDFS 写入速度</h5>

<p>这个除了刚刚提供的控制并发数，还需要保证 Yarn 分配给 Sqoop 的资源充足，不要让资源成为同步的瓶颈。另外，当我们选择 Parquet/ORC 作为存储格式时，数据在写入的时候需要做大量的预计算，这个过程是比较消耗 CPU 和内存的，我们可以控制 MapReduce 参数，适当提高 Sqoop 的资源配额。</p>

<pre><code class="language-bash">sqoop -Dmapreduce.map.cpu.vcores=4 -Dmapreduce.map.memory.mb=8192 ...
</code></pre>

<h5 id="toc_9">数据倾斜</h5>

<p>Sqoop 默认的导入策略是根据主键进行分区导入的，具体的并发粒度取决于 <code>-m</code> 参数。如果主键不连续出现大幅度跳跃，就会导致 Sqoop 导入的时候出现严重的数据倾斜。比如某张表的主键分布是这样的：</p>

<pre><code>1
2
3
...
1000
1001
100000
100001
</code></pre>

<p>Sqoop 计算每个 Mapper 读取的数据范围的时候，会遵循很简单的公式计算：</p>

<pre><code>range = (max(pk) - min(pk)) / mapper
</code></pre>

<p>几乎出现所有的数据 load 都集中在第一个 mapper 上，整体同步相当于没有并发。</p>

<p>参考阅读：</p>

<ul>
<li><a href="https://blog.csdn.net/mike_h/article/details/50148309">Hive 数据倾斜 (Data Skew) 总结 - CSDN博客</a></li>
<li><a href="http://abhinaysandeboina.blogspot.hk/2017/08/avoiding-data-skew-in-sqoop.html">Avoiding Data Skew in Sqoop</a></li>
<li><a href="https://docs.qingcloud.com/guide/sqoop.html">Sqoop 指南 — QingCloud  文档</a></li>
</ul>

<h3 id="toc_10">导出</h3>

<p>Sqoop 支持将 Hive 的数据导出到 MySQL，方便在线系统调用。</p>

<pre><code>sqoop export --connect jdbc:mysql://database.example.com/employees --table employees --username dbuser --password &quot;&quot; --relaxed-isolation --update-key id --update-mode allowinsert --hcatalog-database employees --hcatalog-table employees
</code></pre>

<p>借助 HCatalog 可以比较轻松的将 Hive 表的数据直接导出到 MySQL。更多的详情参考官方文档，这里不多介绍。</p>

<h3 id="toc_11">更进一步</h3>

<p>如果我们要同步的数据非常多，管理同步任务本身就变成了一件复杂的事情。我们不仅要考虑源数据库的负载，安全性。还要考虑同步任务的启动时间，Schema 变更等等问题。实际使用的时候，我们在内部自研了一个平台，管理 MySQL 和 Hive 的数据源并对 Sqoop 任务做了调度。有一部分功能在 Sqoop 2.0 已经实现了。在大规模使用 sqoop 一定要想清楚运维的问题。</p>

<h3 id="toc_12">Reference</h3>

<ul>
<li><a href="https://stackoverflow.com/questions/24987820/not-able-to-run-sqoop-using-oozie">hadoop - Not able to run sqoop using oozie - Stack Overflow</a></li>
<li><a href="https://stackoverflow.com/questions/23250977/how-to-deal-with-sqoop-import-delimiter-issues-r-n">mysql - how to deal with sqoop import delimiter issues \r\n - Stack Overflow</a></li>
<li><a href="https://www.zybuluo.com/aitanjupt/note/209968">使用Sqoop从MySQL导入数据到Hive和HBase 及近期感悟 - 作业部落 Cmd Markdown 编辑阅读器</a></li>
<li><a href="https://community.hortonworks.com/questions/28060/can-sqoop-be-used-to-directly-import-data-into-an.html">Can sqoop be used to directly import data into an ORC table? - Hortonworks</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">LanguageManual ORC - Apache Hive - Apache Software Foundation</a></li>
<li>Hive 增量同步： <a href="https://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/">Four-step Strategy for Incremental Updates in Apache Hive</a></li>
<li>使用 MERGE INTO 更新 Hive 数据： <a href="https://hortonworks.com/blog/update-hive-tables-easy-way/">Update Hive Tables the Easy Way - Hortonworks</a></li>
<li>SQL MERGE 的性能： <a href="https://hortonworks.com/blog/apache-hive-moving-beyond-analytics-offload-with-sql-merge/">Apache Hive: Moving Beyond Analytics Offload with SQL MERGE - Hortonworks</a></li>
<li><a href="https://community.hortonworks.com/questions/11373/sqoop-incremental-import-in-hive-i-get-error-messa.html">sqoop incremental import in hive I get error message hive not support append mode how to solve that - Hortonworks</a></li>
<li><a href="http://www.hadooptechs.com/sqoop/sqoop-incremental-import-mysql-to-hive">Sqoop Incremental Import | MySQL to Hive | Big Data &amp; Hadoop</a></li>
<li><a href="https://ask.hellobi.com/blog/marsj/4114">Sqoop 1.4.6 导入实战 (RDB含MySQL和Oracle) - 天善智能：专注于商业智能BI和数据分析、大数据领域的垂直社区平台</a></li>
</ul>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2018-02-02T15:40:38+08:00" itemprop="datePublished">2018/2/2</time>
			</div>
			
			 
			
		</div>
		<h1 class="title" itemprop="name"><a href="15175572389238.html" itemprop="url">
		使用 BigTop 打包 Hadoop 全家桶</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p>使用 Hadoop 软件好像难免会自己改下代码做些定制，或者在部分组件的版本选择上激进，其他的版本( 比如 HDFS)  上保守。最近在公司升级 Hive 到 2.1.1 ，也对代码做了一些调整确保对业务的兼容性，之前公司使用的是 <code>hive-1.2.2-cdh-5.5.0</code> 。cdh 的发布节奏太慢跟不上社区的节奏，而且截止到现在，社区版本的 BUG 数量和稳定性都可以接受而不是必须选择商业公司给我们提供的发行版。</p>

<p>公司用的服务器是 Debian 7/8，为了方便的把定制过的 Hive 部署到服务器，需要将 Hive 打包成 deb，一直没找到特别好的打包方法。要做到 Cloudera 那样规范的 deb 非常繁琐，要处理启动脚本，环境变量，配置文件的 alternatives 等等。顺着这个思路找到了 Cloudera 官方的打包工具 <a href="https://github.com/cloudera/cdh-package">cdh-package</a> ，但是这个库已经太长时间没有维护了，里面依赖的版本信息非常老旧，而且自己测试也没运行成功。但是 cdh-package 是基于 <a href="https://github.com/apache/bigtop">BigTop</a> 的，BigTop 本身的维护还不错。</p>

<p>Bigtop 非常有野心，它的主要目标就是构建一个 Apache Hadoop 生态系统的包和交互式测试的社区。这个包括对各类不同级别工程进行测试(包，平台，运行时间，升级等...)，它由社区以关注系统作为一个整体开发而来。BigTop 官方除了介绍怎么安装之外没有任何使用文档，不过研究以后发现还算简单，不需要太多的说明。</p>

<h3 id="toc_0">准备 BigTop 环境</h3>

<p>可以根据官方的说明来安装，我这里是直接从 Github 拉了代码：</p>

<pre><code class="language-bash">git clone https://github.com/apache/bigtop.git
cd bigtop
git checkout release-1.2.1
./gradlew
</code></pre>

<p>然后我们可以运行 <code>./gradlew tasks</code> 看下 BigTop 给我们提供的命令，命令遵循下面的格式 <code>./gradlew &lt;package&gt;-&lt;dist&gt;</code> ：</p>

<pre><code class="language-bash">$ ./gradlew tasks
# hide some output
hive-clean - Removing hive component build and output directories
hive-deb - Building DEB for hive artifacts
hive-download - Download hive artifacts
hive-help - List of available tasks for hive
hive-info - Info about hive component build
hive-pkg - Invoking a native binary packaging target deb
hive-relnotes - Preparing release notes for hive. No yet implemented!!!
hive-rpm - Building RPM for hive artifacts
hive-sdeb - Building SDEB for hive artifacts
hive-spkg - Invoking a native binary packaging target sdeb
hive-srpm - Building SRPM for hive artifacts
hive-tar - Preparing a tarball for hive artifacts
hive-version - Show version of hive component
</code></pre>

<p>然后编辑 <code>bigtop.bom</code> 将依赖的版本改成自己需要的版本，注意 BigTop 这里会优先使用 bigtop.bom 中定义的版本号覆盖源代码的版本号。</p>

<pre><code class="language-json">&#39;hive&#39; {
      name    = &#39;hive&#39;
      relNotes = &#39;Apache Hive&#39;
      version { base = &#39;1.2.1&#39;; pkg = base; release = 1 }
      tarball { destination = &quot;apache-${name}-${version.base}-src.tar.gz&quot;
                source      = destination }
      url     { download_path = &quot;/$name/$name-${version.base}/&quot;
                site = &quot;${apache.APACHE_MIRROR}/${download_path}&quot;
                archive = &quot;${apache.APACHE_ARCHIVE}/${download_path}&quot; }
    }
</code></pre>

<p>下面将介绍如何用 BigTop 打包 Hive</p>

<h3 id="toc_1">用 BigTop 打包 Hive</h3>

<p>我们的目标是将一份修改过的 Hive 代码打包成 deb 包分发到集群，首先在编辑机器上准备一些必要的依赖：</p>

<pre><code class="language-bash">sudo apt-get update
sudo apt-get install devscripts
sudo apt-get install dh-make
</code></pre>

<p>接下来准备 Hive 的代码，Bigtop 默认根据 bom 文件里指定的版本号从上游下载 Hive 的代码，解压然后编译。但是由于我们要使用自己修改过的版本，可以修改 <code>bigtop.bom</code> 从内部 git 仓库下载代码。</p>

<pre><code class="language-grovvy">  &#39;hive&#39; {
      name    = &#39;hive&#39;
      relNotes = &#39;Apache Hive&#39;
      version { base = &#39;2.1.1&#39;; pkg = base; release = 1 }
      tarball { destination = &quot;apache-${name}-${version.base}-src.tar.gz&quot;
                source      = destination }
      git     { repo = &quot;https://exmaple.com:hive/hive&quot;
                ref = &quot;release-2.1.1&quot;
                dir  = &quot;${name}-${version.base}&quot; }
    }
</code></pre>

<p>然后就可以开始打包了：</p>

<pre><code class="language-bash">./gradlew hive-deb
</code></pre>

<p>注意 BigTop 在它的仓库里包含了对 Hive 的几个 patch 文件，我这边测试的时候会导致编译失败，建议删除：</p>

<pre><code class="language-bash">rm -f /bigtop-packages/src/common/hive/*
</code></pre>

<p>然后清理构建环境，重新打包：</p>

<pre><code class="language-bash">./gradlew hive-clean
./gradlew hive-deb
</code></pre>

<p>如果需要更新包，可以提升 Release number，默认是 1 ，这个 BigTop 在文档里没有提及：</p>

<pre><code class="language-bash">BIGTOP_BUILD_STAMP=&lt;release&gt; ./gradlew hive-deb
</code></pre>

<h3 id="toc_2">发布 deb 包</h3>

<p>看看我们构建的结果，产生了很多 deb 包，这些包都需要上传到内部的 mirror</p>

<pre><code>$ ls -l output/hive/
total 138516
-rw-r--r-- 1 ck ck 78645700 Feb  1 18:32 hive_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck   314946 Feb  1 18:33 hive_2.1.1-1_amd64.build
-rw-r--r-- 1 ck ck     3461 Feb  1 18:32 hive_2.1.1-1_amd64.changes
-rw-r--r-- 1 ck ck    12500 Feb  1 18:18 hive_2.1.1-1.debian.tar.xz
-rw-r--r-- 1 ck ck     1227 Feb  1 18:18 hive_2.1.1-1.dsc
-rw-r--r-- 1 ck ck     1829 Feb  1 18:18 hive_2.1.1-1_source.changes
-rw-r--r-- 1 ck ck 20999949 Feb  1 18:18 hive_2.1.1.orig.tar.gz
-rw-r--r-- 1 ck ck   107906 Feb  1 18:32 hive-hbase_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck   452862 Feb  1 18:32 hive-hcatalog_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck     3632 Feb  1 18:32 hive-hcatalog-server_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck 39029552 Feb  1 18:32 hive-jdbc_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck     3734 Feb  1 18:32 hive-metastore_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck     3738 Feb  1 18:32 hive-server2_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck  2068240 Feb  1 18:32 hive-webhcat_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck     3608 Feb  1 18:32 hive-webhcat-server_2.1.1-1_all.deb
</code></pre>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2017-11-13T22:53:59+08:00" itemprop="datePublished">2017/11/13</time>
			</div>
			
			 
			
		</div>
		<h1 class="title" itemprop="name"><a href="15105848393062.html" itemprop="url">
		Hive 论文笔记</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p>MapReduce 出现后，对数据的计算需求越来越多，而 MapReduce 提供的 API 太底层，学习成本和开发成本比较高，因此需要一个类 SQL 的工具，来代替大部分的 MapReduce 的使用场景。</p>

<h3 id="toc_0">数据模型，类型系统和查询语言</h3>

<p>Hive 和传统的数据库一样有 Database 和 Table 的概念，数据存储在 Table 中。每个 Table 中会会很多行，每行有多列组成。</p>

<h5 id="toc_1">类型</h5>

<p>Hive 支持原生类型 (primitive types) 和复杂类型 (complex types)。</p>

<p>Primitive:</p>

<ul>
<li>Integers: bigint, int, smallint, tinyint</li>
<li>Float: float, double</li>
<li>String</li>
</ul>

<p>Complex:</p>

<ul>
<li>map<key-type, value-type></li>
<li>list<element-type></li>
<li>struct<field-name, field-type, …></li>
</ul>

<p>SerDe , Format 都是可插拔的，用户可以自定义 SerDe 或者 Format，在查询时可以通过 HiveQL 增加自己的 SerDe:</p>

<pre><code>ADD JAR /jars/myformat.jar;
CREATE TABLE t2
ROW FORMAT SERDE &#39;com.myformat.MySerDe&#39;;
</code></pre>

<h5 id="toc_2">HiveQL</h5>

<p>HiveQL 和传统的 SQL 几乎没有差别，但是存在一些局限：</p>

<ul>
<li>只能使用标准的 ANSI Join 语法</li>
<li>JOIN 条件只能支持 <code>=</code> 运算符，不能使用 <code>&gt;</code>, <code>&lt;</code></li>
<li>Hive 不能支持正常的 <code>INSERT INTO</code>, 只能使用 <code>INSERT INTO OVERWRITE</code> 从已有的数据中生成</li>
</ul>

<p>Hive 中可以直接调用 MapReduce 程序：</p>

<pre><code>FROM (
  MAP doctext USING &#39;python wc_mapper.py&#39; AS (word, cnt)
  FROM docs
  CLUSTER BY word
) a
REDUCE word, cnt USING &#39;python wc_reduce.py&#39;;
</code></pre>

<p>Hive 提供了 <code>CLUSTER BY</code> 和 <code>DISTRIBUTE BY</code> 等语法改善 Reduce 的数据分布，解决数据倾斜问题</p>

<h3 id="toc_3">Data Storage, SerDe and File formats</h3>

<h5 id="toc_4">Data Storage</h5>

<p>Hive 的存储模型有 3 个层级</p>

<ul>
<li>Tables 对应 HDFS 的一个目录</li>
<li>Partitions 是 Table 的子目录</li>
<li>Buckets 是目录下具体的文件</li>
</ul>

<blockquote>
<p>Partition 的字段不是 Table 数据的一部分，而是保存在目录的名称中。比如 <code>/usr/hive/warehouse/t1/p_date=20170701</code></p>
</blockquote>

<p>Partition 可以优化查询性能，当用户指定 Partition 的情况下，Hive 只会扫描指定 Partition 下的文件。当 Hive 运行在 <code>strict</code> 模式时，用户需要指定只要一个 Partition 字段。</p>

<p>Bucket 相当于目录树的叶子节点，在创建表的时候用户可以指定需要多少个 Bucket，Bucket 可以用户数据的快速采样。</p>

<p>由于数据都保存在 HDFS 的表空间下，如果用户需要查询 HDFS 其他目录的文件，可以使用外部表：</p>

<pre><code>CREATE EXTERNAL TABLE test_extern(c1 string, c2 int)
LOCATION &#39;/user/mytables/mydata&#39;;
</code></pre>

<p>外部表和普通表的唯一区别是当我们执行 <code>DROP TABLE</code> 时，外部表不会删除 HDFS 的文件。</p>

<h5 id="toc_5">SerDe</h5>

<p>SerDe 提供了几个 Java 接口，方便在文件格式和 Java Native 类型之间相互转化。默认的 SerDe 实现叫 <code>LazySerDe</code> ，是一种用文本表示数据的存储格式。这种格式用 <code>Ctrl-A</code> 来分割列，<code>\n</code> 来分割行。其他的 SerDe 实现包括 RegexSerDe, Thrift, Avro 等等。</p>

<h5 id="toc_6">File Formats</h5>

<p>Hadoop 上的文件可以以不同格式存储，Hive 默认的存储格式是一种叫 TextFormat 的格式，用类似 CSV 的纯文本表示数据。Format 可以在创建表的时候指定：</p>

<pre><code>CREATE TABLE dest1(key INT, value STRING)
  STORED AS
    INPUTFORMAT &#39;org.apache.hadoop.mapred.SequenceFileInputFormat&#39;
    OUTPUTFORMAT &#39;org.apache.hadoop.mapred.SequenceFileOutputFormat&#39;;
</code></pre>

<p>存储格式可以根据自己的需求扩展，选择合适的存储格式有利于提高性能，比如面向列存储的 ParquetFile 和 ORCFile，可以减少读取的数据量，ORCFile 甚至可以做一些与计算，来满足 Push Down 的需求。</p>

<h5 id="toc_7">系统架构和组件</h5>

<p>Hive 由下面的组件构成：</p>

<ul>
<li>MetaStore - 存储系统目录看，还有数据库，表，列的各种原信息</li>
<li>Driver - 管理 HiveQL 的生命周期</li>
<li>Query Compiler - 将 Hive 的语句转换成一个 MapReduce 表达的 DAG</li>
<li>Execution Engine - 将任务按照依赖顺序执行，早起版本只有 MapReduce，新版本应该有 Tez 和 Spark</li>
<li>HiveServer - 提供 JDBC/ODBS 接口的服务，方便和其他系统集成</li>
<li>Clients - Web UI 和命令行工具</li>
</ul>

<p>A. MetaStore</p>

<p>Hive 的 MetaStore 一般基于一个 RDBMS 来实现，提供了一个 ORM 层来作为数据库的抽象。MetaStore 本身不是为了高并发和高可用设计的，在架构中也是一个单点（新版本有主从了），所以 Hive 在设计的时候需要保证在任务执行期间没有任何对 MetaStore 的访问。</p>

<p>B. Query Compiler</p>

<p>Query Compiler 首先将 HQL 转换成一个 AST，然后进行类型检查和语法分析，将 AST 转换成一个 operator DAG （QB Tree），然后优化器对 QB Tree 进行优化。Hive 只支持基于规则的优化器，比如只读取指定列的数据减少 IO，或者用户指定分区字段时，只读取指定分区下的文件。<br/>
RBO 本质上是一个 Transformer，在 QB Tree 上做一系列的变换来减少查询的代价</p>

<blockquote>
<p>Hive 0.1.4 开始引入了一些基于代价的优化器（CBO）。<a href="https://zh.hortonworks.com/blog/hive-0-14-cost-based-optimizer-cbo-technical-overview/">COST BASED OPTIMIZER</a></p>
</blockquote>

<p>之后就根据优化后的 QB Tree 生成物理执行计划，并且将 jar 包写入到 HDFS 的临时目录，开始运行。</p>

<p>C. Execution Engine</p>

<p>执行引擎会解析 plan.xml ，然后按照顺序将任务提交到 Hadoop，最终的数据库文件也会 move 到 HDFS 的指定目录。</p>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-12-03T15:41:08+08:00" itemprop="datePublished">2016/12/3</time>
			</div>
			
			 
			
		</div>
		<h1 class="title" itemprop="name"><a href="14807508681806.html" itemprop="url">
		金丝雀发布</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p>金丝雀部署（Canary Deployments）在知乎落地差不多一年时间，通过金丝雀避免了很多线上的问题，相比之前的发布模型，极大降低了部署的风险。知乎是非常推崇 Devops 的公司，金丝雀发布作为 Devops 一种实践自然不会落下。但是由于基础设施变得越来越抽象和复杂，理解整个部署的工作流程已经变得比较困难，正好有机会给新的同事科普一下金丝雀发布的架构。</p>

<p>相传上个世纪煤矿工人在作业时，为了避免瓦斯中毒会随身带一只金丝雀下到矿洞，由于金丝雀对二氧化碳非常敏感，所以看到金丝雀昏厥的时候矿工们就知道该逃生了。[1]</p>

<p>金丝雀发布就是用生产环境一小部分流量验证应用的一种方法。从这个名字的由来也可以看到，金丝雀发布并不是完美的，如果代码出现问题，那么背用作测试的小部分流量会出错，就跟矿坑中昏厥的金丝雀一样。这种做法在非常敏感的业务中几乎无法接受，但是当系统复杂的到一定程度，错误无法完全避免的时候，为了避免出现更大的问题，牺牲一小部分流量，就可以将大部分错误的影响控制在一定范围内。</p>

<h3 id="toc_0">金丝雀发布的步骤</h3>

<p>一个典型的金丝雀发布大概包含以下步骤[2]：</p>

<ol>
<li>准备好发布用的 artifact</li>
<li>从负载均衡器上移除金丝雀服务器</li>
<li>升级金丝雀服务器</li>
<li>最应用进行自动化测试</li>
<li>将金丝雀服务器加入到负载均衡列表中</li>
<li>升级剩余的服务版本</li>
</ol>

<p>在知乎，负载均衡器采用的 HAProxy，并且依赖 Consul 作服务注册发现。而服务器可能是一台物理机也可能是 bay 上一个抽象的容器组。</p>

<p><img src="media/14807508681806/Canary%20Deployments.png" alt="Canary Deployments"/></p>

<ul>
<li>对于物理机，我们可以单独为其配置一个一台独立的服务器，通过在 HAProxy 上设置不同于 Production 服务器的权重来控制测试流量。但是这种方法不够方便，做自动化也难一些</li>
<li>对于容器相对简单，我们复制一个 Production 版本的容器组，然后通过控制 Production 和 Canary 容器组的数量就可以控制流量。</li>
</ul>

<p>整个过程是部署系统在中间协调，当我们上线发布完成，部署系统会移除金丝雀服务器，让应用回到 Normal 状态。</p>

<p>如果遇到问题需要回滚，只需要将金丝雀容器组从 HAProxy 上摘掉就可以，基本上可以在几秒内完成。</p>

<h3 id="toc_1">HAProxy</h3>

<p>在整个金丝雀发布的架构中，HAProxy 是非常重要的一个组件，要发现后端的服务地址，并动态控制金丝雀和线上的流量比例。部署时我们并不会直接操作 HAProxy，而是更新 Consul 上的注册信息，通过事件广播告诉 HAProxy 服务地址有变化，这一过程通过 <code>consul-template</code> 完成。</p>

<p><img src="media/14807508681806/consul.png" alt="consu"/><br/>
HAProxy 自己也会注册到 Cosnul，伪装成服务的后端被调用，而服务自身则注册成 <code>服务名 + --instance</code>，在我们内部的 Consul 控制台可以看到。</p>

<p>每个服务都有自己独立的 HAproxy 集群，分布在不同的机器上，每个 HAProxy 只知道自己代理的服务的地址。这样做的好处是单个 HAProxy 崩溃不会影响业务，一组 HAProxy 负载高不会把故障扩散到整个集群。另外附带的一个好处是当我们更新服务注册地址时，不会 reload 整个 HAProxy 集群，只要更新对应的 HAProxy 实例就可以，一定程度上可以规避惊群问题。</p>

<p>HAProxy 的地址通过客户端服务发现获得，客户端发现多个 HAProxy 地址并可以做简单的负载均衡，将请求压力分摊到多个 HAProxy 实例上。</p>

<p>采用类似方案的公司是 Airbnb，不过他们的做法是把 HAProxy 作为一个 Agent 跑在服务器上，HAProxy 更靠近客户端[3]。</p>

<h3 id="toc_2">金丝雀发布的监控</h3>

<p>有了金丝雀发布后，我们还得能区分金丝雀和线上的监控数据，以此判断服务是否正常。</p>

<p>由于有了 zone 和 tzone 框架对监控的支持，这件事推起来相对简单。我们在部署时将服务当前所在的环境注入到环境变量中，然后根据环境变量来决定指标的名称。</p>

<p>比如一个正常的指标名称是:</p>

<pre><code>production.span.multimedia.server.APIUploadHandler_post.request_time
</code></pre>

<p>在金丝雀环境中的名称则是:</p>

<pre><code>canary.span.multimedia.server.APIUploadHandler_post.request_time
</code></pre>

<p>最后我们可以在 Halo 对比服务在金丝雀和生产的表现有何差别：</p>

<p><img src="media/14807508681806/14808361124276.jpg" alt=""/></p>

<h3 id="toc_3">Reference</h3>

<p>[1] 金丝雀发布的由来: <a href="https://blog.ccjeng.com/2015/06/canary-deployment.html">https://blog.ccjeng.com/2015/06/canary-deployment.html</a><br/>
[2] 在生产中使用金丝雀部署来进行测试: <a href="http://www.infoq.com/cn/news/2013/03/canary-release-improve-quality">http://www.infoq.com/cn/news/2013/03/canary-release-improve-quality</a><br/>
[3] Service Discovery in the Cloud: <a href="http://nerds.airbnb.com/smartstack-service-discovery-cloud/">http://nerds.airbnb.com/smartstack-service-discovery-cloud/</a></p>


			
			
		</div>

	</article>
  

</div>
<nav id="pagenavi">
	 
	 <a class="next" href="all_1.html">Next</a> 
	<div class="center"><a href="archives.html">Blog Archives</a></div>

</nav>

</div>



        </div>
			<footer id="footer" class="inner">Copyright &copy; 2014
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; 
Theme by <a href="http://shashankmehta.in/archive/2012/greyshade.html">Shashank Mehta</a>
      </footer>
		</div>
	</div>


<script type="text/javascript">
    var disqus_shortname = 'lfyzjck'; 

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>

<script type="text/javascript">
var disqus_shortname = 'lfyzjck'; 

(function () {
var s = document.createElement('script'); s.async = true;
s.type = 'text/javascript';
s.src = '//' + disqus_shortname + '.disqus.com/count.js';
(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>
  
    
<script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>