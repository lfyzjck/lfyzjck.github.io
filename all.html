<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>  
	  
  	Redmagic
  	
	</title>

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	<link href="atom.xml" rel="alternate" title="Redmagic" type="application/atom+xml">

	<link href="asset/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link href="asset/stylesheets/font-awesome.min.css" media="screen, projection" rel="stylesheet" type="text/css">
	<script src="asset/javascripts/jquery.min.js"></script>
	<script src="asset/highlightjs/highlight.pack.js"></script>
	<link href="asset/highlightjs/styles/solarized_dark.css" media="screen, projection" rel="stylesheet" type="text/css">
<script>hljs.initHighlightingOnLoad();</script>

	<!--[if lt IE 9]><script src="asset/javascripts/html5.js"></script><![endif]-->
	<!-- <link href='http://fonts.googleapis.com/css?family=Nunito:400,300,700' rel='stylesheet' type='text/css'> -->
	<style type="text/css">
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 300;
  src: local('Nunito-Light'), url(asset/font/1TiHc9yag0wq3lDO9cw0voX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 400;
  src: local('Nunito-Regular'), url(asset/font/6TbRXKWJjpj6V2v_WyRbMX-_kf6ByYO6CLYdB4HQE-Y.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 700;
  src: local('Nunito-Bold'), url(asset/font/TttUCfJ272GBgSKaOaD7KoX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
	</style>
	
	<style type="text/css">
	.container .left-col{ opacity: 1;}
	#pagenavi a{ font-size: 1.3em;}
	#pagenavi .next:before{ top: 3px;}
	#pagenavi .prev:before{ top: 3px;}
	.container .mid-col .mid-col-container #content .archives .title{ font-size: 1.5em;}
	.container .mid-col .mid-col-container #content article{ padding: 15px 0px;}
	#header .subtitle {
		line-height: 1.2em;
		padding-top: 8px;
	}
	article pre{ background: none; border: none; padding: 0;}
	article .entry-content{text-align: left;}
	.share-comment{ padding: 25px 0px; clear: both;}
	hr{ margin: 20px 0px;border: 0; border-top:solid 1px #ddd;}
	</style>
  

</head>


<body>
	<div class="container">
		<div class="left-col">
			<div class="intrude-less">
				<header id="header" class="inner">
				 
					
					<h1><a href="index.html">Redmagic</a></h1>
					<p class="subtitle">FFFF</p>
					<nav id="main-nav">
						<ul class="main">
						
						  <li id=""><a target="self" href="index.html">Home</a></li>
						
						  <li id=""><a target="_self" href="archives.html">Archives</a></li>
						
						</ul>
					</nav>

					<nav id="sub-nav">
						<div class="social">













								

								<a class="rss" href="atom.xml" title="RSS">RSS</a>
							
						</div>
					</nav>
				</header>				
			</div>
		</div>	
		<div class="mid-col">
			<div class="mid-col-container"> <div id="content" class="inner">
<div itemscope itemtype="http://schema.org/Blog">


	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2018-07-05T21:00:20+08:00" itemprop="datePublished">2018/7/5</time>
			</div>
			
			 
			
		</div>
		<h1 class="title" itemprop="name"><a href="15307956203332.html" itemprop="url">
		Sqoop 使用指南</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p>Sqoop 是一个数据同步工具，用于关系型数据库和各种大数据存储比如 Hive 之间的数据相互同步。Sqoop 因为它的使用便利得到了广泛使用。类似的工具还有阿里开源的 <a href="https://github.com/alibaba/DataX">DataX</a> 和其他商业工具。</p>

<p><a href="http://sqoop.apache.org/docs/1.99.7/index.html">Sqoop 2.0</a> 主要解决 Sqoop 1.x 扩展难的问题，提出的 Server-Client 模型，具体用的不是特别多。本文主要介绍的还是 Sqoop 1.x，最新的 Sqoop 版本是 1.4.7</p>

<h3 id="toc_0">安装</h3>

<p>Sqoop 安装需要依赖 Hadoop 和 Hive，以 Debain 为例，安装 Sqoop 也比较简单。</p>

<pre><code class="language-bash">apt-get install hadoop hive hive-hbase hive-hcatalog sqoop
</code></pre>

<p>除此之外，针对不同的数据源，需要不同的 JDBC Driver，这个是 Sqoop 默认没有自带的库，需要自行安装。比如 MySQL 的 Driver 是 <code>mysql-connector-java-5.1.13-bin.jar</code> ，确保 Jar 包在 Sqoop 的 classpath 内就行。</p>

<h3 id="toc_1">数据源</h3>

<p>Sqoop 支持非常多的数据源，理论上所有支持 JDBC 的数据源都可以作为 Sqoop 的数据源。最常见的场景还是从关系型数据（RDBMS）导入到 Hive, HBase 或者 HDFS。</p>

<p>Sqoop 的扩展性没有想象中的那么好，但是因为大部分企业的数据仓库还是构建在传统的 Hive 和 HBase 之上的，Sqoop 还是可以满足 80% 的数据同步需求的。</p>

<p>一个简单以 MySQL 作为上游数据源的同步：</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
  --username dbuser --password &quot;&quot; 
</code></pre>

<p>Sqoop 支持将数据同步到 HDFS 或者直接到 Hive：</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
  --username dbuser --password &quot;&quot; --table employees \
  --hive-import --hive-overwrite \
  --hive-database employees --hive-table employees
</code></pre>

<h3 id="toc_2">存储格式</h3>

<p>存储格式主要是 Hive 的概念，但是对于数据同步来讲，格式的选择会影响同步数据，类型系统的兼容性等等，我们必须予以关注。参考下面的表格：</p>

<table>
<thead>
<tr>
<th></th>
<th>压缩比</th>
<th>预计算</th>
<th>类型兼容性</th>
</tr>
</thead>

<tbody>
<tr>
<td>TextFile</td>
<td>无</td>
<td>否</td>
<td>一般</td>
</tr>
<tr>
<td>SequenceFile</td>
<td>中</td>
<td>否</td>
<td>一般</td>
</tr>
<tr>
<td>Parquet</td>
<td>高</td>
<td>是（sqoop 依赖的版本 feature 不完整）</td>
<td>好</td>
</tr>
<tr>
<td>ORC</td>
<td>高</td>
<td>是</td>
<td>好</td>
</tr>
</tbody>
</table>

<p>Hive 默认的存储格式是 TextFile，TextFile 类似一个 CSV 文件，使用不可见服务分割列，同步后的数据可读性比较好。但是因为所有数据都是按文本存储的，对于某些类型（比如 blob/bit ）无法支持。</p>

<p>Parquet/ORC 都是列式存储格式，这里不多介绍。在生产环境中更倾向于选择 Parquet/ORC ，节省空间的同时在 Hive 上的查询速度也更快。</p>

<p>同步为 Parquet 格式：</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
 --username dbuser --password &quot;&quot; --table employees \
 --hive-import --hive-overwrite \
 --hive-database employees --hive-table employees \
 --as-parquetfile
</code></pre>

<p>如果要导出为 ORC 格式，需要借助 Hive 提供的一个组件 HCatalog，同步语法也稍稍不太一样</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
 --username dbuser --password &quot;&quot; --table employees \
 --drop-and-create-hcatalog-table \
 --hcatalog-database employees --hcatalog-table employees \
 --hcatalog-storage-stanza &quot;STORED AS ORC&quot;
</code></pre>

<p>Parquet 理论上也可以通过这种方式同步，不过实测当前 Sqoop 版本 (1.4.7) 还是有 BUG，还是等等吧。</p>

<h3 id="toc_3">类型的兼容性</h3>

<p>由于数据源支持的类型和 Hive 本身可能不太一样，所以必然存在类型转换的问题。实际在使用过程中也是非常头疼的一件事。对于 Hive 来说，支持的类型取决于采用的存储格式。以 MySQL 为例，当存储格式为 Hive 时，基本的类型映射如下：</p>

<pre><code>MySQL(bigint) --&gt; Hive(bigint) 
MySQL(tinyint) --&gt; Hive(tinyint) 
MySQL(int) --&gt; Hive(int) 
MySQL(double) --&gt; Hive(double) 
MySQL(bit) --&gt; Hive(boolean) 
MySQL(varchar) --&gt; Hive(string) 
MySQL(decimal) --&gt; Hive(double) 
MySQL(date/timestamp) --&gt; Hive(string)
</code></pre>

<p>这里的类型映射并不完全准确，因为还取决于目标存储格式支持的类型。</p>

<p>由于 Text 格式非常类似 CSV，使用文本存储所有数据，对于 <code>Binary/Blob</code> 这样的类型就无法支持。Parquet/ORC/Avro 因为引入了序列化协议，本身存储是基于二进制的，所以可以支持绝大部分类型。</p>

<p>如果你在使用 TextFile 需要注意下面的问题：</p>

<ul>
<li>上游数据源中的 <code>NULL</code> 会被转化为字符串的 <code>NULL</code>, Hive 中的 <code>NULL</code> 用 <code>\N</code> 表示</li>
<li>如果内容中含有换行符，同步到 Hive 中会被当做独立的两行来处理，造成查询结果和实际数据不相符</li>
</ul>

<p>处理方法比较简单</p>

<p>如果在使用 Parquet，要注意 Sqoop 自带的 Parquet 库版本比较旧，不支持 DateTime/Timestamp 类型的数据，而是会用一个表示 ms 的 BIGINT 来代替，分析数据的时候应该注意这点。</p>

<h3 id="toc_4">数据校验</h3>

<p>Sqoop 内建有 validate 机制，只能验证单表的 row count: <a href="https://sqoop.apache.org/docs/1.4.3/SqoopUserGuide.html#validation">Sqoop User Guide (v1.4.3)</a></p>

<h3 id="toc_5">增量导入</h3>

<p>对于数据量很大的库，全量同步会非常痛，但是如果可以选择还是尽可能的选择全量同步，这种同步模式对数据一致性的保证最好，没有状态。如果不得不进行增量同步，可以继续往后看。</p>

<p>增量导入对业务是有一定侵入的，Schema 的设计和数据写入模式需要遵守一定的规范：</p>

<ul>
<li>增量同步表，最好有一个 Primary Key ，最好是单调递增的 ID</li>
<li>数据的写入模式满足下面两种情形之一

<ul>
<li>（Append）表的内容类似日志，一次写入不做修改和删除</li>
<li>（LastModified）表的内容有修改和删除，但是删除操作是逻辑删除，比如用 <code>is_deleted</code> 字段标识，并且有一个最后更新的时间戳比如 <code>updated_at</code>，<code>updated_at</code> 上有索引。</li>
</ul></li>
</ul>

<p>增量的数据同步大致分为 2 个阶段：读取增量数据和合并数据。对 Sqoop 来说，增量同步需要 <a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_literal_sqoop_metastore_literal">sqoop-metastore</a> 的支持，用于保存上次同步的位置。</p>

<p>比如对于 Append 模式，假设我们有一张表叫 <code>employees</code>，Primary Key 是 <code>id</code>，上一次同步到 <code>id &lt;= 10000</code> 的数据：</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
  --username dbuser --password &quot;&quot; --table employees \
  --target-dir &lt;path/to/hive/table/location&gt; \
  --incremental append --check-column id --last-value 10000
</code></pre>

<p>我们直接将数据 load 到了 Hive 的表空间里，Hive 可以直接查询到最新增量的数据。<br/>
对 LastModified 模式会稍微复杂一些，除了加载增量数据，还涉及数据合并的问题，这里唯一的主键就特别重要了。</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
  --username dbuser --password &quot;&quot; --table employees \
  --target-dir &lt;path/to/hive/table/location&gt; \
  --incremental lastmodified --check-column updated_at --last-value &#39;2018-07-05 00:00:00&#39;
</code></pre>

<p>Sqoop 会在同步结束后再启动一个 merge 任务对数据去重，如果表太小，可能 merge 的代价比全量同步的还要高，我们就要慎重考虑全量同步是不是值得了。</p>

<blockquote>
<p>由于 HDFS 不支持修改文件，sqoop 的 <code>--incremental</code> 和 <code>--hive-import</code> 不能同时使用</p>
</blockquote>

<p>Sqoop 也提供了单独的 <code>sqoop merge</code> 工具，我们也可以分开进行 import 和 merge 这两个步骤。</p>

<h3 id="toc_6">加速同步</h3>

<p>这个小节讨论一下如何加快 Sqoop 的同步速度，Sqoop 同步速度大致取决于下面的几个因素：</p>

<ul>
<li>数据源的读取速度</li>
<li>HDFS 写入速度</li>
<li>数据倾斜程度</li>
</ul>

<h5 id="toc_7">数据源的读取速度</h5>

<p>如果上游数据源是 MySQL，可以考虑更换 SSD，保证 MySQL 实例的负载不要太高。除此之外，Sqoop 可以通过参数控制并发读取的 Mapper 个数加快读取速度。</p>

<pre><code class="language-bash">sqoop -m &lt;mapper_num&gt; ......
</code></pre>

<p>注意 <code>-m</code> 并不是越大越高，并发数过高会把数据库实例打死，同步速度反而变慢。</p>

<p>Sqoop 默认会通过 jdbc 的 API 来读取数据，但是可以通过参数控制使用 MySQL 自己的 <code>mysqldump</code> 来导出数据，这种方式比 jdbc 快一些，缺点是你不能选择要同步的列。另外只能支持目标格式为 Textfile。比较局限但是特定情况下还是很好使的。</p>

<h5 id="toc_8">HDFS 写入速度</h5>

<p>这个除了刚刚提供的控制并发数，还需要保证 Yarn 分配给 Sqoop 的资源充足，不要让资源成为同步的瓶颈。另外，当我们选择 Parquet/ORC 作为存储格式时，数据在写入的时候需要做大量的预计算，这个过程是比较消耗 CPU 和内存的，我们可以控制 MapReduce 参数，适当提高 Sqoop 的资源配额。</p>

<pre><code class="language-bash">sqoop -Dmapreduce.map.cpu.vcores=4 -Dmapreduce.map.memory.mb=8192 ...
</code></pre>

<h5 id="toc_9">数据倾斜</h5>

<p>Sqoop 默认的导入策略是根据主键进行分区导入的，具体的并发粒度取决于 <code>-m</code> 参数。如果主键不连续出现大幅度跳跃，就会导致 Sqoop 导入的时候出现严重的数据倾斜。比如某张表的主键分布是这样的：</p>

<pre><code>1
2
3
...
1000
1001
100000
100001
</code></pre>

<p>Sqoop 计算每个 Mapper 读取的数据范围的时候，会遵循很简单的公式计算：</p>

<pre><code>range = (max(pk) - min(pk)) / mapper
</code></pre>

<p>几乎出现所有的数据 load 都集中在第一个 mapper 上，整体同步相当于没有并发。</p>

<p>参考阅读：</p>

<ul>
<li><a href="https://blog.csdn.net/mike_h/article/details/50148309">Hive 数据倾斜 (Data Skew) 总结 - CSDN博客</a></li>
<li><a href="http://abhinaysandeboina.blogspot.hk/2017/08/avoiding-data-skew-in-sqoop.html">Avoiding Data Skew in Sqoop</a></li>
<li><a href="https://docs.qingcloud.com/guide/sqoop.html">Sqoop 指南 — QingCloud  文档</a></li>
</ul>

<h3 id="toc_10">导出</h3>

<p>Sqoop 支持将 Hive 的数据导出到 MySQL，方便在线系统调用。</p>

<pre><code>sqoop export --connect jdbc:mysql://database.example.com/employees --table employees --username dbuser --password &quot;&quot; --relaxed-isolation --update-key id --update-mode allowinsert --hcatalog-database employees --hcatalog-table employees
</code></pre>

<p>借助 HCatalog 可以比较轻松的将 Hive 表的数据直接导出到 MySQL。更多的详情参考官方文档，这里不多介绍。</p>

<h3 id="toc_11">更进一步</h3>

<p>如果我们要同步的数据非常多，管理同步任务本身就变成了一件复杂的事情。我们不仅要考虑源数据库的负载，安全性。还要考虑同步任务的启动时间，Schema 变更等等问题。实际使用的时候，我们在内部自研了一个平台，管理 MySQL 和 Hive 的数据源并对 Sqoop 任务做了调度。有一部分功能在 Sqoop 2.0 已经实现了。在大规模使用 sqoop 一定要想清楚运维的问题。</p>

<h3 id="toc_12">Reference</h3>

<ul>
<li><a href="https://stackoverflow.com/questions/24987820/not-able-to-run-sqoop-using-oozie">hadoop - Not able to run sqoop using oozie - Stack Overflow</a></li>
<li><a href="https://stackoverflow.com/questions/23250977/how-to-deal-with-sqoop-import-delimiter-issues-r-n">mysql - how to deal with sqoop import delimiter issues \r\n - Stack Overflow</a></li>
<li><a href="https://www.zybuluo.com/aitanjupt/note/209968">使用Sqoop从MySQL导入数据到Hive和HBase 及近期感悟 - 作业部落 Cmd Markdown 编辑阅读器</a></li>
<li><a href="https://community.hortonworks.com/questions/28060/can-sqoop-be-used-to-directly-import-data-into-an.html">Can sqoop be used to directly import data into an ORC table? - Hortonworks</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">LanguageManual ORC - Apache Hive - Apache Software Foundation</a></li>
<li>Hive 增量同步： <a href="https://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/">Four-step Strategy for Incremental Updates in Apache Hive</a></li>
<li>使用 MERGE INTO 更新 Hive 数据： <a href="https://hortonworks.com/blog/update-hive-tables-easy-way/">Update Hive Tables the Easy Way - Hortonworks</a></li>
<li>SQL MERGE 的性能： <a href="https://hortonworks.com/blog/apache-hive-moving-beyond-analytics-offload-with-sql-merge/">Apache Hive: Moving Beyond Analytics Offload with SQL MERGE - Hortonworks</a></li>
<li><a href="https://community.hortonworks.com/questions/11373/sqoop-incremental-import-in-hive-i-get-error-messa.html">sqoop incremental import in hive I get error message hive not support append mode how to solve that - Hortonworks</a></li>
<li><a href="http://www.hadooptechs.com/sqoop/sqoop-incremental-import-mysql-to-hive">Sqoop Incremental Import | MySQL to Hive | Big Data &amp; Hadoop</a></li>
<li><a href="https://ask.hellobi.com/blog/marsj/4114">Sqoop 1.4.6 导入实战 (RDB含MySQL和Oracle) - 天善智能：专注于商业智能BI和数据分析、大数据领域的垂直社区平台</a></li>
</ul>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2018-02-02T15:40:38+08:00" itemprop="datePublished">2018/2/2</time>
			</div>
			
			 
			
		</div>
		<h1 class="title" itemprop="name"><a href="15175572389238.html" itemprop="url">
		使用 BigTop 打包 Hadoop 全家桶</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p>使用 Hadoop 软件好像难免会自己改下代码做些定制，或者在部分组件的版本选择上激进，其他的版本( 比如 HDFS)  上保守。最近在公司升级 Hive 到 2.1.1 ，也对代码做了一些调整确保对业务的兼容性，之前公司使用的是 <code>hive-1.2.2-cdh-5.5.0</code> 。cdh 的发布节奏太慢跟不上社区的节奏，而且截止到现在，社区版本的 BUG 数量和稳定性都可以接受而不是必须选择商业公司给我们提供的发行版。</p>

<p>公司用的服务器是 Debian 7/8，为了方便的把定制过的 Hive 部署到服务器，需要将 Hive 打包成 deb，一直没找到特别好的打包方法。要做到 Cloudera 那样规范的 deb 非常繁琐，要处理启动脚本，环境变量，配置文件的 alternatives 等等。顺着这个思路找到了 Cloudera 官方的打包工具 <a href="https://github.com/cloudera/cdh-package">cdh-package</a> ，但是这个库已经太长时间没有维护了，里面依赖的版本信息非常老旧，而且自己测试也没运行成功。但是 cdh-package 是基于 <a href="https://github.com/apache/bigtop">BigTop</a> 的，BigTop 本身的维护还不错。</p>

<p>Bigtop 非常有野心，它的主要目标就是构建一个 Apache Hadoop 生态系统的包和交互式测试的社区。这个包括对各类不同级别工程进行测试(包，平台，运行时间，升级等...)，它由社区以关注系统作为一个整体开发而来。BigTop 官方除了介绍怎么安装之外没有任何使用文档，不过研究以后发现还算简单，不需要太多的说明。</p>

<h3 id="toc_0">准备 BigTop 环境</h3>

<p>可以根据官方的说明来安装，我这里是直接从 Github 拉了代码：</p>

<pre><code class="language-bash">git clone https://github.com/apache/bigtop.git
cd bigtop
git checkout release-1.2.1
./gradlew
</code></pre>

<p>然后我们可以运行 <code>./gradlew tasks</code> 看下 BigTop 给我们提供的命令，命令遵循下面的格式 <code>./gradlew &lt;package&gt;-&lt;dist&gt;</code> ：</p>

<pre><code class="language-bash">$ ./gradlew tasks
# hide some output
hive-clean - Removing hive component build and output directories
hive-deb - Building DEB for hive artifacts
hive-download - Download hive artifacts
hive-help - List of available tasks for hive
hive-info - Info about hive component build
hive-pkg - Invoking a native binary packaging target deb
hive-relnotes - Preparing release notes for hive. No yet implemented!!!
hive-rpm - Building RPM for hive artifacts
hive-sdeb - Building SDEB for hive artifacts
hive-spkg - Invoking a native binary packaging target sdeb
hive-srpm - Building SRPM for hive artifacts
hive-tar - Preparing a tarball for hive artifacts
hive-version - Show version of hive component
</code></pre>

<p>然后编辑 <code>bigtop.bom</code> 将依赖的版本改成自己需要的版本，注意 BigTop 这里会优先使用 bigtop.bom 中定义的版本号覆盖源代码的版本号。</p>

<pre><code class="language-json">&#39;hive&#39; {
      name    = &#39;hive&#39;
      relNotes = &#39;Apache Hive&#39;
      version { base = &#39;1.2.1&#39;; pkg = base; release = 1 }
      tarball { destination = &quot;apache-${name}-${version.base}-src.tar.gz&quot;
                source      = destination }
      url     { download_path = &quot;/$name/$name-${version.base}/&quot;
                site = &quot;${apache.APACHE_MIRROR}/${download_path}&quot;
                archive = &quot;${apache.APACHE_ARCHIVE}/${download_path}&quot; }
    }
</code></pre>

<p>下面将介绍如何用 BigTop 打包 Hive</p>

<h3 id="toc_1">用 BigTop 打包 Hive</h3>

<p>我们的目标是将一份修改过的 Hive 代码打包成 deb 包分发到集群，首先在编辑机器上准备一些必要的依赖：</p>

<pre><code class="language-bash">sudo apt-get update
sudo apt-get install devscripts
sudo apt-get install dh-make
</code></pre>

<p>接下来准备 Hive 的代码，Bigtop 默认根据 bom 文件里指定的版本号从上游下载 Hive 的代码，解压然后编译。但是由于我们要使用自己修改过的版本，可以修改 <code>bigtop.bom</code> 从内部 git 仓库下载代码。</p>

<pre><code class="language-grovvy">  &#39;hive&#39; {
      name    = &#39;hive&#39;
      relNotes = &#39;Apache Hive&#39;
      version { base = &#39;2.1.1&#39;; pkg = base; release = 1 }
      tarball { destination = &quot;apache-${name}-${version.base}-src.tar.gz&quot;
                source      = destination }
      git     { repo = &quot;https://exmaple.com:hive/hive&quot;
                ref = &quot;release-2.1.1&quot;
                dir  = &quot;${name}-${version.base}&quot; }
    }
</code></pre>

<p>然后就可以开始打包了：</p>

<pre><code class="language-bash">./gradlew hive-deb
</code></pre>

<p>注意 BigTop 在它的仓库里包含了对 Hive 的几个 patch 文件，我这边测试的时候会导致编译失败，建议删除：</p>

<pre><code class="language-bash">rm -f /bigtop-packages/src/common/hive/*
</code></pre>

<p>然后清理构建环境，重新打包：</p>

<pre><code class="language-bash">./gradlew hive-clean
./gradlew hive-deb
</code></pre>

<p>如果需要更新包，可以提升 Release number，默认是 1 ，这个 BigTop 在文档里没有提及：</p>

<pre><code class="language-bash">BIGTOP_BUILD_STAMP=&lt;release&gt; ./gradlew hive-deb
</code></pre>

<h3 id="toc_2">发布 deb 包</h3>

<p>看看我们构建的结果，产生了很多 deb 包，这些包都需要上传到内部的 mirror</p>

<pre><code>$ ls -l output/hive/
total 138516
-rw-r--r-- 1 ck ck 78645700 Feb  1 18:32 hive_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck   314946 Feb  1 18:33 hive_2.1.1-1_amd64.build
-rw-r--r-- 1 ck ck     3461 Feb  1 18:32 hive_2.1.1-1_amd64.changes
-rw-r--r-- 1 ck ck    12500 Feb  1 18:18 hive_2.1.1-1.debian.tar.xz
-rw-r--r-- 1 ck ck     1227 Feb  1 18:18 hive_2.1.1-1.dsc
-rw-r--r-- 1 ck ck     1829 Feb  1 18:18 hive_2.1.1-1_source.changes
-rw-r--r-- 1 ck ck 20999949 Feb  1 18:18 hive_2.1.1.orig.tar.gz
-rw-r--r-- 1 ck ck   107906 Feb  1 18:32 hive-hbase_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck   452862 Feb  1 18:32 hive-hcatalog_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck     3632 Feb  1 18:32 hive-hcatalog-server_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck 39029552 Feb  1 18:32 hive-jdbc_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck     3734 Feb  1 18:32 hive-metastore_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck     3738 Feb  1 18:32 hive-server2_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck  2068240 Feb  1 18:32 hive-webhcat_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck     3608 Feb  1 18:32 hive-webhcat-server_2.1.1-1_all.deb
</code></pre>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2017-11-13T22:53:59+08:00" itemprop="datePublished">2017/11/13</time>
			</div>
			
			 
			
		</div>
		<h1 class="title" itemprop="name"><a href="15105848393062.html" itemprop="url">
		Hive 论文笔记</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p>MapReduce 出现后，对数据的计算需求越来越多，而 MapReduce 提供的 API 太底层，学习成本和开发成本比较高，因此需要一个类 SQL 的工具，来代替大部分的 MapReduce 的使用场景。</p>

<h3 id="toc_0">数据模型，类型系统和查询语言</h3>

<p>Hive 和传统的数据库一样有 Database 和 Table 的概念，数据存储在 Table 中。每个 Table 中会会很多行，每行有多列组成。</p>

<h5 id="toc_1">类型</h5>

<p>Hive 支持原生类型 (primitive types) 和复杂类型 (complex types)。</p>

<p>Primitive:</p>

<ul>
<li>Integers: bigint, int, smallint, tinyint</li>
<li>Float: float, double</li>
<li>String</li>
</ul>

<p>Complex:</p>

<ul>
<li>map<key-type, value-type></li>
<li>list<element-type></li>
<li>struct<field-name, field-type, …></li>
</ul>

<p>SerDe , Format 都是可插拔的，用户可以自定义 SerDe 或者 Format，在查询时可以通过 HiveQL 增加自己的 SerDe:</p>

<pre><code>ADD JAR /jars/myformat.jar;
CREATE TABLE t2
ROW FORMAT SERDE &#39;com.myformat.MySerDe&#39;;
</code></pre>

<h5 id="toc_2">HiveQL</h5>

<p>HiveQL 和传统的 SQL 几乎没有差别，但是存在一些局限：</p>

<ul>
<li>只能使用标准的 ANSI Join 语法</li>
<li>JOIN 条件只能支持 <code>=</code> 运算符，不能使用 <code>&gt;</code>, <code>&lt;</code></li>
<li>Hive 不能支持正常的 <code>INSERT INTO</code>, 只能使用 <code>INSERT INTO OVERWRITE</code> 从已有的数据中生成</li>
</ul>

<p>Hive 中可以直接调用 MapReduce 程序：</p>

<pre><code>FROM (
  MAP doctext USING &#39;python wc_mapper.py&#39; AS (word, cnt)
  FROM docs
  CLUSTER BY word
) a
REDUCE word, cnt USING &#39;python wc_reduce.py&#39;;
</code></pre>

<p>Hive 提供了 <code>CLUSTER BY</code> 和 <code>DISTRIBUTE BY</code> 等语法改善 Reduce 的数据分布，解决数据倾斜问题</p>

<h3 id="toc_3">Data Storage, SerDe and File formats</h3>

<h5 id="toc_4">Data Storage</h5>

<p>Hive 的存储模型有 3 个层级</p>

<ul>
<li>Tables 对应 HDFS 的一个目录</li>
<li>Partitions 是 Table 的子目录</li>
<li>Buckets 是目录下具体的文件</li>
</ul>

<blockquote>
<p>Partition 的字段不是 Table 数据的一部分，而是保存在目录的名称中。比如 <code>/usr/hive/warehouse/t1/p_date=20170701</code></p>
</blockquote>

<p>Partition 可以优化查询性能，当用户指定 Partition 的情况下，Hive 只会扫描指定 Partition 下的文件。当 Hive 运行在 <code>strict</code> 模式时，用户需要指定只要一个 Partition 字段。</p>

<p>Bucket 相当于目录树的叶子节点，在创建表的时候用户可以指定需要多少个 Bucket，Bucket 可以用户数据的快速采样。</p>

<p>由于数据都保存在 HDFS 的表空间下，如果用户需要查询 HDFS 其他目录的文件，可以使用外部表：</p>

<pre><code>CREATE EXTERNAL TABLE test_extern(c1 string, c2 int)
LOCATION &#39;/user/mytables/mydata&#39;;
</code></pre>

<p>外部表和普通表的唯一区别是当我们执行 <code>DROP TABLE</code> 时，外部表不会删除 HDFS 的文件。</p>

<h5 id="toc_5">SerDe</h5>

<p>SerDe 提供了几个 Java 接口，方便在文件格式和 Java Native 类型之间相互转化。默认的 SerDe 实现叫 <code>LazySerDe</code> ，是一种用文本表示数据的存储格式。这种格式用 <code>Ctrl-A</code> 来分割列，<code>\n</code> 来分割行。其他的 SerDe 实现包括 RegexSerDe, Thrift, Avro 等等。</p>

<h5 id="toc_6">File Formats</h5>

<p>Hadoop 上的文件可以以不同格式存储，Hive 默认的存储格式是一种叫 TextFormat 的格式，用类似 CSV 的纯文本表示数据。Format 可以在创建表的时候指定：</p>

<pre><code>CREATE TABLE dest1(key INT, value STRING)
  STORED AS
    INPUTFORMAT &#39;org.apache.hadoop.mapred.SequenceFileInputFormat&#39;
    OUTPUTFORMAT &#39;org.apache.hadoop.mapred.SequenceFileOutputFormat&#39;;
</code></pre>

<p>存储格式可以根据自己的需求扩展，选择合适的存储格式有利于提高性能，比如面向列存储的 ParquetFile 和 ORCFile，可以减少读取的数据量，ORCFile 甚至可以做一些与计算，来满足 Push Down 的需求。</p>

<h5 id="toc_7">系统架构和组件</h5>

<p>Hive 由下面的组件构成：</p>

<ul>
<li>MetaStore - 存储系统目录看，还有数据库，表，列的各种原信息</li>
<li>Driver - 管理 HiveQL 的生命周期</li>
<li>Query Compiler - 将 Hive 的语句转换成一个 MapReduce 表达的 DAG</li>
<li>Execution Engine - 将任务按照依赖顺序执行，早起版本只有 MapReduce，新版本应该有 Tez 和 Spark</li>
<li>HiveServer - 提供 JDBC/ODBS 接口的服务，方便和其他系统集成</li>
<li>Clients - Web UI 和命令行工具</li>
</ul>

<p>A. MetaStore</p>

<p>Hive 的 MetaStore 一般基于一个 RDBMS 来实现，提供了一个 ORM 层来作为数据库的抽象。MetaStore 本身不是为了高并发和高可用设计的，在架构中也是一个单点（新版本有主从了），所以 Hive 在设计的时候需要保证在任务执行期间没有任何对 MetaStore 的访问。</p>

<p>B. Query Compiler</p>

<p>Query Compiler 首先将 HQL 转换成一个 AST，然后进行类型检查和语法分析，将 AST 转换成一个 operator DAG （QB Tree），然后优化器对 QB Tree 进行优化。Hive 只支持基于规则的优化器，比如只读取指定列的数据减少 IO，或者用户指定分区字段时，只读取指定分区下的文件。<br/>
RBO 本质上是一个 Transformer，在 QB Tree 上做一系列的变换来减少查询的代价</p>

<blockquote>
<p>Hive 0.1.4 开始引入了一些基于代价的优化器（CBO）。<a href="https://zh.hortonworks.com/blog/hive-0-14-cost-based-optimizer-cbo-technical-overview/">COST BASED OPTIMIZER</a></p>
</blockquote>

<p>之后就根据优化后的 QB Tree 生成物理执行计划，并且将 jar 包写入到 HDFS 的临时目录，开始运行。</p>

<p>C. Execution Engine</p>

<p>执行引擎会解析 plan.xml ，然后按照顺序将任务提交到 Hadoop，最终的数据库文件也会 move 到 HDFS 的指定目录。</p>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-12-03T15:41:08+08:00" itemprop="datePublished">2016/12/3</time>
			</div>
			
			 
			
		</div>
		<h1 class="title" itemprop="name"><a href="14807508681806.html" itemprop="url">
		金丝雀发布</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p>金丝雀部署（Canary Deployments）在知乎落地差不多一年时间，通过金丝雀避免了很多线上的问题，相比之前的发布模型，极大降低了部署的风险。知乎是非常推崇 Devops 的公司，金丝雀发布作为 Devops 一种实践自然不会落下。但是由于基础设施变得越来越抽象和复杂，理解整个部署的工作流程已经变得比较困难，正好有机会给新的同事科普一下金丝雀发布的架构。</p>

<p>相传上个世纪煤矿工人在作业时，为了避免瓦斯中毒会随身带一只金丝雀下到矿洞，由于金丝雀对二氧化碳非常敏感，所以看到金丝雀昏厥的时候矿工们就知道该逃生了。[1]</p>

<p>金丝雀发布就是用生产环境一小部分流量验证应用的一种方法。从这个名字的由来也可以看到，金丝雀发布并不是完美的，如果代码出现问题，那么背用作测试的小部分流量会出错，就跟矿坑中昏厥的金丝雀一样。这种做法在非常敏感的业务中几乎无法接受，但是当系统复杂的到一定程度，错误无法完全避免的时候，为了避免出现更大的问题，牺牲一小部分流量，就可以将大部分错误的影响控制在一定范围内。</p>

<h3 id="toc_0">金丝雀发布的步骤</h3>

<p>一个典型的金丝雀发布大概包含以下步骤[2]：</p>

<ol>
<li>准备好发布用的 artifact</li>
<li>从负载均衡器上移除金丝雀服务器</li>
<li>升级金丝雀服务器</li>
<li>最应用进行自动化测试</li>
<li>将金丝雀服务器加入到负载均衡列表中</li>
<li>升级剩余的服务版本</li>
</ol>

<p>在知乎，负载均衡器采用的 HAProxy，并且依赖 Consul 作服务注册发现。而服务器可能是一台物理机也可能是 bay 上一个抽象的容器组。</p>

<p><img src="media/14807508681806/Canary%20Deployments.png" alt="Canary Deployments"/></p>

<ul>
<li>对于物理机，我们可以单独为其配置一个一台独立的服务器，通过在 HAProxy 上设置不同于 Production 服务器的权重来控制测试流量。但是这种方法不够方便，做自动化也难一些</li>
<li>对于容器相对简单，我们复制一个 Production 版本的容器组，然后通过控制 Production 和 Canary 容器组的数量就可以控制流量。</li>
</ul>

<p>整个过程是部署系统在中间协调，当我们上线发布完成，部署系统会移除金丝雀服务器，让应用回到 Normal 状态。</p>

<p>如果遇到问题需要回滚，只需要将金丝雀容器组从 HAProxy 上摘掉就可以，基本上可以在几秒内完成。</p>

<h3 id="toc_1">HAProxy</h3>

<p>在整个金丝雀发布的架构中，HAProxy 是非常重要的一个组件，要发现后端的服务地址，并动态控制金丝雀和线上的流量比例。部署时我们并不会直接操作 HAProxy，而是更新 Consul 上的注册信息，通过事件广播告诉 HAProxy 服务地址有变化，这一过程通过 <code>consul-template</code> 完成。</p>

<p><img src="media/14807508681806/consul.png" alt="consu"/><br/>
HAProxy 自己也会注册到 Cosnul，伪装成服务的后端被调用，而服务自身则注册成 <code>服务名 + --instance</code>，在我们内部的 Consul 控制台可以看到。</p>

<p>每个服务都有自己独立的 HAproxy 集群，分布在不同的机器上，每个 HAProxy 只知道自己代理的服务的地址。这样做的好处是单个 HAProxy 崩溃不会影响业务，一组 HAProxy 负载高不会把故障扩散到整个集群。另外附带的一个好处是当我们更新服务注册地址时，不会 reload 整个 HAProxy 集群，只要更新对应的 HAProxy 实例就可以，一定程度上可以规避惊群问题。</p>

<p>HAProxy 的地址通过客户端服务发现获得，客户端发现多个 HAProxy 地址并可以做简单的负载均衡，将请求压力分摊到多个 HAProxy 实例上。</p>

<p>采用类似方案的公司是 Airbnb，不过他们的做法是把 HAProxy 作为一个 Agent 跑在服务器上，HAProxy 更靠近客户端[3]。</p>

<h3 id="toc_2">金丝雀发布的监控</h3>

<p>有了金丝雀发布后，我们还得能区分金丝雀和线上的监控数据，以此判断服务是否正常。</p>

<p>由于有了 zone 和 tzone 框架对监控的支持，这件事推起来相对简单。我们在部署时将服务当前所在的环境注入到环境变量中，然后根据环境变量来决定指标的名称。</p>

<p>比如一个正常的指标名称是:</p>

<pre><code>production.span.multimedia.server.APIUploadHandler_post.request_time
</code></pre>

<p>在金丝雀环境中的名称则是:</p>

<pre><code>canary.span.multimedia.server.APIUploadHandler_post.request_time
</code></pre>

<p>最后我们可以在 Halo 对比服务在金丝雀和生产的表现有何差别：</p>

<p><img src="media/14807508681806/14808361124276.jpg" alt=""/></p>

<h3 id="toc_3">Reference</h3>

<p>[1] 金丝雀发布的由来: <a href="https://blog.ccjeng.com/2015/06/canary-deployment.html">https://blog.ccjeng.com/2015/06/canary-deployment.html</a><br/>
[2] 在生产中使用金丝雀部署来进行测试: <a href="http://www.infoq.com/cn/news/2013/03/canary-release-improve-quality">http://www.infoq.com/cn/news/2013/03/canary-release-improve-quality</a><br/>
[3] Service Discovery in the Cloud: <a href="http://nerds.airbnb.com/smartstack-service-discovery-cloud/">http://nerds.airbnb.com/smartstack-service-discovery-cloud/</a></p>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-08-10T10:14:46+08:00" itemprop="datePublished">2016/8/10</time>
			</div>
			
			 
			
		</div>
		<h1 class="title" itemprop="name"><a href="14707952861924.html" itemprop="url">
		Graphviz 入门指南</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p>Graphviz 是一个开源的图可视化工具，非常适合绘制结构化的图标和网络。Graphviz 使用一种叫 DOT 的语言来表示图形。</p>

<h3 id="toc_0">DOT 语言</h3>

<p>DOT 语言是一种图形描述语言。能够以简单的方式描述图形，并且为人和计算机所理解。</p>

<h4 id="toc_1">无向图</h4>

<pre><code>graph graphname {
    a -- b -- c;
   b -- d;
}
</code></pre>

<p><img src="media/14707952861924/14710194114259.jpg" alt="" style="width:220px;"/></p>

<h4 id="toc_2">有向图</h4>

<pre><code>digraph graphname {
    a -&gt; b -&gt; c;
    b -&gt; d;
}
</code></pre>

<p><img src="media/14707952861924/14710200127648.jpg" alt=""/></p>

<h4 id="toc_3">设置属性</h4>

<p>属性可以设置在节点和边上，用一对 <code>[]</code> 表示，多个属性可以用空格或者 <code>,</code> 隔开。</p>

<pre><code>strict graph {
    // 设置节点属性
  b [shape=box];
  c [shape=triangle];

  // 设置边属性
  a -- b [color=blue];
  a -- c [style=dotted];
}
</code></pre>

<p>完整的属性列表可以参考 <a href="http://www.graphviz.org/content/attrs">attrs | Graphviz - Graph Visualization Software</a></p>

<h4 id="toc_4">子图</h4>

<p><code>subgraph</code> 的作用主要有 3 个：</p>

<ol>
<li>表示图的结构，对节点和边进行分组</li>
<li>提供一个单独的上下位文设置属性</li>
<li>针对特定引擎使用特殊的布局。比如下面的例子，如果 <code>subgraph</code> 的名字以 <code>cluster</code> 开头，所有属于这个子图的节点会用一个矩形和其他节点分开。</li>
</ol>

<pre><code>digraph graphname{ 
    a -&gt; {b c};
    c -&gt; e;
    b -&gt; d;
    
    subgraph cluster_bc {
        bgcolor=red;
        b;
        c;
    }
    
    subgraph cluster_de {
        label=&quot;Block&quot;
        d;
        e;
    }
}
</code></pre>

<p><img src="media/14707952861924/14710216720774.jpg" alt=""/></p>

<h4 id="toc_5">布局</h4>

<p>默认情况下图是从上到下布局的，通过设置 <code>rankdir=&quot;LR&quot;</code> 可以让图从左到右布局。</p>

<p>一个简单的表示 CI&amp;CD 过程的图：</p>

<pre><code>digraph pipleline {
    rankdir=LR;
    g [label=&quot;Gitlab&quot;];
    j [label=&quot;Jenkins&quot;];
    t [label=&quot;Testing&quot;];
    p [label=&quot;Production&quot; color=red];
    
    g -&gt; j [label=&quot;Trigger&quot;];
    j -&gt; t [label=&quot;Build&quot;];
    t -&gt; p [label=&quot;Approved&quot;];
}
</code></pre>

<p><img src="media/14707952861924/14710227380504.jpg" alt=""/></p>

<h3 id="toc_6">工具</h3>

<p>有非常多的工具可以支持 DOT 语言，这些工具都被集成在 Graphviz 的软件包中，可以简单安装使用。</p>

<p>dot </p>

<blockquote>
<p>一个用来将生成的图形转换成多种输出格式的命令行工具。其输出格式包括PostScript，PDF，SVG，PNG，含注解的文本等等。</p>
</blockquote>

<p>neato </p>

<blockquote>
<p>用于sprint model的生成（在Mac OS版本中称为energy minimized）。</p>
</blockquote>

<p>twopi </p>

<blockquote>
<p>用于放射状图形的生成</p>
</blockquote>

<p>circo </p>

<blockquote>
<p>用于圆形图形的生成。</p>
</blockquote>

<p>fdp </p>

<blockquote>
<p>另一个用于生成无向图的工具。</p>
</blockquote>

<p>dotty </p>

<blockquote>
<p>一个用于可视化与修改图形的图形用户界面程序。</p>
</blockquote>

<p>lefty </p>

<blockquote>
<p>一个可编程的(使用一种被EZ影响的语言[4])控件，它可以显示DOT图形，并允许用户用鼠标在图上执行操作。Lefty可以作为MVC模型的使用图形的GUI程序中的视图部分。</p>
</blockquote>

<p>另外介绍 2 个在线生成 Graphviz 的网站：</p>

<ul>
<li><a href="http://www.webgraphviz.com/">http://www.webgraphviz.com/</a></li>
<li><a href="http://dreampuf.github.io/GraphvizOnline/">http://dreampuf.github.io/GraphvizOnline/</a></li>
</ul>

<h3 id="toc_7">Reference</h3>

<p>[1] <a href="https://www.ibm.com/developerworks/cn/aix/library/au-aix-graphviz/">使用 Graphviz 生成自动化系统图</a><br/>
[2] <a href="https://zh.wikipedia.org/wiki/DOT%E8%AF%AD%E8%A8%80">DOT语言 - 维基百科，自由的百科全书</a><br/>
[3] <a href="http://www.graphviz.org/content/dot-language">http://www.graphviz.org/content/dot-language</a></p>


			
			
		</div>

	</article>
  

</div>
<nav id="pagenavi">
	 
	 <a class="next" href="all_1.html">Next</a> 
	<div class="center"><a href="archives.html">Blog Archives</a></div>

</nav>

</div>



        </div>
			<footer id="footer" class="inner">Copyright &copy; 2014
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; 
Theme by <a href="http://shashankmehta.in/archive/2012/greyshade.html">Shashank Mehta</a>
      </footer>
		</div>
	</div>


<script type="text/javascript">
    var disqus_shortname = 'lfyzjck'; 

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>

<script type="text/javascript">
var disqus_shortname = 'lfyzjck'; 

(function () {
var s = document.createElement('script'); s.async = true;
s.type = 'text/javascript';
s.src = '//' + disqus_shortname + '.disqus.com/count.js';
(document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>
  
    
<script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>