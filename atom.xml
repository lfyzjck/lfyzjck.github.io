<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Redmagic]]></title>
  <link href="http://blog.lfyzjck.com/atom.xml" rel="self"/>
  <link href="http://blog.lfyzjck.com/"/>
  <updated>2019-09-10T11:58:49+08:00</updated>
  <id>http://blog.lfyzjck.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.coderforart.com/">CoderForArt</generator>

  
  <entry>
    <title type="html"><![CDATA[Ansible 快速入门]]></title>
    <link href="http://blog.lfyzjck.com/15915346076929.html"/>
    <updated>2020-06-07T20:56:47+08:00</updated>
    <id>http://blog.lfyzjck.com/15915346076929.html</id>
    <content type="html"><![CDATA[
<p>Ansible 是一个 IT 自动化软件，类似于 Puppet 和 Chef，是实现 Infra-as-a-code 的一种工具。</p>

<h2 id="toc_0">QuickStart</h2>

<h3 id="toc_1">理解 Ansible 如何控制远程服务器</h3>

<p>Ansible 的所有操作是基于 ssh 协议的，我们可以通过 ssh 命令在远程服务器上执行命令：</p>

<pre><code class="language-bash">$ ssh root@ip &#39;ping baidu.com&#39;
PING baidu.com (39.156.69.79) 56(84) bytes of data.
64 bytes from 39.156.69.79 (39.156.69.79): icmp_seq=1 ttl=48 time=7.83 ms
64 bytes from 39.156.69.79 (39.156.69.79): icmp_seq=2 ttl=48 time=3.43 ms
64 bytes from 39.156.69.79 (39.156.69.79): icmp_seq=3 ttl=48 time=3.45 ms
64 bytes from 39.156.69.79 (39.156.69.79): icmp_seq=4 ttl=48 time=3.46 ms
</code></pre>

<p>在 Ansible 中编写的脚本最终也会被翻译成类似上面命令的方式被执行。</p>

<p>在 Ansible 执行的过程中，需要两种角色的节点，分别对应 ssh 的 client 和 server：</p>

<ul>
<li>管理节点（执行 ansible 命令的机器）</li>
<li>托管节点（被 ansible 的管理的机器）</li>
</ul>

<p>Ansible 的管理节点目前支持类 unix 系统，也就是 linux 和 macos。暂时不支持 windows 作为管理节点。管理节点可以是自己本地的电脑，但是由于私有化部署时我们面对的通常都是客户的内网环境，一般都无法在自己的开发机直接访问。为了简化使用方式，我们选择客户的一台服务器及作为管理节点也同时作为托管节点，所有 ansible 的操作总是从管理节点发起。</p>

<p>既然 Ansible 是基于 ssh 协议实现的，我们必须决定以哪个<strong>用户</strong>的身份登陆，以何种方式进行 <strong>ssh 认证</strong>，以及确保登陆用户有 <strong>sudo</strong> 的权限。ssh 常用的认证方式支持密码和证书两种，由于我们希望自动化部署，不想在部署过程中频繁的输入密码，因此配置证书认证是比较好的选择。另外很多客户环境下 sudo 也需要额外的密码，因此需要在初始化环境时配置 ssh 的登陆用户可以免密的进行 sudo 操作。</p>

<p>总结一下，我们需要：</p>

<ul>
<li>选择一台客户内网的服务器作为管理节点，该节点通常也是我们的托管节点</li>
<li>对托管节点配置基于证书的 ssh 免密登陆</li>
<li>确保 ssh 登陆用户可以免密进行 sudo 操作</li>
</ul>

<h3 id="toc_2">Inventory File</h3>

<p>Ansible 可同时操作属于一个组的多台主机,组和主机之间的关系通过 inventory 文件配置。默认的文件路径为 <code>/etc/ansible/hosts</code> 。不过我们通常不把  inventory file 放在系统目录下，而是放在单独的配置目录。</p>

<p>我们试着写一个 inventory file :</p>

<pre><code class="language-yaml">[webserver]
host1
host2
</code></pre>

<p>保存为 <code>hosts</code> ，然后通过 ansible 的命令行执行 adhoc 命令</p>

<pre><code class="language-yaml">$ ansible all -i hosts -m ping
host1 | UNREACHABLE! =&gt; {
    &quot;changed&quot;: false,
    &quot;msg&quot;: &quot;Failed to connect to the host via ssh: ssh: Could not resolve hostname host1: nodename nor servname provided, or not known&quot;,
    &quot;unreachable&quot;: true
}
host2 | UNREACHABLE! =&gt; {
    &quot;changed&quot;: false,
    &quot;msg&quot;: &quot;Failed to connect to the host via ssh: ssh: Could not resolve hostname host2: nodename nor servname provided, or not known&quot;,
    &quot;unreachable&quot;: true
}
</code></pre>

<p>其中 <code>all</code> 是 <code>&lt;host-pattern&gt;</code> ,  <code>all</code> 是一个特殊的 pattern，匹配所有 hosts。提示报错是预期的结果，因为 host1, host2 并不存在。</p>

<h3 id="toc_3">Playbook，Role，Task</h3>

<p>上一节展示了如何通过 ansible 执行 adhoc 命令，但是大部分时候我们用的更多的是 <code>ansible-playbook</code> 命令。Playbook 是 ansible 的任务编排语言，通过 YAML 来描述。关于 Playbook 和 role， task 的关系，一句话就可以说清楚：剧本 (playbook) 开始，角色们（role) 依次登上舞台，完成自己的任务 (task)。</p>

<p>playbook 一般是 ansible 执行的入口；role 更像是模块，是我们复用代码的基本单位；task 是一个个具体的实现。</p>

<p>先从一个不考虑复用的 playbook 开始：</p>

<pre><code class="language-yaml">- hosts: all
  tasks:
    - name: install vim
      yum:
        name: vim
        state: present

    - name: install jdk
      yum:
        name: openjdk
        state: present
</code></pre>

<p>这个 playbook 会在所有机器上安装 vim 和 openjdk。yum 是 ansible 提供的一个模块，ansible 拥有非常强大的生态，我们需要几乎所有操作都被 ansible 很好的封装了。所有模块的文档可以参考：<a href="https://docs.ansible.com/ansible/latest/modules/modules_by_category.html">https://docs.ansible.com/ansible/latest/modules/modules_by_category.html</a></p>

<p>随着 task 越来越多，playbook 会变得非常长而且不容易维护，中间如果有重复部分也难以实现代码复用。这个时候就轮到角色（role） 登场了。</p>

<h3 id="toc_4">编写可复用的脚本</h3>

<p>role 从代码上看就是一个特定结构的目录，典型的 role 目录结构如下：</p>

<pre><code class="language-yaml">site.yml
webservers.yml
fooservers.yml
roles/
   common/
     files/
     templates/
     tasks/
     handlers/
     vars/
     defaults/
     meta/
   webservers/
     files/
     templates/
     tasks/
     handlers/
     vars/
     defaults/
     meta/
</code></pre>

<p>每个目录下默认的入口都是 <code>main.yml</code> 这个文件。然后我们可以在 playbook 中引入这个 role：</p>

<pre><code class="language-yaml">---
- hosts: webservers
  roles:
     - common
     - webservers
</code></pre>

<p>role 和 role 之间可以设置依赖关系，通过依赖我们可以隐式的执行某些 role。角色依赖需要定义在 <code>meta/main.yml</code> 文件中：</p>

<pre><code class="language-yaml">---
dependencies:
  - { role: common, some_parameter: 3 }
  - { role: apache, port: 80 }
  - { role: postgres, dbname: blarg, other_parameter: 12 }
</code></pre>

<blockquote>
<p>当一个 playbook 包含多个 role，并且 role 依赖了共同的 role 时，可能会有重复执行的情况。ansible 有一些机制避免重复无意义的执行，但是该规则不是很容易直观理解。</p>
</blockquote>

<h2 id="toc_5">变量</h2>

<h3 id="toc_6"><strong>变量的定义</strong></h3>

<p><strong>Inventory File</strong></p>

<pre><code>[all:vars]
foo1=bar
foo2=bar2

[group1]
host1
host2

[group1:vars]
foo3=bar3
</code></pre>

<p><strong>PlayBook</strong></p>

<pre><code class="language-yaml">---
- hosts: all
  vars:
    http_port: 80
  vars_prompt:
    - name: service_name
      prompt: &quot;Which service do you want to remove?&quot;
      private: no
  vars_files:
    - /vars/external_vars.yml
</code></pre>

<p><strong>Role</strong></p>

<p>有些变量只用于当前 role，可能会定义在 role 当中。一般位于 <code>defaults/mian.yml</code> 或者 <code>vars/</code> 目录下。</p>

<p>具体的使用参见：<a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html">https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html</a></p>

<p><strong>Ansible 预定义</strong></p>

<p>部分变量是 Ansible 自己定义的变量，比如我们要获取机器的 hostname：</p>

<pre><code class="language-bash">{{ ansible_facts[&#39;nodename&#39;] }}
</code></pre>

<p>这部分预定义变量非常多，可以通过下面命令查看：</p>

<pre><code class="language-bash">$ ansible hostname -m setup
{
    &quot;ansible_all_ipv4_addresses&quot;: [
        &quot;REDACTED IP ADDRESS&quot;
    ],
    &quot;ansible_all_ipv6_addresses&quot;: [
        &quot;REDACTED IPV6 ADDRESS&quot;
    ],
    &quot;ansible_apparmor&quot;: {
        &quot;status&quot;: &quot;disabled&quot;
    },
    &quot;ansible_architecture&quot;: &quot;x86_64&quot;,
    &quot;ansible_bios_date&quot;: &quot;11/28/2013&quot;
        ......
}
</code></pre>

<p><strong>Command Line</strong></p>

<pre><code class="language-bash">ansible-playbook site.yml --extra-vars=&#39;{&quot;foo&quot;: &quot;bar&quot;}&#39;
</code></pre>

<p><a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html">https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html</a></p>

<h3 id="toc_7">变量的作用域</h3>

<p>ansible 的变量作用域主要有 3 种：</p>

<ul>
<li>Global：ansible 配置文件，环境变量，命令行</li>
<li>Play：playbook vars, vars_prompt, vars_files; role defaults, vars</li>
<li>Host: Inventory 中定义的的 host vars; facts</li>
</ul>

<h3 id="toc_8">变量使用</h3>

<p>ansible 允许你在各处通过 jinja2 的语法使用变量。jinja2 是一个用 Python 开发的模版引擎，本身并不复杂，核心东西就 3 个：变量的输出，控制流，filter</p>

<p>两个大括号包起来表示输出变量：</p>

<pre><code>I&#39;m {{ name }}
</code></pre>

<p>变量输出时可以用过 filter 控制格式，类似 bash 里的 pipeline。filter 本质上是一个 Python 的函数，有一个入参和一个返回结果：</p>

<pre><code>I&#39;m {{ name | trim | title }}
</code></pre>

<p>控制流需要区别于输出，用 <code>{% %}</code> 表示。比如一个 for 循环</p>

<pre><code>{% for name in names %}
I&#39;m {{ name | title }}
{% endfor %}
</code></pre>

<p>条件判断</p>

<pre><code>{% for name in names %}
{% if ignore_case %}
I&#39;m {{ name | lower }}
{% else %}
I&#39;m {{ name }}
{% endif %}
{% endif %}
</code></pre>

<p>上面就是 jinja2 的介绍，更多细节需要去看文档。</p>

<p>需要注意的是，在 Ansible 中如果你要使用 jinja2 的语法去引用一个变量，必须用双引号内使用。</p>

<pre><code class="language-yaml">- hosts: all
  vars:
    deploy_path: &quot;{{ home_dir }}/apps&quot;
</code></pre>

<p>比如我们想根据各种上下文生成 nginx 的配置文件，可以通过 template 命令来渲染。首先定一个模版文件 <code>nginx.conf.j2</code></p>

<pre><code>server {
  server_name {{ server_name }};
  listen 80;
  
  location / {
    try_files $uri $uri/ /index.html;
  }
}
</code></pre>

<p>我们希望这个配置文件可以覆盖默认的 nginx 配置：</p>

<pre><code class="language-yaml">- hosts: nginx
  vars:
    server_name: gio.com
    nginx_user: nginx
    nginx_group: &quot;{{ nginx_user }}&quot;
  tasks:
    - name: generate nginx config file
      template:
        src: nginx.conf.j2
        dest: /etc/nginx/nginx.conf
        owner: &quot;{{ nginx_user }}&quot;
        group: &quot;{{ nginx_group }}&quot;
      become: yes
</code></pre>

<h2 id="toc_9">最佳实践</h2>

<h3 id="toc_10">保持操作的幂等</h3>

<p>shell 脚本在执行时，很多命令的执行结果不是确定的。很多时候我们无法避免的需要反复重试某些脚本：</p>

<ul>
<li>脚本自身有 bug，修完以后需要重试</li>
<li>机器状态不确定。比如脚本执行过程中机器重启了；免密 sudo 忘记配置结果执行到某个需要 sudo 的 task 时跪了。</li>
<li>命令本身执行结果就是不确定的，比如通过 systemd 启动一个进程，systemctl start 命令返回成功并不代表真的启动成功了。</li>
</ul>

<p>ansible 的写法都是声明式而不是命令式。命令描述过程，声明式描述意图。明确的意图可以让 ansible 可以更好的帮你决定是否要重试某个任务，安全的隐藏细节。比如我们删除某个文件，命令式描述这样的：</p>

<pre><code class="language-yaml">- name: delete file
  command: rm /tmp/xxx
</code></pre>

<p>但是当我们重复跑这个 task 时，已经被删除文件无法再次被删除，需要在每次删除前检查目标文件是否已经被删除。但是用声明式的写法就很容易实现：</p>

<pre><code class="language-yaml">- name: delete file
  file:
    src: /tmp/xxx
    state: absent
</code></pre>

<p>编写 ansible 脚本时，要始终记得一件事：不要想着你要做什么操作，而是描述你期望某个对象的状态是怎么样的。</p>

<h3 id="toc_11">谨慎处理非 root 用户运行时的逻辑</h3>

<p>ansible 的 task 有两个属性 <code>become</code> 和 <code>become_user</code> ，分别代表是否要使用 sudo 以及 sudo 的用户。比如创建一个目录并指定目录的 owner 和 group</p>

<pre><code class="language-yaml">- name: create dir
  file:
    path: /etc/foo
    state: directory
    owner: foo
    group: foo
</code></pre>

<p>如果我们以 root 用户的身份执行 ansible 脚本，上面的脚本没有任何问题。但是如果是一个有 sudo 权限的普通用户，如果没有显式使用 sudo 的话，没有权限在 <code>/etc</code> 目录下创建任何东西。这是时候就需要使用 <code>become</code></p>

<pre><code class="language-yaml">- name: create dir
  file:
    path: /etc/foo
    state: directory
    owner: foo
    group: foo
  become: yes
</code></pre>

<h3 id="toc_12">测试脚本</h3>

<p>检查语法：</p>

<pre><code class="language-yaml">ansible-playbook --syntax-check &lt;playbook&gt;
</code></pre>

<p>Dry-run:</p>

<pre><code class="language-yaml">ansible-playbook --check &lt;playbook&gt;
</code></pre>

<p>真实执行脚本：</p>

<p>由于测试可能需要反复执行，每次都申请服务器显然不现实，推荐本地用 VirtualBox + Vagrant 来进行测试。</p>

<p>首先定义 Vargrantfile，来创建 3 个虚拟机，hostname 是 hadoop[1-3]</p>

<pre><code class="language-yaml">$ cat Vagrantfile                                                                                                                                                    
# -*- mode: ruby -*-
# vi: set ft=ruby :

# All Vagrant configuration is done below. The &quot;2&quot; in Vagrant.configure
# configures the configuration version (we support older styles for
# backwards compatibility). Please don&#39;t change it unless you know what
# you&#39;re doing.
Vagrant.configure(&quot;2&quot;) do |config|
  config.vm.define &#39;hadoop01&#39; do |hadoop01|
    hadoop01.vm.box = &#39;centos/7&#39;
    hadoop01.vm.hostname = &#39;hadoop01&#39;
    hadoop01.vm.network &quot;forwarded_port&quot;, guest: 80, host: 8000
    hadoop01.vm.network :private_network, :ip =&gt; &#39;192.168.10.192&#39;
    hadoop01.vm.provision :hosts, :sync_hosts =&gt; true
    hadoop01.vm.provision :hosts, :add_localhost_hostnames =&gt; false
    hadoop01.vm.provider :virtualbox do |v|
      v.customize [&quot;modifyvm&quot;, :id, &quot;--natdnshostresolver1&quot;, &quot;on&quot;]
      v.customize [&quot;modifyvm&quot;, :id, &quot;--memory&quot;, 2048]
      v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;hadoop01&quot;]
    end
  end

  config.vm.define &#39;hadoop02&#39; do |hadoop02|
    hadoop02.vm.box = &#39;centos/7&#39;
    hadoop02.vm.hostname = &#39;hadoop02&#39;
    hadoop02.vm.network &quot;forwarded_port&quot;, guest: 80, host: 8001
    hadoop02.vm.network :private_network, :ip =&gt; &#39;192.168.10.193&#39;
    hadoop02.vm.provision :hosts, :sync_hosts =&gt; true
    hadoop02.vm.provision :hosts, :add_localhost_hostnames =&gt; false
    hadoop02.vm.provider :virtualbox do |v|
      v.customize [&quot;modifyvm&quot;, :id, &quot;--natdnshostresolver1&quot;, &quot;on&quot;]
      v.customize [&quot;modifyvm&quot;, :id, &quot;--memory&quot;, 2048]
      v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;hadoop02&quot;]
    end
  end

  config.vm.define &#39;hadoop03&#39; do |hadoop03|
    hadoop03.vm.box = &#39;centos/7&#39;
    hadoop03.vm.hostname = &#39;hadoop03&#39;
    hadoop03.vm.network &quot;forwarded_port&quot;, guest: 80, host: 8002
    hadoop03.vm.network :private_network, :ip =&gt; &#39;192.168.10.194&#39;
    hadoop03.vm.provision :hosts, :sync_hosts =&gt; true
    hadoop03.vm.provision :hosts, :add_localhost_hostnames =&gt; false
    hadoop03.vm.provider :virtualbox do |v|
      v.customize [&quot;modifyvm&quot;, :id, &quot;--natdnshostresolver1&quot;, &quot;on&quot;]
      v.customize [&quot;modifyvm&quot;, :id, &quot;--memory&quot;, 2048]
      v.customize [&quot;modifyvm&quot;, :id, &quot;--name&quot;, &quot;hadoop03&quot;]
    end
  end
end
</code></pre>

<p>启动服务器并配置 ssh</p>

<pre><code class="language-yaml">$ vagrant up

$ vagrat ssh-config
</code></pre>

<p>创建一个用于测试的 inventory file，然后执行 playbook：</p>

<pre><code class="language-yaml">ansible-playbook &lt;playbook&gt; -i &lt;inventory_file&gt;
</code></pre>

<p>单元测试：</p>

<p>ansible 的单元测试比较接近集成测试。有第一个第三方的框架可以支持：</p>

<p><a href="https://molecule.readthedocs.io/en/latest/">https://molecule.readthedocs.io/en/latest/</a></p>

<p>细节比较多这里不单独介绍，执行也需要依赖 docker 或者 vagrant</p>

<h2 id="toc_13">Reference</h2>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Infrastructure_as_code">https://en.wikipedia.org/wiki/Infrastructure_as_code</a></li>
<li><a href="https://ansible-tran.readthedocs.io/en/latest/docs/intro.html">https://ansible-tran.readthedocs.io/en/latest/docs/intro.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hive 锁机制]]></title>
    <link href="http://blog.lfyzjck.com/15680871440612.html"/>
    <updated>2019-09-10T11:45:44+08:00</updated>
    <id>http://blog.lfyzjck.com/15680871440612.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">背景</h2>

<p>Hive 锁机制是为了让 Hive 支持并发读写而设计的 feature，另外要解决并发读写的情况下”脏读“ （Read uncommited）的问题。脏读的问题本身通过实现了原子的 reader/writer 已经得到解决（<a href="https://issues.apache.org/jira/browse/HIVE-829">https://issues.apache.org/jira/browse/HIVE-829</a>）和锁机制并不绑定。</p>

<h2 id="toc_1">锁机制</h2>

<p>Hive 内部定义了两种类型的锁：</p>

<ul>
<li>共享锁(Share)</li>
<li>互斥锁(Exclusive)</li>
</ul>

<p>不同锁之间的兼容性入下面表格：</p>

<table>
<thead>
<tr>
<th>Lock Compatibility</th>
<th>Existing Lock（S）</th>
<th>Existing Lock（X）</th>
</tr>
</thead>

<tbody>
<tr>
<td>Requested Lock（S）</td>
<td>True</td>
<td>False</td>
</tr>
<tr>
<td>Requested Lock（X）</td>
<td>False</td>
<td>False</td>
</tr>
</tbody>
</table>

<p>锁的基本机制是：</p>

<ul>
<li>元信息和数据的变更需要互斥锁</li>
<li>数据的读取需要共享锁</li>
</ul>

<p>根据这个机制，Hive 的一些场景操作对应的锁级别如下：</p>

<table>
<thead>
<tr>
<th>Hive command</th>
<th>Locks Acquired</th>
</tr>
</thead>

<tbody>
<tr>
<td>select .. T1 partition P1</td>
<td>S on T1, T1.P1</td>
</tr>
<tr>
<td>insert into T2(partition P2) select .. T1 partition P1</td>
<td>S on T2, T1, T1.P1 and X on T2.P2</td>
</tr>
<tr>
<td>insert into T2(partition P.Q) select .. T1 partition P1</td>
<td>S on T2, T2.P, T1, T1.P1 and X on T2.P.Q</td>
</tr>
<tr>
<td>alter table T1 rename T2</td>
<td>X on T1</td>
</tr>
<tr>
<td>alter table T1 add cols</td>
<td>X on T1</td>
</tr>
<tr>
<td>alter table T1 replace cols</td>
<td>X on T1</td>
</tr>
<tr>
<td>alter table T1 change cols</td>
<td>X on T1</td>
</tr>
<tr>
<td>alter table T1 concatenate</td>
<td>X on T1</td>
</tr>
<tr>
<td>alter table T1 add partition P1</td>
<td>S on T1, X on T1.P1</td>
</tr>
<tr>
<td>alter table T1 drop partition P1</td>
<td>S on T1, X on T1.P1</td>
</tr>
<tr>
<td>alter table T1 touch partition P1</td>
<td>S on T1, X on T1.P1</td>
</tr>
<tr>
<td>alter table T1 set serdeproperties</td>
<td>S on T1</td>
</tr>
<tr>
<td>alter table T1 set serializer</td>
<td>S on T1</td>
</tr>
<tr>
<td>alter table T1 set file format</td>
<td>S on T1</td>
</tr>
<tr>
<td>alter table T1 set tblproperties</td>
<td>X on T1</td>
</tr>
<tr>
<td>alter table T1 partition P1 concatenate</td>
<td>X on T1.P1</td>
</tr>
<tr>
<td>drop table T1</td>
<td>X on T1</td>
</tr>
</tbody>
</table>

<p>Hive 锁在 zookeeper 上会对应 ephemeral 的节点，避免释放锁失败导致死锁</p>

<h2 id="toc_2">调试锁🔐</h2>

<p>可以通过下面命令查看某个表或者分区的锁</p>

<ul>
<li>SHOW LOCKS <TABLE_NAME>;</li>
<li>SHOW LOCKS <TABLE_NAME> EXTENDED;</li>
<li>SHOW LOCKS <TABLE_NAME> PARTITION (<PARTITION_DESC>);</li>
<li>SHOW LOCKS <TABLE_NAME> PARTITION (<PARTITION_DESC>) EXTENDED;</li>
</ul>

<p>See also <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Explain#LanguageManualExplain-TheLOCKSClause">EXPLAIN LOCKS</a>.</p>

<h2 id="toc_3">关闭锁机制</h2>

<p>可以通过设置 <code>hive.support.concurrency=fasle</code> 来解决</p>

<p>关闭锁机制会造成下面影响：</p>

<ul>
<li>并发读写同一份数据时，读操作可能会随机失败</li>
<li>并发写操作的结果在随机出现，后完成的任务覆盖之前完成任务的结果</li>
<li>SHOW LOCKS， UNLOCK TABLE 会报错</li>
</ul>

<h2 id="toc_4">HiveLockManager 的实现</h2>

<p>在关闭 Hive 锁的过程中，发现粗暴的禁用 concurrency 会导致 UNLOCK TABLE 语法报错。一些遗留系统已经依赖这个语法来确保自身任务不被阻塞，这样的修改会导致这些程序出现问题。于是转而研究有没有其他简单锁的实现可以达到类似的效果。粗看 Hive 的代码找到这 3  种实现：</p>

<ul>
<li>DbLockManager 配合 DbTxnManager 用于在 Hive 中实现事务，不能单独使用</li>
<li>EmbeddedLockManager HiveServer 级别基于内存实现的锁</li>
<li>ZooKeeperHiveLockManager 默认的 LockManager 实现，基于 zookeeper 实现的分布式协调锁</li>
</ul>

<h2 id="toc_5">Hive Zookeeper 锁泄露问题</h2>

<p>在 cancel Hive 查询时，有概率发生 Zookeeper 锁释放失败的问题。因为 Hive 的锁在Zookeeper 是持久节点，累计的锁释放失败可能造成 Zookeeper 的 Node 数量过多，影响 Zookeeper 的性能。社区有对应的 ISSUE，该问题在 2.3.0 版本才被 FIX: <a href="https://issues.apache.org/jira/browse/HIVE-15997">https://issues.apache.org/jira/browse/HIVE-15997</a></p>

<p>HiveServer 上可以发现类似日志，就是锁释放失败的标志：</p>

<pre><code>2019-03-06T07:41:56,556 ERROR [HiveServer2-Background-Pool: Thread-45399] ZooKeeperHiveLockManager: Failed to release ZooKeeper lock:
java.lang.InterruptedException
        at java.lang.Object.wait(Native Method) ~[?:1.8.0_45]
        at java.lang.Object.wait(Object.java:502) ~[?:1.8.0_45]
        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1342) ~[zookeeper-3.4.5-cdh5.5.0.jar:3.4.5-cdh5.5.0--1]
        at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:871) ~[zookeeper-3.4.5-cdh5.5.0.jar:3.4.5-cdh5.5.0--1]
        at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:239) ~[curator-framework-2.6.0.jar:?]
        at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:234) ~[curator-framework-2.6.0.jar:?]
        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[curator-client-2.6.0.jar:?]
        at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230) ~[curator-framework-2.6.0.jar:?]
        at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:215) ~[curator-framework-2.6.0.jar:?]
        at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:42) ~[curator-framework-2.6.0.jar:?]
        at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlockPrimitive(ZooKeeperHiveLockManager.java:474) [hive-exec-2.1.1.jar:2.1.1]
        at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlockWithRetry(ZooKeeperHiveLockManager.java:452) [hive-exec-2.1.1.jar:2.1.1]
        at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlock(ZooKeeperHiveLockManager.java:440) [hive-exec-2.1.1.jar:2.1.1]
        at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.releaseLocks(ZooKeeperHiveLockManager.java:222) [hive-exec-2.1.1.jar:2.1.1]
        at org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.releaseLocks(DummyTxnManager.java:188) [hive-exec-2.1.1.jar:2.1.1]
        at org.apache.hadoop.hive.ql.Driver.releaseLocksAndCommitOrRollback(Driver.java:1136) [hive-exec-2.1.1.jar:2.1.1]
        at org.apache.hadoop.hive.ql.Driver.rollback(Driver.java:1516) [hive-exec-2.1.1.jar:2.1.1]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1456) [hive-exec-2.1.1.jar:2.1.1]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1171) [hive-exec-2.1.1.jar:2.1.1]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1166) [hive-exec-2.1.1.jar:2.1.1]
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:242) [hive-service-2.1.1.jar:2.1.1]
        at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91) [hive-service-2.1.1.jar:2.1.1]
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:334) [hive-service-2.1.1.jar:2.1.1]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_45]
        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_45]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671) [hadoop-common-2.6.0-cdh5.5.0.jar:?]
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:347) [hive-service-2.1.1.jar:2.1.1]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_45]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_45]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]
</code></pre>

<p>锁泄露除了修复这个 ISSUE 以外比较难处理。在公司中，如果有成熟的调度器协调任务的依赖关系，那么非常建议禁用掉 Hive 的锁机制。在表数量众多，分区众多的场景下，使用 Zookeeper 的代价也是非常高的。</p>

<h2 id="toc_6">参考资料</h2>

<ul>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/Locking">https://cwiki.apache.org/confluence/display/Hive/Locking</a></li>
<li><a href="https://issues.apache.org/jira/browse/HIVE-1293">https://issues.apache.org/jira/browse/HIVE-1293</a></li>
<li><a href="https://issues.apache.org/jira/browse/HIVE-15997">https://issues.apache.org/jira/browse/HIVE-15997</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[大数据平台的数据同步服务实践]]></title>
    <link href="http://blog.lfyzjck.com/15418152524286.html"/>
    <updated>2018-11-10T10:00:52+08:00</updated>
    <id>http://blog.lfyzjck.com/15418152524286.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">引言</h2>

<p>在大数据系统中，我们往往无法直接对在线系统中的数据直接进行检索和计算。在线系统所使用关系型数据库，缓存存储数据的方式都非常不同，很多系统不适合 OLAP 式的查询，也不允许 OLAP 查询影响到在线业务的稳定性。从数仓建设的角度思考，稳定规范的数仓必定依赖于稳定和规范的数据源，数据需要经过采集加工后才能真正被数仓所使用。推动数据同步服务的平台化，才有可能从源头规范数据的产出。数据同步服务不像数据挖掘一样可以直接产生价值，但它更像是连接不同存储的高速公路，好的同步工具可以很大程度上提升数据开发的效率。</p>

<p>本文主要介绍知乎在数据同步这方面的建设，工具选型和平台化的实践。</p>

<h2 id="toc_1">业务场景及架构</h2>

<p>在线业务的数据库在知乎内部还是以 MySQL 和 HBase 为主，所以数据源方面主要考虑 MySQL 和 Hive 的互相同步，后续可以支持 HBase。早期数据同步使用 Oozie + Sqoop 来完成，基本满足业务需求。但是随着数仓任务不断变多，出现了很多重复同步的例子，对同步任务的负载管理也是空白。凌晨同步数据导致 MySQL 不断报警，DBA 苦不堪言。对于业务来说，哪些表已经被同步了，哪些表还没有也是一个黑盒子，依赖其他业务方的数据都只能靠口头的约定。为了解决这些问题，决定对数据同步做一个统一的平台，简化同步任务的配置，调度平衡负载，管理元信息等等。</p>

<h2 id="toc_2">技术选型</h2>

<p>数据同步工具市面上有很多解决方案，面向批的主要有 <a href="http://sqoop.apache.org/">Apache Sqoop</a> 和阿里开源的 <a href="https://github.com/alibaba/DataX">DataX</a>，下面主要对比这两种数据同步工具。</p>

<h5 id="toc_3">Sqoop</h5>

<p>Pros：</p>

<ul>
<li>基于 MapReduce 实现，容易并行和利用现有集群的计算资源</li>
<li>和 Hive 兼容性好，支持 Parquet，ORC 等格式</li>
<li>支持自动迁移 Schema</li>
<li>社区强大，遇到的问题容易解决</li>
</ul>

<p>Cons：</p>

<ul>
<li>支持的数据源不算太丰富（比如 ES），扩展难度大</li>
<li>不支持限速，容易对 MySQL 造成压力</li>
</ul>

<h5 id="toc_4">DataX</h5>

<p>Pros：</p>

<ul>
<li>支持的数据源丰富尤其是支持从非关系型数据库到关系型数据库的同步</li>
<li>支持限速</li>
<li>扩展方便，插件开发难度低</li>
</ul>

<p>Cons：</p>

<ul>
<li>需要额外的运行资源，当任务比较多的时候费机器</li>
<li>没有原生支持导出到 Hive，需要做很多额外的工作才能满足需求</li>
</ul>

<p>考虑到同步本身要消耗不少的计算和带宽资源，Sqoop 可以更好的利用 Hadoop 集群的资源，而且和 Hive 适配的更好，最终选择了 Sqoop 作为数据同步的工具。</p>

<h2 id="toc_5">平台化及实践</h2>

<p>平台化的目标是构建一个相对通用的数据同步平台，更好的支持新业务的接入，和公司内部的系统集成，满足业务需求。平台初期设计的目标有以下几个：</p>

<ul>
<li>简单的任务配置界面，方便新的任务接入</li>
<li>监控和报警</li>
<li>屏蔽 MySQL DDL 造成的影响</li>
<li>可扩展新数据源</li>
</ul>

<h3 id="toc_6">简化任务接入</h3>

<p>平台不应该要求每个用户都理解底层数据同步的原理，对用户而言，应该是描述数据源 (Source) 和目标存储(Sink)，还有同步周期等配置。所有提供的同步任务应该经过审核，防止未经许可的数据被同步，或者同步配置不合理，增加平台负担。最后暴露给用户的 UI 大概如下图。</p>

<p><img src="media/15418152524286/15421169494010.jpg" alt="15421169494010" style="width:600px;"/></p>

<h3 id="toc_7">增量同步</h3>

<p>对于数据量非常大的数据源，如果每次同步都是全量，对于 MySQL 的压力会特别大，同步需要的时间也会很长。因此需要一种可以每次只同步新增数据的机制，减少对于 MySQL 端的压力。但是增量同步不是没有代价的，它要求业务在设计表结构的时候，满足一些约束：</p>

<ul>
<li>业务对数据没有物理的删除操作，而是采用类似标记删除的机制</li>
<li>数据没有 UPDATE （类似日志） 或者有 UPDATE 但是提供 updated_at 来标记每一行最后一次更新的时间</li>
</ul>

<p>对于满足上面条件，数据量比较大的表就可以采用增量同步的方式拉取。小数据量的表不需要考虑增量同步，因为数据和合并也需要时间，如果收益不大就不应该引入额外的复杂性。一个经验值是行数 &lt;= 2000w 的都属于数据量比较小的表，具体还取决于存储的数据内容（比如有很多 Text 类型的字段）。</p>

<h3 id="toc_8">处理 Schema 变更</h3>

<p>做数据同步永远回避不掉的一个问题就是 Schema 的变更，对 MySQL 来说，Schema 变更就是数据库的 DDL 操作。数据同步平台应该尽可能屏蔽 MySQL DDL 对同步任务的影响，并且对兼容的变更，及时变更推送到目标存储。</p>

<p>数据同步平台会定时的扫描每个同步任务上游的数据源，保存当前 Schema 的快照，如果发现 Schema 发生变化，就通知下游做出一样的变更。绝大部分的 DDL 还是增加字段，对于这种情况数据同步平台可以很好屏蔽变更对数仓的影响。对于删除字段的操作原则上禁止的，如果一定要做，需要走变更流程，通知到依赖该表的业务方，进行 Schema 同步的调整。</p>

<h3 id="toc_9">存储格式</h3>

<p>Hive 默认的格式是 Textfile，这是一种类似 CSV 的存储方式，但是对于 OLAP 查询来说性能并不友好。通常我们会选择一些列式存储提高存储和检索的效率。Hive 中比较成熟的列式存储格式有 Parquet 和 ORC。这两个存储的查询性能相差不大，但是 ORC 和 Hive 集成更好而且对于非嵌套数据结构查询性能是优于 Parquet 的。但是知乎内部因为也用了 Impala，早期的 Impala 版本不支持 ORC 格式的文件，为了兼容 Impala 最终选择了 Parquet 作为默认的存储格式。</p>

<p>关于列式存储的原理和 Benchmark，可以参考这个 <a href="https://www.slideshare.net/oom65/file-format-benchmarks-avro-json-orc-parquet">Slide</a></p>

<h3 id="toc_10">负载管理</h3>

<p>当同步任务越来越多时，单纯的按照任务启动时间来触发同步任务已经不能满足需求。数据同步应该保证对于线上业务没有影响，在此基础上速度越快越好。本质上是让 Sqoop 充分利用 MySQL 节点的 iops。要避免对线上服务的影响，对于需要数据同步的库单独建立一个从节点，隔离线上流量。初次之外，需要一个调度策略来决定一个任务何时执行。由于任务的总数量并不多，但是每个任务可能会执行非常长的时间，最终决定采用一个中央式的调度器，调度器的状态都持久化在数据库中，方便重启或者故障恢复。最终架构图如下</p>

<p><img src="media/15418152524286/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E8%B0%83%E5%BA%A6%E5%99%A8.jpg" alt="数据同步调度器" style="width:336px;"/></p>

<p>最终任务的调度流程如下：</p>

<ol>
<li>每个 MySQL 实例是调度器的一个队列，根据同步的元信息决定该任务属于哪个队列</li>
<li>根据要同步数据量预估资源消耗，向调度器申请资源</li>
<li>调度器将任务提交到执行队列，没有意外的话会立刻开始执行</li>
<li>Monitor 定时向调度器汇报 MySQL 节点的负载，如果负载过高就停止向该队列提交新的任务</li>
<li>任务结束后从调度器归还资源</li>
</ol>

<h3 id="toc_11">性能优化</h3>

<h5 id="toc_12">针对不同的数据源选择合适的并发数</h5>

<p>Sqoop 是基于 MapReduce 实现的，提交任务前先会生成 MapReduce 代码，然后提交到 Hadoop 集群。Job 整体的并发度就取决于 Mapper 的个数。Sqoop 默认的并发数是 4，对于数据量比较大的表的同步显然是不够的，对于数据量比较小的任务又太多了，这个参数一定要在运行时根据数据源的元信息去动态决定。</p>

<h5 id="toc_13">优化 <a href="https://community.hortonworks.com/questions/79556/what-is-distributed-cache-in-hadoop.html">Distributed Cache</a> 避免任务启动对 HDFS 的压力</h5>

<p>在平台上线后，随着任务越来越多，发现如果 HDFS 的性能出现抖动，对同步任务整体的执行时间影响非常大，导致夜间的很多后继任务受到影响。开始推测是数据写入 HDFS 性能慢导致同步出现延时，但是任务大多数会卡在提交阶段。随着进一步排查，发现 MapReduce 为了解决不同作业依赖问题，引入了 Distributed Cache 机制可以将 Job 依赖的 lib 上传到 HDFS，然后再启动作业。Sqoop 也使用了类似的机制，会依赖 Hive 的相关 lib，这些依赖加起来有好几十个文件，总大小接近 150MB，虽然对于 HDFS 来说是很小数字，但是当同步任务非常多的时候，集群一点点的性能抖动都会导致调度器的吞吐大幅度下降，最终同步的产出会有严重延时。最后的解决方法是将 Sqoop 安装到集群中，然后通过 Sqoop 的参数 <code>--skip-distcache</code> 避免在任务提交阶段上传依赖的 jar。</p>

<h5 id="toc_14">关闭推测执行（Speculative Execution）</h5>

<p>所谓推测执行是这样一种机制：在集群环境下运行 MapReduce，一个 job 下的多个 task 执行速度不一致，比如有的任务已经完成，但是有些任务可能只跑了10%，这些任务将会成为整个 job 的短板。推测执行会对运行慢的 task 启动备份任务，然后以先运行完成的 task 的结果为准，kill 掉另外一个 task。这个策略可以提升 job 的稳定性，在一些极端情况下加快 job 的执行速度。</p>

<p>Sqoop 默认的分片策略是按照数据库的主键和 Mapper 数量来决定每个分片拉取的数据量。如果主键不是单调递增或者递增的步长有大幅波动，分片就会出现数据倾斜。对于一个数据量较大的表来说，适度的数据倾斜是一定会存在的情况，当 Mapper 结束时间不均而触发推测执行机制时，MySQL 的数据被重复且并发的读取，占用了大量 io 资源，也会影响到其他同步的任务。在一个 Hadoop 集群中，我们仍然认为一个节点不可用导致整个 MapReduce 失败仍然是小概率事件，对这种错误，在调度器上增加重试就可以很好的解决问题而不是依赖推测执行机制。</p>

<h3 id="toc_15">监控和报警</h3>

<p>根据 <a href="http://ylzheng.com/2018/02/02/monitor-best-praticase4-golden-signals/"><strong>USE</strong></a> 原则，大概整理出下面几个需要监控的指标：</p>

<ul>
<li>MySQL 机器的负载，IOPS，出入带宽</li>
<li>调度队列长度，Yarn 提交队列长度</li>
<li>任务执行错误数</li>
</ul>

<p>报警更多是针对队列饱和度和同步错误进行的</p>

<h3 id="toc_16">和离线作业调度器集成</h3>

<h2 id="toc_17">展望</h2>

<p>数据同步发展到比较多的任务后，新增的同步任务越来越多，删除的速度远远跟不上新增的速度，总体来说同步的压力会越来越大，需要一个更好的机制去发现无用的同步任务并通知业务删除，减轻平台的压力。</p>

<p>另外就是数据源的支持不够，Hive 和 HBase、ElasticSearch 互通已经成了一个呼声很强烈的需求。Hive 虽然可以通过挂外部表用 SQL 的方式写入数据，但是效率不高有很难控制并发，很容易影响到线上集群，需要有更好的实现方案才能在生产环境真正的运行起来。</p>

<p>另外这里没有谈到的一个话题就是流式数据如何做同步，一个典型的场景就是 Kafka 的日志实时落地然后实时进行 OLAP 的查询，或者通过 MySQL binlog 实时更新 ElasticSearch 的索引。关于这块的基础设置知乎也在快速建设中，非常欢迎感兴趣同学投简历到 <a href="mailto:ck@zhihu.com">ck@zhihu.com</a> ，加入知乎大数据计算平台组。</p>

<h2 id="toc_18">参考资料</h2>

<ul>
<li><a href="https://chu888chu888.gitbooks.io/hadoopstudy/content/Content/9/datax/datax.html">Datax 性能对比</a></li>
<li><a href="https://blog.csdn.net/zhaodedong/article/details/54177686">漫谈数据仓库之拉链表</a></li>
<li><a href="https://orc.apache.org/">Apache ORC</a></li>
<li><a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html">Apache Sqoop</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sqoop 使用指南]]></title>
    <link href="http://blog.lfyzjck.com/15307956203332.html"/>
    <updated>2018-07-05T21:00:20+08:00</updated>
    <id>http://blog.lfyzjck.com/15307956203332.html</id>
    <content type="html"><![CDATA[
<p>Sqoop 是一个数据同步工具，用于关系型数据库和各种大数据存储比如 Hive 之间的数据相互同步。Sqoop 因为它的使用便利得到了广泛使用。类似的工具还有阿里开源的 <a href="https://github.com/alibaba/DataX">DataX</a> 和其他商业工具。</p>

<p><a href="http://sqoop.apache.org/docs/1.99.7/index.html">Sqoop 2.0</a> 主要解决 Sqoop 1.x 扩展难的问题，提出的 Server-Client 模型，具体用的不是特别多。本文主要介绍的还是 Sqoop 1.x，最新的 Sqoop 版本是 1.4.7</p>

<h3 id="toc_0">安装</h3>

<p>Sqoop 安装需要依赖 Hadoop 和 Hive，以 Debain 为例，安装 Sqoop 也比较简单。</p>

<pre><code class="language-bash">apt-get install hadoop hive hive-hbase hive-hcatalog sqoop
</code></pre>

<p>除此之外，针对不同的数据源，需要不同的 JDBC Driver，这个是 Sqoop 默认没有自带的库，需要自行安装。比如 MySQL 的 Driver 是 <code>mysql-connector-java-5.1.13-bin.jar</code> ，确保 Jar 包在 Sqoop 的 classpath 内就行。</p>

<h3 id="toc_1">数据源</h3>

<p>Sqoop 支持非常多的数据源，理论上所有支持 JDBC 的数据源都可以作为 Sqoop 的数据源。最常见的场景还是从关系型数据（RDBMS）导入到 Hive, HBase 或者 HDFS。</p>

<p>Sqoop 的扩展性没有想象中的那么好，但是因为大部分企业的数据仓库还是构建在传统的 Hive 和 HBase 之上的，Sqoop 还是可以满足 80% 的数据同步需求的。</p>

<p>一个简单以 MySQL 作为上游数据源的同步：</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
  --username dbuser --password &quot;&quot; 
</code></pre>

<p>Sqoop 支持将数据同步到 HDFS 或者直接到 Hive：</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
  --username dbuser --password &quot;&quot; --table employees \
  --hive-import --hive-overwrite \
  --hive-database employees --hive-table employees
</code></pre>

<h3 id="toc_2">存储格式</h3>

<p>存储格式主要是 Hive 的概念，但是对于数据同步来讲，格式的选择会影响同步数据，类型系统的兼容性等等，我们必须予以关注。参考下面的表格：</p>

<table>
<thead>
<tr>
<th></th>
<th>压缩比</th>
<th>预计算</th>
<th>类型兼容性</th>
</tr>
</thead>

<tbody>
<tr>
<td>TextFile</td>
<td>无</td>
<td>否</td>
<td>一般</td>
</tr>
<tr>
<td>SequenceFile</td>
<td>中</td>
<td>否</td>
<td>一般</td>
</tr>
<tr>
<td>Parquet</td>
<td>高</td>
<td>是（sqoop 依赖的版本 feature 不完整）</td>
<td>好</td>
</tr>
<tr>
<td>ORC</td>
<td>高</td>
<td>是</td>
<td>好</td>
</tr>
</tbody>
</table>

<p>Hive 默认的存储格式是 TextFile，TextFile 类似一个 CSV 文件，使用不可见服务分割列，同步后的数据可读性比较好。但是因为所有数据都是按文本存储的，对于某些类型（比如 blob/bit ）无法支持。</p>

<p>Parquet/ORC 都是列式存储格式，这里不多介绍。在生产环境中更倾向于选择 Parquet/ORC ，节省空间的同时在 Hive 上的查询速度也更快。</p>

<p>同步为 Parquet 格式：</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
 --username dbuser --password &quot;&quot; --table employees \
 --hive-import --hive-overwrite \
 --hive-database employees --hive-table employees \
 --as-parquetfile
</code></pre>

<p>如果要导出为 ORC 格式，需要借助 Hive 提供的一个组件 HCatalog，同步语法也稍稍不太一样</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
 --username dbuser --password &quot;&quot; --table employees \
 --drop-and-create-hcatalog-table \
 --hcatalog-database employees --hcatalog-table employees \
 --hcatalog-storage-stanza &quot;STORED AS ORC&quot;
</code></pre>

<p>Parquet 理论上也可以通过这种方式同步，不过实测当前 Sqoop 版本 (1.4.7) 还是有 BUG，还是等等吧。</p>

<h3 id="toc_3">类型的兼容性</h3>

<p>由于数据源支持的类型和 Hive 本身可能不太一样，所以必然存在类型转换的问题。实际在使用过程中也是非常头疼的一件事。对于 Hive 来说，支持的类型取决于采用的存储格式。以 MySQL 为例，当存储格式为 Hive 时，基本的类型映射如下：</p>

<pre><code>MySQL(bigint) --&gt; Hive(bigint) 
MySQL(tinyint) --&gt; Hive(tinyint) 
MySQL(int) --&gt; Hive(int) 
MySQL(double) --&gt; Hive(double) 
MySQL(bit) --&gt; Hive(boolean) 
MySQL(varchar) --&gt; Hive(string) 
MySQL(decimal) --&gt; Hive(double) 
MySQL(date/timestamp) --&gt; Hive(string)
</code></pre>

<p>这里的类型映射并不完全准确，因为还取决于目标存储格式支持的类型。</p>

<p>由于 Text 格式非常类似 CSV，使用文本存储所有数据，对于 <code>Binary/Blob</code> 这样的类型就无法支持。Parquet/ORC/Avro 因为引入了序列化协议，本身存储是基于二进制的，所以可以支持绝大部分类型。</p>

<p>如果你在使用 TextFile 需要注意下面的问题：</p>

<ul>
<li>上游数据源中的 <code>NULL</code> 会被转化为字符串的 <code>NULL</code>, Hive 中的 <code>NULL</code> 用 <code>\N</code> 表示</li>
<li>如果内容中含有换行符，同步到 Hive 中会被当做独立的两行来处理，造成查询结果和实际数据不相符</li>
</ul>

<p>处理方法比较简单</p>

<p>如果在使用 Parquet，要注意 Sqoop 自带的 Parquet 库版本比较旧，不支持 DateTime/Timestamp 类型的数据，而是会用一个表示 ms 的 BIGINT 来代替，分析数据的时候应该注意这点。</p>

<h3 id="toc_4">数据校验</h3>

<p>Sqoop 内建有 validate 机制，只能验证单表的 row count: <a href="https://sqoop.apache.org/docs/1.4.3/SqoopUserGuide.html#validation">Sqoop User Guide (v1.4.3)</a></p>

<h3 id="toc_5">增量导入</h3>

<p>对于数据量很大的库，全量同步会非常痛，但是如果可以选择还是尽可能的选择全量同步，这种同步模式对数据一致性的保证最好，没有状态。如果不得不进行增量同步，可以继续往后看。</p>

<p>增量导入对业务是有一定侵入的，Schema 的设计和数据写入模式需要遵守一定的规范：</p>

<ul>
<li>增量同步表，最好有一个 Primary Key ，最好是单调递增的 ID</li>
<li>数据的写入模式满足下面两种情形之一

<ul>
<li>（Append）表的内容类似日志，一次写入不做修改和删除</li>
<li>（LastModified）表的内容有修改和删除，但是删除操作是逻辑删除，比如用 <code>is_deleted</code> 字段标识，并且有一个最后更新的时间戳比如 <code>updated_at</code>，<code>updated_at</code> 上有索引。</li>
</ul></li>
</ul>

<p>增量的数据同步大致分为 2 个阶段：读取增量数据和合并数据。对 Sqoop 来说，增量同步需要 <a href="http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_literal_sqoop_metastore_literal">sqoop-metastore</a> 的支持，用于保存上次同步的位置。</p>

<p>比如对于 Append 模式，假设我们有一张表叫 <code>employees</code>，Primary Key 是 <code>id</code>，上一次同步到 <code>id &lt;= 10000</code> 的数据：</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
  --username dbuser --password &quot;&quot; --table employees \
  --target-dir &lt;path/to/hive/table/location&gt; \
  --incremental append --check-column id --last-value 10000
</code></pre>

<p>我们直接将数据 load 到了 Hive 的表空间里，Hive 可以直接查询到最新增量的数据。<br/>
对 LastModified 模式会稍微复杂一些，除了加载增量数据，还涉及数据合并的问题，这里唯一的主键就特别重要了。</p>

<pre><code class="language-bash">sqoop import --connect jdbc:mysql://database.example.com/employees \
  --username dbuser --password &quot;&quot; --table employees \
  --target-dir &lt;path/to/hive/table/location&gt; \
  --incremental lastmodified --check-column updated_at --last-value &#39;2018-07-05 00:00:00&#39;
</code></pre>

<p>Sqoop 会在同步结束后再启动一个 merge 任务对数据去重，如果表太小，可能 merge 的代价比全量同步的还要高，我们就要慎重考虑全量同步是不是值得了。</p>

<blockquote>
<p>由于 HDFS 不支持修改文件，sqoop 的 <code>--incremental</code> 和 <code>--hive-import</code> 不能同时使用</p>
</blockquote>

<p>Sqoop 也提供了单独的 <code>sqoop merge</code> 工具，我们也可以分开进行 import 和 merge 这两个步骤。</p>

<h3 id="toc_6">加速同步</h3>

<p>这个小节讨论一下如何加快 Sqoop 的同步速度，Sqoop 同步速度大致取决于下面的几个因素：</p>

<ul>
<li>数据源的读取速度</li>
<li>HDFS 写入速度</li>
<li>数据倾斜程度</li>
</ul>

<h5 id="toc_7">数据源的读取速度</h5>

<p>如果上游数据源是 MySQL，可以考虑更换 SSD，保证 MySQL 实例的负载不要太高。除此之外，Sqoop 可以通过参数控制并发读取的 Mapper 个数加快读取速度。</p>

<pre><code class="language-bash">sqoop -m &lt;mapper_num&gt; ......
</code></pre>

<p>注意 <code>-m</code> 并不是越大越高，并发数过高会把数据库实例打死，同步速度反而变慢。</p>

<p>Sqoop 默认会通过 jdbc 的 API 来读取数据，但是可以通过参数控制使用 MySQL 自己的 <code>mysqldump</code> 来导出数据，这种方式比 jdbc 快一些，缺点是你不能选择要同步的列。另外只能支持目标格式为 Textfile。比较局限但是特定情况下还是很好使的。</p>

<h5 id="toc_8">HDFS 写入速度</h5>

<p>这个除了刚刚提供的控制并发数，还需要保证 Yarn 分配给 Sqoop 的资源充足，不要让资源成为同步的瓶颈。另外，当我们选择 Parquet/ORC 作为存储格式时，数据在写入的时候需要做大量的预计算，这个过程是比较消耗 CPU 和内存的，我们可以控制 MapReduce 参数，适当提高 Sqoop 的资源配额。</p>

<pre><code class="language-bash">sqoop -Dmapreduce.map.cpu.vcores=4 -Dmapreduce.map.memory.mb=8192 ...
</code></pre>

<h5 id="toc_9">数据倾斜</h5>

<p>Sqoop 默认的导入策略是根据主键进行分区导入的，具体的并发粒度取决于 <code>-m</code> 参数。如果主键不连续出现大幅度跳跃，就会导致 Sqoop 导入的时候出现严重的数据倾斜。比如某张表的主键分布是这样的：</p>

<pre><code>1
2
3
...
1000
1001
100000
100001
</code></pre>

<p>Sqoop 计算每个 Mapper 读取的数据范围的时候，会遵循很简单的公式计算：</p>

<pre><code>range = (max(pk) - min(pk)) / mapper
</code></pre>

<p>几乎出现所有的数据 load 都集中在第一个 mapper 上，整体同步相当于没有并发。</p>

<p>参考阅读：</p>

<ul>
<li><a href="https://blog.csdn.net/mike_h/article/details/50148309">Hive 数据倾斜 (Data Skew) 总结 - CSDN博客</a></li>
<li><a href="http://abhinaysandeboina.blogspot.hk/2017/08/avoiding-data-skew-in-sqoop.html">Avoiding Data Skew in Sqoop</a></li>
<li><a href="https://docs.qingcloud.com/guide/sqoop.html">Sqoop 指南 — QingCloud  文档</a></li>
</ul>

<h3 id="toc_10">导出</h3>

<p>Sqoop 支持将 Hive 的数据导出到 MySQL，方便在线系统调用。</p>

<pre><code>sqoop export --connect jdbc:mysql://database.example.com/employees --table employees --username dbuser --password &quot;&quot; --relaxed-isolation --update-key id --update-mode allowinsert --hcatalog-database employees --hcatalog-table employees
</code></pre>

<p>借助 HCatalog 可以比较轻松的将 Hive 表的数据直接导出到 MySQL。更多的详情参考官方文档，这里不多介绍。</p>

<h3 id="toc_11">更进一步</h3>

<p>如果我们要同步的数据非常多，管理同步任务本身就变成了一件复杂的事情。我们不仅要考虑源数据库的负载，安全性。还要考虑同步任务的启动时间，Schema 变更等等问题。实际使用的时候，我们在内部自研了一个平台，管理 MySQL 和 Hive 的数据源并对 Sqoop 任务做了调度。有一部分功能在 Sqoop 2.0 已经实现了。在大规模使用 sqoop 一定要想清楚运维的问题。</p>

<h3 id="toc_12">Reference</h3>

<ul>
<li><a href="https://stackoverflow.com/questions/24987820/not-able-to-run-sqoop-using-oozie">hadoop - Not able to run sqoop using oozie - Stack Overflow</a></li>
<li><a href="https://stackoverflow.com/questions/23250977/how-to-deal-with-sqoop-import-delimiter-issues-r-n">mysql - how to deal with sqoop import delimiter issues \r\n - Stack Overflow</a></li>
<li><a href="https://www.zybuluo.com/aitanjupt/note/209968">使用Sqoop从MySQL导入数据到Hive和HBase 及近期感悟 - 作业部落 Cmd Markdown 编辑阅读器</a></li>
<li><a href="https://community.hortonworks.com/questions/28060/can-sqoop-be-used-to-directly-import-data-into-an.html">Can sqoop be used to directly import data into an ORC table? - Hortonworks</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC">LanguageManual ORC - Apache Hive - Apache Software Foundation</a></li>
<li>Hive 增量同步： <a href="https://hortonworks.com/blog/four-step-strategy-incremental-updates-hive/">Four-step Strategy for Incremental Updates in Apache Hive</a></li>
<li>使用 MERGE INTO 更新 Hive 数据： <a href="https://hortonworks.com/blog/update-hive-tables-easy-way/">Update Hive Tables the Easy Way - Hortonworks</a></li>
<li>SQL MERGE 的性能： <a href="https://hortonworks.com/blog/apache-hive-moving-beyond-analytics-offload-with-sql-merge/">Apache Hive: Moving Beyond Analytics Offload with SQL MERGE - Hortonworks</a></li>
<li><a href="https://community.hortonworks.com/questions/11373/sqoop-incremental-import-in-hive-i-get-error-messa.html">sqoop incremental import in hive I get error message hive not support append mode how to solve that - Hortonworks</a></li>
<li><a href="http://www.hadooptechs.com/sqoop/sqoop-incremental-import-mysql-to-hive">Sqoop Incremental Import | MySQL to Hive | Big Data &amp; Hadoop</a></li>
<li><a href="https://ask.hellobi.com/blog/marsj/4114">Sqoop 1.4.6 导入实战 (RDB含MySQL和Oracle) - 天善智能：专注于商业智能BI和数据分析、大数据领域的垂直社区平台</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用 BigTop 打包 Hadoop 全家桶]]></title>
    <link href="http://blog.lfyzjck.com/15175572389238.html"/>
    <updated>2018-02-02T15:40:38+08:00</updated>
    <id>http://blog.lfyzjck.com/15175572389238.html</id>
    <content type="html"><![CDATA[
<p>使用 Hadoop 软件好像难免会自己改下代码做些定制，或者在部分组件的版本选择上激进，其他的版本( 比如 HDFS)  上保守。最近在公司升级 Hive 到 2.1.1 ，也对代码做了一些调整确保对业务的兼容性，之前公司使用的是 <code>hive-1.2.2-cdh-5.5.0</code> 。cdh 的发布节奏太慢跟不上社区的节奏，而且截止到现在，社区版本的 BUG 数量和稳定性都可以接受而不是必须选择商业公司给我们提供的发行版。</p>

<p>公司用的服务器是 Debian 7/8，为了方便的把定制过的 Hive 部署到服务器，需要将 Hive 打包成 deb，一直没找到特别好的打包方法。要做到 Cloudera 那样规范的 deb 非常繁琐，要处理启动脚本，环境变量，配置文件的 alternatives 等等。顺着这个思路找到了 Cloudera 官方的打包工具 <a href="https://github.com/cloudera/cdh-package">cdh-package</a> ，但是这个库已经太长时间没有维护了，里面依赖的版本信息非常老旧，而且自己测试也没运行成功。但是 cdh-package 是基于 <a href="https://github.com/apache/bigtop">BigTop</a> 的，BigTop 本身的维护还不错。</p>

<p>Bigtop 非常有野心，它的主要目标就是构建一个 Apache Hadoop 生态系统的包和交互式测试的社区。这个包括对各类不同级别工程进行测试(包，平台，运行时间，升级等...)，它由社区以关注系统作为一个整体开发而来。BigTop 官方除了介绍怎么安装之外没有任何使用文档，不过研究以后发现还算简单，不需要太多的说明。</p>

<h3 id="toc_0">准备 BigTop 环境</h3>

<p>可以根据官方的说明来安装，我这里是直接从 Github 拉了代码：</p>

<pre><code class="language-bash">git clone https://github.com/apache/bigtop.git
cd bigtop
git checkout release-1.2.1
./gradlew
</code></pre>

<p>然后我们可以运行 <code>./gradlew tasks</code> 看下 BigTop 给我们提供的命令，命令遵循下面的格式 <code>./gradlew &lt;package&gt;-&lt;dist&gt;</code> ：</p>

<pre><code class="language-bash">$ ./gradlew tasks
# hide some output
hive-clean - Removing hive component build and output directories
hive-deb - Building DEB for hive artifacts
hive-download - Download hive artifacts
hive-help - List of available tasks for hive
hive-info - Info about hive component build
hive-pkg - Invoking a native binary packaging target deb
hive-relnotes - Preparing release notes for hive. No yet implemented!!!
hive-rpm - Building RPM for hive artifacts
hive-sdeb - Building SDEB for hive artifacts
hive-spkg - Invoking a native binary packaging target sdeb
hive-srpm - Building SRPM for hive artifacts
hive-tar - Preparing a tarball for hive artifacts
hive-version - Show version of hive component
</code></pre>

<p>然后编辑 <code>bigtop.bom</code> 将依赖的版本改成自己需要的版本，注意 BigTop 这里会优先使用 bigtop.bom 中定义的版本号覆盖源代码的版本号。</p>

<pre><code class="language-json">&#39;hive&#39; {
      name    = &#39;hive&#39;
      relNotes = &#39;Apache Hive&#39;
      version { base = &#39;1.2.1&#39;; pkg = base; release = 1 }
      tarball { destination = &quot;apache-${name}-${version.base}-src.tar.gz&quot;
                source      = destination }
      url     { download_path = &quot;/$name/$name-${version.base}/&quot;
                site = &quot;${apache.APACHE_MIRROR}/${download_path}&quot;
                archive = &quot;${apache.APACHE_ARCHIVE}/${download_path}&quot; }
    }
</code></pre>

<p>下面将介绍如何用 BigTop 打包 Hive</p>

<h3 id="toc_1">用 BigTop 打包 Hive</h3>

<p>我们的目标是将一份修改过的 Hive 代码打包成 deb 包分发到集群，首先在编辑机器上准备一些必要的依赖：</p>

<pre><code class="language-bash">sudo apt-get update
sudo apt-get install devscripts
sudo apt-get install dh-make
</code></pre>

<p>接下来准备 Hive 的代码，Bigtop 默认根据 bom 文件里指定的版本号从上游下载 Hive 的代码，解压然后编译。但是由于我们要使用自己修改过的版本，可以修改 <code>bigtop.bom</code> 从内部 git 仓库下载代码。</p>

<pre><code class="language-grovvy">  &#39;hive&#39; {
      name    = &#39;hive&#39;
      relNotes = &#39;Apache Hive&#39;
      version { base = &#39;2.1.1&#39;; pkg = base; release = 1 }
      tarball { destination = &quot;apache-${name}-${version.base}-src.tar.gz&quot;
                source      = destination }
      git     { repo = &quot;https://exmaple.com:hive/hive&quot;
                ref = &quot;release-2.1.1&quot;
                dir  = &quot;${name}-${version.base}&quot; }
    }
</code></pre>

<p>然后就可以开始打包了：</p>

<pre><code class="language-bash">./gradlew hive-deb
</code></pre>

<p>注意 BigTop 在它的仓库里包含了对 Hive 的几个 patch 文件，我这边测试的时候会导致编译失败，建议删除：</p>

<pre><code class="language-bash">rm -f /bigtop-packages/src/common/hive/*
</code></pre>

<p>然后清理构建环境，重新打包：</p>

<pre><code class="language-bash">./gradlew hive-clean
./gradlew hive-deb
</code></pre>

<p>如果需要更新包，可以提升 Release number，默认是 1 ，这个 BigTop 在文档里没有提及：</p>

<pre><code class="language-bash">BIGTOP_BUILD_STAMP=&lt;release&gt; ./gradlew hive-deb
</code></pre>

<h3 id="toc_2">发布 deb 包</h3>

<p>看看我们构建的结果，产生了很多 deb 包，这些包都需要上传到内部的 mirror</p>

<pre><code>$ ls -l output/hive/
total 138516
-rw-r--r-- 1 ck ck 78645700 Feb  1 18:32 hive_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck   314946 Feb  1 18:33 hive_2.1.1-1_amd64.build
-rw-r--r-- 1 ck ck     3461 Feb  1 18:32 hive_2.1.1-1_amd64.changes
-rw-r--r-- 1 ck ck    12500 Feb  1 18:18 hive_2.1.1-1.debian.tar.xz
-rw-r--r-- 1 ck ck     1227 Feb  1 18:18 hive_2.1.1-1.dsc
-rw-r--r-- 1 ck ck     1829 Feb  1 18:18 hive_2.1.1-1_source.changes
-rw-r--r-- 1 ck ck 20999949 Feb  1 18:18 hive_2.1.1.orig.tar.gz
-rw-r--r-- 1 ck ck   107906 Feb  1 18:32 hive-hbase_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck   452862 Feb  1 18:32 hive-hcatalog_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck     3632 Feb  1 18:32 hive-hcatalog-server_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck 39029552 Feb  1 18:32 hive-jdbc_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck     3734 Feb  1 18:32 hive-metastore_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck     3738 Feb  1 18:32 hive-server2_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck  2068240 Feb  1 18:32 hive-webhcat_2.1.1-1_all.deb
-rw-r--r-- 1 ck ck     3608 Feb  1 18:32 hive-webhcat-server_2.1.1-1_all.deb
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hive 论文笔记]]></title>
    <link href="http://blog.lfyzjck.com/15105848393062.html"/>
    <updated>2017-11-13T22:53:59+08:00</updated>
    <id>http://blog.lfyzjck.com/15105848393062.html</id>
    <content type="html"><![CDATA[
<p>MapReduce 出现后，对数据的计算需求越来越多，而 MapReduce 提供的 API 太底层，学习成本和开发成本比较高，因此需要一个类 SQL 的工具，来代替大部分的 MapReduce 的使用场景。</p>

<h3 id="toc_0">数据模型，类型系统和查询语言</h3>

<p>Hive 和传统的数据库一样有 Database 和 Table 的概念，数据存储在 Table 中。每个 Table 中会会很多行，每行有多列组成。</p>

<h5 id="toc_1">类型</h5>

<p>Hive 支持原生类型 (primitive types) 和复杂类型 (complex types)。</p>

<p>Primitive:</p>

<ul>
<li>Integers: bigint, int, smallint, tinyint</li>
<li>Float: float, double</li>
<li>String</li>
</ul>

<p>Complex:</p>

<ul>
<li>map<key-type, value-type></li>
<li>list<element-type></li>
<li>struct<field-name, field-type, …></li>
</ul>

<p>SerDe , Format 都是可插拔的，用户可以自定义 SerDe 或者 Format，在查询时可以通过 HiveQL 增加自己的 SerDe:</p>

<pre><code>ADD JAR /jars/myformat.jar;
CREATE TABLE t2
ROW FORMAT SERDE &#39;com.myformat.MySerDe&#39;;
</code></pre>

<h5 id="toc_2">HiveQL</h5>

<p>HiveQL 和传统的 SQL 几乎没有差别，但是存在一些局限：</p>

<ul>
<li>只能使用标准的 ANSI Join 语法</li>
<li>JOIN 条件只能支持 <code>=</code> 运算符，不能使用 <code>&gt;</code>, <code>&lt;</code></li>
<li>Hive 不能支持正常的 <code>INSERT INTO</code>, 只能使用 <code>INSERT INTO OVERWRITE</code> 从已有的数据中生成</li>
</ul>

<p>Hive 中可以直接调用 MapReduce 程序：</p>

<pre><code>FROM (
  MAP doctext USING &#39;python wc_mapper.py&#39; AS (word, cnt)
  FROM docs
  CLUSTER BY word
) a
REDUCE word, cnt USING &#39;python wc_reduce.py&#39;;
</code></pre>

<p>Hive 提供了 <code>CLUSTER BY</code> 和 <code>DISTRIBUTE BY</code> 等语法改善 Reduce 的数据分布，解决数据倾斜问题</p>

<h3 id="toc_3">Data Storage, SerDe and File formats</h3>

<h5 id="toc_4">Data Storage</h5>

<p>Hive 的存储模型有 3 个层级</p>

<ul>
<li>Tables 对应 HDFS 的一个目录</li>
<li>Partitions 是 Table 的子目录</li>
<li>Buckets 是目录下具体的文件</li>
</ul>

<blockquote>
<p>Partition 的字段不是 Table 数据的一部分，而是保存在目录的名称中。比如 <code>/usr/hive/warehouse/t1/p_date=20170701</code></p>
</blockquote>

<p>Partition 可以优化查询性能，当用户指定 Partition 的情况下，Hive 只会扫描指定 Partition 下的文件。当 Hive 运行在 <code>strict</code> 模式时，用户需要指定只要一个 Partition 字段。</p>

<p>Bucket 相当于目录树的叶子节点，在创建表的时候用户可以指定需要多少个 Bucket，Bucket 可以用户数据的快速采样。</p>

<p>由于数据都保存在 HDFS 的表空间下，如果用户需要查询 HDFS 其他目录的文件，可以使用外部表：</p>

<pre><code>CREATE EXTERNAL TABLE test_extern(c1 string, c2 int)
LOCATION &#39;/user/mytables/mydata&#39;;
</code></pre>

<p>外部表和普通表的唯一区别是当我们执行 <code>DROP TABLE</code> 时，外部表不会删除 HDFS 的文件。</p>

<h5 id="toc_5">SerDe</h5>

<p>SerDe 提供了几个 Java 接口，方便在文件格式和 Java Native 类型之间相互转化。默认的 SerDe 实现叫 <code>LazySerDe</code> ，是一种用文本表示数据的存储格式。这种格式用 <code>Ctrl-A</code> 来分割列，<code>\n</code> 来分割行。其他的 SerDe 实现包括 RegexSerDe, Thrift, Avro 等等。</p>

<h5 id="toc_6">File Formats</h5>

<p>Hadoop 上的文件可以以不同格式存储，Hive 默认的存储格式是一种叫 TextFormat 的格式，用类似 CSV 的纯文本表示数据。Format 可以在创建表的时候指定：</p>

<pre><code>CREATE TABLE dest1(key INT, value STRING)
  STORED AS
    INPUTFORMAT &#39;org.apache.hadoop.mapred.SequenceFileInputFormat&#39;
    OUTPUTFORMAT &#39;org.apache.hadoop.mapred.SequenceFileOutputFormat&#39;;
</code></pre>

<p>存储格式可以根据自己的需求扩展，选择合适的存储格式有利于提高性能，比如面向列存储的 ParquetFile 和 ORCFile，可以减少读取的数据量，ORCFile 甚至可以做一些与计算，来满足 Push Down 的需求。</p>

<h5 id="toc_7">系统架构和组件</h5>

<p>Hive 由下面的组件构成：</p>

<ul>
<li>MetaStore - 存储系统目录看，还有数据库，表，列的各种原信息</li>
<li>Driver - 管理 HiveQL 的生命周期</li>
<li>Query Compiler - 将 Hive 的语句转换成一个 MapReduce 表达的 DAG</li>
<li>Execution Engine - 将任务按照依赖顺序执行，早起版本只有 MapReduce，新版本应该有 Tez 和 Spark</li>
<li>HiveServer - 提供 JDBC/ODBS 接口的服务，方便和其他系统集成</li>
<li>Clients - Web UI 和命令行工具</li>
</ul>

<p>A. MetaStore</p>

<p>Hive 的 MetaStore 一般基于一个 RDBMS 来实现，提供了一个 ORM 层来作为数据库的抽象。MetaStore 本身不是为了高并发和高可用设计的，在架构中也是一个单点（新版本有主从了），所以 Hive 在设计的时候需要保证在任务执行期间没有任何对 MetaStore 的访问。</p>

<p>B. Query Compiler</p>

<p>Query Compiler 首先将 HQL 转换成一个 AST，然后进行类型检查和语法分析，将 AST 转换成一个 operator DAG （QB Tree），然后优化器对 QB Tree 进行优化。Hive 只支持基于规则的优化器，比如只读取指定列的数据减少 IO，或者用户指定分区字段时，只读取指定分区下的文件。<br/>
RBO 本质上是一个 Transformer，在 QB Tree 上做一系列的变换来减少查询的代价</p>

<blockquote>
<p>Hive 0.1.4 开始引入了一些基于代价的优化器（CBO）。<a href="https://zh.hortonworks.com/blog/hive-0-14-cost-based-optimizer-cbo-technical-overview/">COST BASED OPTIMIZER</a></p>
</blockquote>

<p>之后就根据优化后的 QB Tree 生成物理执行计划，并且将 jar 包写入到 HDFS 的临时目录，开始运行。</p>

<p>C. Execution Engine</p>

<p>执行引擎会解析 plan.xml ，然后按照顺序将任务提交到 Hadoop，最终的数据库文件也会 move 到 HDFS 的指定目录。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[金丝雀发布]]></title>
    <link href="http://blog.lfyzjck.com/14807508681806.html"/>
    <updated>2016-12-03T15:41:08+08:00</updated>
    <id>http://blog.lfyzjck.com/14807508681806.html</id>
    <content type="html"><![CDATA[
<p>金丝雀部署（Canary Deployments）在知乎落地差不多一年时间，通过金丝雀避免了很多线上的问题，相比之前的发布模型，极大降低了部署的风险。知乎是非常推崇 Devops 的公司，金丝雀发布作为 Devops 一种实践自然不会落下。但是由于基础设施变得越来越抽象和复杂，理解整个部署的工作流程已经变得比较困难，正好有机会给新的同事科普一下金丝雀发布的架构。</p>

<p>相传上个世纪煤矿工人在作业时，为了避免瓦斯中毒会随身带一只金丝雀下到矿洞，由于金丝雀对二氧化碳非常敏感，所以看到金丝雀昏厥的时候矿工们就知道该逃生了。[1]</p>

<p>金丝雀发布就是用生产环境一小部分流量验证应用的一种方法。从这个名字的由来也可以看到，金丝雀发布并不是完美的，如果代码出现问题，那么背用作测试的小部分流量会出错，就跟矿坑中昏厥的金丝雀一样。这种做法在非常敏感的业务中几乎无法接受，但是当系统复杂的到一定程度，错误无法完全避免的时候，为了避免出现更大的问题，牺牲一小部分流量，就可以将大部分错误的影响控制在一定范围内。</p>

<h3 id="toc_0">金丝雀发布的步骤</h3>

<p>一个典型的金丝雀发布大概包含以下步骤[2]：</p>

<ol>
<li>准备好发布用的 artifact</li>
<li>从负载均衡器上移除金丝雀服务器</li>
<li>升级金丝雀服务器</li>
<li>最应用进行自动化测试</li>
<li>将金丝雀服务器加入到负载均衡列表中</li>
<li>升级剩余的服务版本</li>
</ol>

<p>在知乎，负载均衡器采用的 HAProxy，并且依赖 Consul 作服务注册发现。而服务器可能是一台物理机也可能是 bay 上一个抽象的容器组。</p>

<p><img src="media/14807508681806/Canary%20Deployments.png" alt="Canary Deployments"/></p>

<ul>
<li>对于物理机，我们可以单独为其配置一个一台独立的服务器，通过在 HAProxy 上设置不同于 Production 服务器的权重来控制测试流量。但是这种方法不够方便，做自动化也难一些</li>
<li>对于容器相对简单，我们复制一个 Production 版本的容器组，然后通过控制 Production 和 Canary 容器组的数量就可以控制流量。</li>
</ul>

<p>整个过程是部署系统在中间协调，当我们上线发布完成，部署系统会移除金丝雀服务器，让应用回到 Normal 状态。</p>

<p>如果遇到问题需要回滚，只需要将金丝雀容器组从 HAProxy 上摘掉就可以，基本上可以在几秒内完成。</p>

<h3 id="toc_1">HAProxy</h3>

<p>在整个金丝雀发布的架构中，HAProxy 是非常重要的一个组件，要发现后端的服务地址，并动态控制金丝雀和线上的流量比例。部署时我们并不会直接操作 HAProxy，而是更新 Consul 上的注册信息，通过事件广播告诉 HAProxy 服务地址有变化，这一过程通过 <code>consul-template</code> 完成。</p>

<p><img src="media/14807508681806/consul.png" alt="consu"/><br/>
HAProxy 自己也会注册到 Cosnul，伪装成服务的后端被调用，而服务自身则注册成 <code>服务名 + --instance</code>，在我们内部的 Consul 控制台可以看到。</p>

<p>每个服务都有自己独立的 HAproxy 集群，分布在不同的机器上，每个 HAProxy 只知道自己代理的服务的地址。这样做的好处是单个 HAProxy 崩溃不会影响业务，一组 HAProxy 负载高不会把故障扩散到整个集群。另外附带的一个好处是当我们更新服务注册地址时，不会 reload 整个 HAProxy 集群，只要更新对应的 HAProxy 实例就可以，一定程度上可以规避惊群问题。</p>

<p>HAProxy 的地址通过客户端服务发现获得，客户端发现多个 HAProxy 地址并可以做简单的负载均衡，将请求压力分摊到多个 HAProxy 实例上。</p>

<p>采用类似方案的公司是 Airbnb，不过他们的做法是把 HAProxy 作为一个 Agent 跑在服务器上，HAProxy 更靠近客户端[3]。</p>

<h3 id="toc_2">金丝雀发布的监控</h3>

<p>有了金丝雀发布后，我们还得能区分金丝雀和线上的监控数据，以此判断服务是否正常。</p>

<p>由于有了 zone 和 tzone 框架对监控的支持，这件事推起来相对简单。我们在部署时将服务当前所在的环境注入到环境变量中，然后根据环境变量来决定指标的名称。</p>

<p>比如一个正常的指标名称是:</p>

<pre><code>production.span.multimedia.server.APIUploadHandler_post.request_time
</code></pre>

<p>在金丝雀环境中的名称则是:</p>

<pre><code>canary.span.multimedia.server.APIUploadHandler_post.request_time
</code></pre>

<p>最后我们可以在 Halo 对比服务在金丝雀和生产的表现有何差别：</p>

<p><img src="media/14807508681806/14808361124276.jpg" alt=""/></p>

<h3 id="toc_3">Reference</h3>

<p>[1] 金丝雀发布的由来: <a href="https://blog.ccjeng.com/2015/06/canary-deployment.html">https://blog.ccjeng.com/2015/06/canary-deployment.html</a><br/>
[2] 在生产中使用金丝雀部署来进行测试: <a href="http://www.infoq.com/cn/news/2013/03/canary-release-improve-quality">http://www.infoq.com/cn/news/2013/03/canary-release-improve-quality</a><br/>
[3] Service Discovery in the Cloud: <a href="http://nerds.airbnb.com/smartstack-service-discovery-cloud/">http://nerds.airbnb.com/smartstack-service-discovery-cloud/</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Graphviz 入门指南]]></title>
    <link href="http://blog.lfyzjck.com/14707952861924.html"/>
    <updated>2016-08-10T10:14:46+08:00</updated>
    <id>http://blog.lfyzjck.com/14707952861924.html</id>
    <content type="html"><![CDATA[
<p>Graphviz 是一个开源的图可视化工具，非常适合绘制结构化的图标和网络。Graphviz 使用一种叫 DOT 的语言来表示图形。</p>

<h3 id="toc_0">DOT 语言</h3>

<p>DOT 语言是一种图形描述语言。能够以简单的方式描述图形，并且为人和计算机所理解。</p>

<h4 id="toc_1">无向图</h4>

<pre><code>graph graphname {
    a -- b -- c;
   b -- d;
}
</code></pre>

<p><img src="media/14707952861924/14710194114259.jpg" alt="" style="width:220px;"/></p>

<h4 id="toc_2">有向图</h4>

<pre><code>digraph graphname {
    a -&gt; b -&gt; c;
    b -&gt; d;
}
</code></pre>

<p><img src="media/14707952861924/14710200127648.jpg" alt=""/></p>

<h4 id="toc_3">设置属性</h4>

<p>属性可以设置在节点和边上，用一对 <code>[]</code> 表示，多个属性可以用空格或者 <code>,</code> 隔开。</p>

<pre><code>strict graph {
    // 设置节点属性
  b [shape=box];
  c [shape=triangle];

  // 设置边属性
  a -- b [color=blue];
  a -- c [style=dotted];
}
</code></pre>

<p>完整的属性列表可以参考 <a href="http://www.graphviz.org/content/attrs">attrs | Graphviz - Graph Visualization Software</a></p>

<h4 id="toc_4">子图</h4>

<p><code>subgraph</code> 的作用主要有 3 个：</p>

<ol>
<li>表示图的结构，对节点和边进行分组</li>
<li>提供一个单独的上下位文设置属性</li>
<li>针对特定引擎使用特殊的布局。比如下面的例子，如果 <code>subgraph</code> 的名字以 <code>cluster</code> 开头，所有属于这个子图的节点会用一个矩形和其他节点分开。</li>
</ol>

<pre><code>digraph graphname{ 
    a -&gt; {b c};
    c -&gt; e;
    b -&gt; d;
    
    subgraph cluster_bc {
        bgcolor=red;
        b;
        c;
    }
    
    subgraph cluster_de {
        label=&quot;Block&quot;
        d;
        e;
    }
}
</code></pre>

<p><img src="media/14707952861924/14710216720774.jpg" alt=""/></p>

<h4 id="toc_5">布局</h4>

<p>默认情况下图是从上到下布局的，通过设置 <code>rankdir=&quot;LR&quot;</code> 可以让图从左到右布局。</p>

<p>一个简单的表示 CI&amp;CD 过程的图：</p>

<pre><code>digraph pipleline {
    rankdir=LR;
    g [label=&quot;Gitlab&quot;];
    j [label=&quot;Jenkins&quot;];
    t [label=&quot;Testing&quot;];
    p [label=&quot;Production&quot; color=red];
    
    g -&gt; j [label=&quot;Trigger&quot;];
    j -&gt; t [label=&quot;Build&quot;];
    t -&gt; p [label=&quot;Approved&quot;];
}
</code></pre>

<p><img src="media/14707952861924/14710227380504.jpg" alt=""/></p>

<h3 id="toc_6">工具</h3>

<p>有非常多的工具可以支持 DOT 语言，这些工具都被集成在 Graphviz 的软件包中，可以简单安装使用。</p>

<p>dot </p>

<blockquote>
<p>一个用来将生成的图形转换成多种输出格式的命令行工具。其输出格式包括PostScript，PDF，SVG，PNG，含注解的文本等等。</p>
</blockquote>

<p>neato </p>

<blockquote>
<p>用于sprint model的生成（在Mac OS版本中称为energy minimized）。</p>
</blockquote>

<p>twopi </p>

<blockquote>
<p>用于放射状图形的生成</p>
</blockquote>

<p>circo </p>

<blockquote>
<p>用于圆形图形的生成。</p>
</blockquote>

<p>fdp </p>

<blockquote>
<p>另一个用于生成无向图的工具。</p>
</blockquote>

<p>dotty </p>

<blockquote>
<p>一个用于可视化与修改图形的图形用户界面程序。</p>
</blockquote>

<p>lefty </p>

<blockquote>
<p>一个可编程的(使用一种被EZ影响的语言[4])控件，它可以显示DOT图形，并允许用户用鼠标在图上执行操作。Lefty可以作为MVC模型的使用图形的GUI程序中的视图部分。</p>
</blockquote>

<p>另外介绍 2 个在线生成 Graphviz 的网站：</p>

<ul>
<li><a href="http://www.webgraphviz.com/">http://www.webgraphviz.com/</a></li>
<li><a href="http://dreampuf.github.io/GraphvizOnline/">http://dreampuf.github.io/GraphvizOnline/</a></li>
</ul>

<h3 id="toc_7">Reference</h3>

<p>[1] <a href="https://www.ibm.com/developerworks/cn/aix/library/au-aix-graphviz/">使用 Graphviz 生成自动化系统图</a><br/>
[2] <a href="https://zh.wikipedia.org/wiki/DOT%E8%AF%AD%E8%A8%80">DOT语言 - 维基百科，自由的百科全书</a><br/>
[3] <a href="http://www.graphviz.org/content/dot-language">http://www.graphviz.org/content/dot-language</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[elixir-overview]]></title>
    <link href="http://blog.lfyzjck.com/14506675584773.html"/>
    <updated>2015-12-21T11:12:38+08:00</updated>
    <id>http://blog.lfyzjck.com/14506675584773.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0"></h1>

<h1 id="toc_1">初窥 Elixir</h1>

<p>Elixir 是基于 Erlang VM 开发一个新语言，继承了 Erlang 的很多有点，在保持和 Erlang ByteCode 兼容的提前下提供了非常简洁易懂的语法。单从语法层面可能和 Ruby 比较像，不过仔细看还是和 Erlang 有很多一致的地方，如果你不习惯 Erlang 奇怪的语法(其实看过 Prolog 的语法以后就不觉得 Erlang 奇怪了)，但是又想享受 Erlang/OTP 带来的各种好处，Elixir 是不错的选择。</p>

<p>Elixir 可以调用 Erlang 的标准库，如果基本不用担心 第三方库不足的问题， Erlang 几十年的积累在这里放着呢，难怪 Joe Armstrong 大叔也对 Elixir 赞不绝口。</p>

<h2 id="toc_2">类型</h2>

<p>用 <code>iex</code> 来启动的 Elixir Shell，可以看到一个很像 Erlang Shell 的东西：</p>

<pre><code>$ iex
Erlang/OTP 17 [erts-6.1] [source] [64-bit] [smp:4:4] [async-threads:10] [hipe] [kernel-poll:false] [dtrace]

Interactive Elixir (1.0.3) - press Ctrl+C to exit (type h() ENTER for help)
iex&gt;
</code></pre>

<p>Elixir 支持 Erlang 大部分的类型，integer, float, atom, list, tuple, binary, keywords, map ...</p>

<pre><code>iex&gt; 1
1
iex&gt; &quot;foobar&quot;
&quot;foobar&quot;
iex&gt; &#39;foobar&#39;
&#39;foobar&#39;
iex&gt; :foo
:foo
iex&gt; [1, 2, 3]
[1, 2, 3]
iex&gt; {:foo, :bar}
{:foo, :bar}
iex&gt; &lt;&lt;104, 101, 108, 108, 111, 0&gt;&gt;
&lt;&lt;104, 101, 108, 108, 111, 0&gt;&gt;
iex&gt; [{:foo, 1}, {:bar, 2}]
[foo: 1, bar: 2]
iex&gt; %{:ok =&gt; 1, :fail =&gt; 2}
%{fail: 2, ok: 1}
</code></pre>

<p><code>&quot;foobar&quot;</code> 和 <code>&#39;foobar&#39;</code> 是不同的类型，前者是 string，后者是是一个 charlist</p>

<pre><code>iex&gt; is_binary &quot;foobar&quot;
true
iex&gt; is_list &#39;foobar&#39;
true
</code></pre>

<p>Elixir 引入了一个很像 Map 的类型叫 Keyword，但其实 Keyword  就是一个二元的 <code>tuple</code> 列表，不过要求 <code>tuple</code> 的第一个元素是 <code>atom</code>。Keyword 支持类似 Map 的访问方式</p>

<pre><code>iex&gt; keyword1 = [{:foo, 1}, {:bar, 2}]
[foo: 1, bar: 2]
iex&gt; keyword1[:foo]
1
iex&gt; keyword1[:bar]
2
</code></pre>

<p>Map 通过 <code>%{ key1 =&gt; value1, key2 =&gt; value2 ...}</code> 的语法来创建，访问 Map 中的值有 3 种方式：</p>

<pre><code>iex&gt; map = %{:a =&gt; 1, 2 =&gt; :b}
%{2 =&gt; :b, :a =&gt; 1}
iex&gt; map[:a]
1
iex&gt; map[2]
:b
</code></pre>

<p>不论 Map 还是 Keyword lists 都实现了 Dict 的访问协议，我们可以可以用 Dict 模块同时处理这 2 种数据结构。</p>

<pre><code>iex&gt; map
%{2 =&gt; :b, :a =&gt; 1}
iex&gt; Dict.get(map, :a)
1
iex&gt; Dict.put_new(map, :c, 3)
%{2 =&gt; :b, :a =&gt; 1, :c =&gt; 3}
</code></pre>

<h2 id="toc_3">模式匹配和递归</h2>

<p>Elixir 和 Erlang 一样，实现了模式匹配。模式匹配的强大不用多说，在解析数据的时候可以少写很多很多的 if...else... </p>

<pre><code>iex&gt; {:ok, msg} = {:ok, &quot;success&quot;}
{:ok, &quot;success&quot;}
iex&gt; %{:a =&gt; x} = %{:a =&gt; 1, :b =&gt; 2}
%{a: 1, b: 2}
iex&gt; %{:a =&gt; x} = %{:b =&gt; 2}
** (MatchError) no match of right hand side value: %{b: 2}
</code></pre>

<p>Elixir 里的函数必须定义在 Module 里，让我们实现一个计算斐波那契数列的模块。</p>

<pre><code>defmodule Math do
  @doc&quot;&quot;&quot;
  Calculate Fibonacci sequence value
  &quot;&quot;&quot;
  def fab(1) do 1 end
  def fab(2) do 1 end
  def fab(n) do
    fab(n-1) + fab(n-2)
  end

  @doc &quot;&quot;&quot;
  Calculates the sum of a number list
  &quot;&quot;&quot;
  def sum(list) do sum(list, 0) end
  def sum([], sum) do sum end
  def sum([h|t], sum) do sum(t, sum + h) end
  
end
</code></pre>

<h2 id="toc_4">Enumerables &amp; streams &amp; Comprehensions</h2>

<p><code>Enum</code> 和 <code>Stream</code> 最大的不同是 <code>Stream</code> 是惰性求值，类似 Python 里的 generator，只有需要计算的时候真正计算。</p>

<pre><code>iex&gt; Enum.map(1..10, fn x -&gt; x * x end)
[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]
iex&gt; Enum.reduce(1..10, 0, &amp;(&amp;1+&amp;2))
55
iex&gt; 1..10 |&gt; Enum.map(&amp;(&amp;1*&amp;1)) |&gt; Enum.reduce(&amp;(&amp;1+&amp;2))
385
iex&gt; 1..100 |&gt; Stream.map(&amp;(&amp;1*&amp;1)) |&gt; Stream.filter(&amp;(rem(&amp;1, 2) != 0)) |&gt; Enum.sum
166650
</code></pre>

<p>Elixir 也提供了列表推倒的机制，利用 for 的 filter 机制，还能方便实现一些功能。</p>

<pre><code>iex&gt; for n &lt;- 1..4, do: n * n
[1, 4, 9, 16]
</code></pre>

<p>找到 n 以内满足 <code>a^2 + b^2 = c^2</code> 的 <code>{a, b, c}</code> 元组：</p>

<pre><code>defmodule Triple do
  def pythagorean(n) when n &gt; 0 do
    for a &lt;- 1..n,
        b &lt;- 1..n,
        c &lt;- 1..n,
        a + b + c &lt;= n,
        a*a + b*b == c*c,
        do: {a, b, c}
  end
end
</code></pre>

<h2 id="toc_5">异常处理</h2>

<p>Elixir 提供了 3 种错误处理机制：</p>

<ul>
<li>Errors &amp; Exceptions</li>
<li>Throws</li>
<li>Exits</li>
</ul>

<pre><code>iex&gt; try do
...&gt;   raise &quot;oops&quot;
...&gt; rescue
...&gt;   RuntimeError -&gt; &quot;Error!&quot;
...&gt; end
&quot;Error!&quot;

iex&gt; spawn_link fn -&gt; exit(1) end
#PID&lt;0.56.0&gt;
** (EXIT from #PID&lt;0.56.0&gt;) 1
</code></pre>

<h2 id="toc_6">调用 Erlang 代码</h2>

<pre><code>iex&gt; angle_45_deg = :math.pi() * 45.0 / 180.0
iex&gt; :math.sin(angle_45_deg)
0.7071067811865475
</code></pre>

<h2 id="toc_7">Process</h2>

<p>Elixir 借助 ErlangVM 实现了 Actor 模型，进程和进程之间仅通过消息通信，每个 Actor 都有一个 Mailbox ，消息总是被拷贝到 MailBox 中然后等待进程处理。这里的 Process 和操作系统的 Process 不能等同，它比操作系统的 Process 要轻量很多，可能很轻松在单机上开启几十万的 Process。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Golang编程风格不完全指南]]></title>
    <link href="http://blog.lfyzjck.com/14506675584810.html"/>
    <updated>2015-12-21T11:12:38+08:00</updated>
    <id>http://blog.lfyzjck.com/14506675584810.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0"></h1>

<h2 id="toc_1">语言规范</h2>

<h4 id="toc_2">简介</h4>

<p>学习Go规范的最好办法就是看Go的源码，官方是这么说的</p>

<h4 id="toc_3">格式化</h4>

<p>格式这个东西，对大部分语言来说都挺重要的，不过Go有个很牛逼的工具叫<code>gofmt</code>，可以从<code>package</code>级别对代码的格式进行格式化，所以这种脏活就交给工具来做吧。</p>

<h4 id="toc_4">注释</h4>

<p>Go提供了C-style的/* */的块注释，和C++-style的// 行内注释</p>

<p>对于每个<code>package</code>，都应当提供响应的注释，如果这个<code>package</code>提供的功能很简单，那么用行内注释写一些简单的说明也是可以的。不必在块注释的每行前都加上<code>*</code>号，注释内的字体可能不是等宽字体，所以也不要依赖空格来对齐，<code>godoc</code>会帮你完成这些事情。</p>

<p>在一个<code>package</code>中，每个被<code>export</code>的方法，都应该包含注释.注释最好以方法名开头，然后用最简单的一句话概括这个函数的用途。</p>

<h4 id="toc_5">命名</h4>

<p>Go推荐采用类似Java的驼峰式的命名，而不是用下划线。</p>

<h5 id="toc_6">Package names</h5>

<p>包名应该尽量简洁且容易记忆，处于习惯一般采用全部小写字母。每一个使用你提供的<code>Package</code>的用户，都需要提前<code>import</code>，所以你也不用特地的检测名字的冲突，万一不行还可以在本地使用别的名字代替。</p>

<h5 id="toc_7">Getter&amp;Setter</h5>

<p><code>GetAttr</code>就命名成<code>Attr()</code><br/>
<code>SetAttr</code>还是保持<code>SetAttr()</code><br/>
然后注意首字母大写，确保这个函数是被<code>export</code>的</p>

<h5 id="toc_8">Interface</h5>

<p>一般就是加个<code>er</code>的后缀，比如Reader, Writer, Notifer</p>

<h4 id="toc_9">循环结构</h4>

<p>Go的循环类似于C，稍有不同。Go统一了<code>for</code>和<code>while</code>，而且没有<code>do-while</code></p>

<pre><code>// Like a C for
for init; condition; post { }

// Like a C while
for condition { }

// Like a C for(;;)
for { }
</code></pre>

<p>如果要遍历的对象是<code>array</code>, <code>slice</code>, <code>string</code>, <code>map</code>或者从channel读取数据，可以使用<code>range</code>关键字。类似于Python的<code>for ... in ...</code>。在<code>range</code>循环里，可以单独访问key&amp;value，或者一起访问。</p>

<pre><code>// 只访问key
for key := range m {
    if key.expired() {
        delete(m, key)
    }
}
// 只访问value
sum := 0
for _, value := range array {
    sum += value
}
// 两个都要用
for key, value := range oldMap {
    newMap[key] = value
}
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Golang依赖管理实践]]></title>
    <link href="http://blog.lfyzjck.com/14506675584846.html"/>
    <updated>2015-12-21T11:12:38+08:00</updated>
    <id>http://blog.lfyzjck.com/14506675584846.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0"></h1>

<h2 id="toc_1">引子</h2>

<p>这篇文章似乎拖得有点久了，好在现在在写了。Golang 的风潮尚未过去，大红大紫的 Docker 进一步提高了人们对这门语言的期待，但是当你把 Golang 用在线上生产环境的时候，就会面临一个比较严重问题：依赖管理。</p>

<p>对于依赖管理，Java 有 <code>Maven</code>，Python 有 <code>pip</code>，Nodejs 也有 <code>npm</code>，Golang 自己本身作为一种编译型的语言，依赖管理只能靠官方提供的 <code>go-get</code> 工具，大部分情况下，这个工具对我们在开发时还是相当友好的。</p>

<h2 id="toc_2">go get 简介</h2>

<p><code>go-get</code>是 Golang 自带的一个依赖管理工具，他可以从远程下载你需要的包，但是注意Golang并没有统一的仓库，所以你传给 <code>go get</code> 的参数其实是一个URL。对于 Github, Google Code, Bitbucket 这样的网站，<code>go get</code> 可以自动解析到 repo 地址，典型的 <code>go get</code> 大概会包含下面几个步骤：</p>

<ul>
<li>fetch repo 到本地的 <code>$GOPATH/src</code> 下, 此过程会依赖 repo 对应版本管理软件，比如你要安装一个 Github 上的包，那么你本地必须安装了 git</li>
<li>执行<code>go install</code>命令，对源码进行编译并把编译好的二进制文件拷贝到<code>$GOPATH/pkg</code>下面</li>
<li>如果不是 Github 这样第三方托管网站，比如公司内的 Gitlab 系统，<code>go get</code>会解析 html 返回中名为<code>go-import</code>的<code>meta</code>标签，找到 repo 的类型和地址，然后重复上面的事情</li>
</ul>

<p>不过这样的实现会有一点问题，假如我们用了类似Gitlab的系统，在公司内部的很多 repo 都是 Internal 的，<code>go get</code>无法直接访问到 repo 页面，自然谈不上解析 <code>go-import</code> ，<code>go get</code> 还没只能到可以自动登录，所以一定是失败的，我们依赖的 Package 还是无法自动安装......</p>

<h2 id="toc_3">生产环境的挑战和 Godep 工具</h2>

<p>不同于本地开发可以想到什么依赖就安装一下，少了东西可以再补，生产环境通常需要一套可靠的自动化流程来完成软件极其依赖的构建，比如像 <code>Maven</code> 和 <code>buildout</code> 这样的工具，既可以做到自动构建，又可以做到环境隔离。</p>

<p><code>go get</code> 工具能用但是我们经常会忘记在 go get install 列表里写上我们的依赖，上线的时候才发现：&quot;啊，我这个包手贱忘记写进去了，怎么办！&quot;</p>

<p>程序员的这种需求不可能不被照顾到，所以就有了 <a href="title" title="Godep">Godep</a> 这个工具，<a href="https://github.com/tools/godep">Godep</a> 使用还是很简单的，常用的命令就2个：</p>

<ul>
<li><code>godep save</code>, 扫描你的代码中的<code>import</code>，把所有依赖的包都写入一个 json 文件</li>
<li><code>godep restore</code>, 读取上一步生成的 json 文件，调用 <code>go get</code> 安装每个依赖</li>
</ul>

<p>所以很多行的<code>go get</code>命令变成了一行命令，妈妈再也不用担心我上学出门忘记背书包了。</p>

<p>但是文章写到这里，似乎还有几个问题没解决:</p>

<ul>
<li>三方库可以从 Github 或者 Bitbucket 这样的网站获取，公司内部的二方库该如何解决呢？我们的 Gitlab 仓库就是私有的怎么办？</li>
<li>Google Code 分明在天朝无法通过正常途径访问，Github 也经常抽风，你这工具是要闹咋样</li>
</ul>

<p>对，线上的服务器无法和我们自己开发时的网络环境等同，所以不论是 Golang 自带 <code>go get</code> 还是 <a href="https://github.com/tools/godep">Godep</a> 都有不完美的地方，所以还是本地大法好。</p>

<h2 id="toc_4">Vendor方式管理的依赖</h2>

<p>这种方式就是来解决前面说的 2 个问题的，基本思路就是在项目中新建一个名为 <code>vendor</code> 的目录，把依赖放在该目录下，实践中我们一般会采用 <code>git submodule</code> 来做这件事情：</p>

<pre><code class="language-bash">cd YOUR_PROJECT
mkdir vendor
git submodule add git://repo.git vendor/
</code></pre>

<p>然后在项目发布构建时：</p>

<pre><code class="language-bash">cd YOUR_PROJECT
git submodule update --init
</code></pre>

<p>好处是不需要把依赖包的代码直接放入项目源代码中，并且可以实现自动化的构建。坏处也是有的，你必须修改代码中 <code>import</code> 的地址。</p>

<p>实践中，一般推荐 <a href="https://github.com/tools/godep">Godep</a> 和 <code>vendor</code> 结合的方式来管理项目中的依赖。</p>

<h2 id="toc_5">结论</h2>

<p>上面已经写过了。</p>

<h2 id="toc_6">参考</h2>

<ol>
<li><a href="http://golang.org/doc/code.html#remote">http://golang.org/doc/code.html#remote</a></li>
<li><a href="http://peter.bourgon.org/go-in-production/">http://peter.bourgon.org/go-in-production/</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ioutil.ReadAll 引发的内存泄露]]></title>
    <link href="http://blog.lfyzjck.com/14506675584881.html"/>
    <updated>2015-12-21T11:12:38+08:00</updated>
    <id>http://blog.lfyzjck.com/14506675584881.html</id>
    <content type="html"><![CDATA[
<h1 id="toc_0"></h1>

<p>最近线上的一个计数服务突然出现内存不足的情况，服务器 4G 的内存会在几分钟内被耗尽。计数服务是兼容 <a href="http://redis.io/topics/protocol">Redis Protocol</a> 的，多方查找，最终确定问题出在 <a href="http://redis.io/topics/protocol">Redis Protocol</a> 的解析上面。协议解析的实现部分借鉴了 <a href="https://github.com/docker/go-redis-server">docker/go-redis-server</a> 。</p>

<p>简化的代码如下：</p>

<pre><code class="language-Golang">// parse redis protocol from bufio.Reader
func parse(r *bufio.Reader) ([][]byte, error) {
    line, err := r.ReadString(&#39;\n&#39;)
    var argsCount int
    if line[0] == &#39;*&#39; {
        fmt.Sscanf(line, &quot;*%d\r&quot;, &amp;argsCount)
    }
    args := make([][]byte, argsCount, argsCount)
    for i := 0; i &lt; argsCount; i++ {
        if args[i], err = readArgument(r); err != nil {
            return nil, err
        }
    }
    return args, nil
}

func readArgument(r *bufio.Reader) ([]byte, error) {
    line, err := r.ReadString(&#39;\n&#39;)
    var argSize int
    _, err = fmt.Sscanf(line, &quot;$%d\r&quot;, &amp;argSize)
    if err != nil {
        return nil, err
    }
    data, err := ioutil.ReadAll(io.LimitReader(r, int64(argSize)))
    if err != nil {
        return nil, err
    }
    // check data size
    if len(data) != argSize {
        return nil, fmt.Errorf(&quot;error length of data.&quot;)
    }
    // check \r
    if b, err := r.ReadByte(); err != nil || b != &#39;\r&#39; {
        fmt.Printf(&quot;%s\n&quot;, string(b))
        return nil, fmt.Errorf(&quot;line should end with \\r\\n&quot;)
    }
    // check \n
    if b, err := r.ReadByte(); err != nil || b != &#39;\n&#39; {
        return nil, fmt.Errorf(&quot;line should end with \\r\\n&quot;)
    }
    return data, nil

}
</code></pre>

<p>然后调用 1w 次，看看系统总共分配的内存，这里我们会用到 Golang 的 <a href="http://golang.org/pkg/runtime/pprof/">pprof</a> 工具，详细的用法可以参考这里 <a href="http://blog.golang.org/profiling-go-programs">profiling-go-programs</a> 。</p>

<pre><code class="language-go">// write heap profile to a tmp file.
func writeHeap(filename string) {
    f, err := os.Create(filename)

    if err != nil {
        log.Fatal(err)
    }
    pprof.WriteHeapProfile(f)

    defer f.Close()
}

func main() {
    s := []byte(&quot;*2\r\n$3\r\nGET\r\n$3\r\nKEY\r\n&quot;)
    for i := 0; i &lt; 10000; i++ {
        buffer := bytes.NewReader(s)
        r := bufio.NewReaderSize(buffer, len(s))
        data, err := parse(r)
        if err != nil {
            panic(err)
        }
        fmt.Printf(&quot;%s\n&quot;, data)
    }
    writeHeap(&quot;bytes.mprof&quot;)
}
</code></pre>

<p>假设以上代码保存为 <code>readBytes.go</code>, 通过 <code>go build</code> 编译然后生成二进制文件（ pprof 需要），运行一下，我们会得到 <code>bytes.mprof</code> 文件。在 shell 里执行 pprof 命令：</p>

<pre><code>$ go tool pprof --alloc_space readBytes bytes.mprof
Adjusting heap profiles for 1-in-524288 sampling rate
Welcome to pprof!  For help, type &#39;help&#39;.
(pprof) top
Total: 49.5 MB
    31.5  63.6%  63.6%     31.5  63.6% bytes.makeSlice
    13.0  26.3%  89.9%     44.5  89.9% io/ioutil.readAll
     1.5   3.0%  92.9%      1.5   3.0% fmt.newScanState
     0.5   1.0%  93.9%      0.5   1.0% allocg
     0.5   1.0%  94.9%      0.5   1.0% fmt.(*ss).scanNumber
     0.5   1.0%  96.0%      2.5   5.1% fmt.Sscanf
     0.5   1.0%  97.0%     47.5  96.0% main.readArgument
     0.5   1.0%  98.0%      0.5   1.0% reflect.unsafe_New
     0.5   1.0%  99.0%      0.5   1.0% runtime.convT2E
     0.5   1.0% 100.0%      0.5   1.0% runtime.mallocinit
</code></pre>

<p>一个简单的程序怎么会消耗掉 49.5 MB 内存？我们要解析的源字符串很小，只有 22 Bytes 。处理 1w 次加成自身复制的开销，估算一下内存开销大概是:</p>

<blockquote>
<p><code>22 Bytes * 10000 * 2 = 440000 Bytes = 400 KB</code></p>
</blockquote>

<p>那么这夸张的 49.5 MB从何而来？根据 pprof 的分析，大部分内存开销集中在 <code>bytes.makeSlice</code> 和 <code>io/ioutil.readAll</code> 上面，我们的程序也确实有调用 <code>ioutil.ReadAll()</code></p>

<h2 id="toc_1">ioutil.ReadAll实现</h2>

<pre><code class="language-go">func readAll(r io.Reader, capacity int64) (b []byte, err error) {
    buf := bytes.NewBuffer(make([]byte, 0, capacity))
    defer func() {
        e := recover()
        if e == nil {
            return
        }
        if panicErr, ok := e.(error); ok &amp;&amp; panicErr == bytes.ErrTooLarge {
            err = panicErr
        } else {
            panic(e)
        }
    }()
    _, err = buf.ReadFrom(r)
    return buf.Bytes(), err
}

// bytes.MinRead = 512
func ReadAll(r io.Reader) ([]byte, error) {
    return readAll(r, bytes.MinRead)
}
</code></pre>

<p>可以看到，<code>ioutil.ReadAll</code> 每次都会分配初始化一个大小为 <code>bytes.MinRead</code> 的 buffer ，<code>bytes.MinRead</code> 在 Golang 里是一个常量，值为 <code>512</code> 。就是说每次调用 <code>ioutil.ReadAll</code> 都会分配一块大小为 512 字节的内存，由于我们要解析的 Redis Protocol 包含 2 个参数，所以每次解析会掉用 2 次 <code>ioutil.ReadAll()</code>。总共分配的内存大小为：</p>

<blockquote>
<p><code>512 Bytes * 2 * 10000 = 10240000 Bytes = 10.24 MB</code></p>
</blockquote>

<p>这个数字很接近 pprof 上给的 13 MB 了，那么剩下的就是 <code>bytes.makeSlice</code> 的内存开销了。搜索了一下，发现 <code>ioutil.ReadAll()</code> 里会调用 <code>bytes.Buffer.ReadFrom</code>, 而 <code>bytes.Buffer.ReadFrom</code> 会进行 <code>makeSlice</code> :</p>

<pre><code>// ReadFrom reads data from r until EOF and appends it to the buffer, growing
// the buffer as needed. The return value n is the number of bytes read. Any
// error except io.EOF encountered during the read is also returned. If the
// buffer becomes too large, ReadFrom will panic with ErrTooLarge.
func (b *Buffer) ReadFrom(r io.Reader) (n int64, err error) {
    b.lastRead = opInvalid
    // If buffer is empty, reset to recover space.
    if b.off &gt;= len(b.buf) {
        b.Truncate(0)
    }
    for {
        if free := cap(b.buf) - len(b.buf); free &lt; MinRead {
            // not enough space at end
            newBuf := b.buf
            if b.off+free &lt; MinRead {
                // not enough space using beginning of buffer;
                // double buffer capacity
                newBuf = makeSlice(2*cap(b.buf) + MinRead)
            }
            copy(newBuf, b.buf[b.off:])
            b.buf = newBuf[:len(b.buf)-b.off]
            b.off = 0
        }
        m, e := r.Read(b.buf[len(b.buf):cap(b.buf)])
        b.buf = b.buf[0 : len(b.buf)+m]
        n += int64(m)
        if e == io.EOF {
            break
        }
        if e != nil {
            return n, e
        }
    }
    return n, nil // err is EOF, so return nil explicitly
}
</code></pre>

<p>这个函数主要作用就是从 <code>io.Reader</code> 里读取的数据放入 buffer 中，如果 buffer 空间不够，就按照每次 <code>2x + MinRead</code> 的算法递增，这里 <code>MinRead</code> 的大小也是 512 Bytes ，由于我们每次调用 <code>ioutil.ReadAll</code> 读取的数据远小于 512 Bytes ，照理说不会触发 <code>buffer grow</code> 的算法，但是仔细看下实现发现不是这么回事，<code>buffer grow</code> 的算法大概是是这样子的：</p>

<ol>
<li>计算 buffer 剩余大小 free;</li>
<li>如果 free &lt; MinRead, 分配一个大小为 <code>2 * cap + MinRead</code> 的 newBuf, 把老
buffer 的数据拷贝到 newBuf;</li>
<li>如果 free &gt;= MinRead, 读取数据到 buffer, 遇到错误就返回，否则跳转到第 1 步.</li>
</ol>

<p>因为我们用了 <code>io.LimitReader</code> , 第一趟循环只会读取固定字节的数据，不会触发任何错误。但是第二次循环的时候，由于 buffer 的剩余空间不足，就会触发一次 <code>buffer grow</code> 的算法，再分配一个大小为 1536 Bytes 的 Buffer , 然后再次 Read() 的时候会返回 <code>io.EOF</code> 的错误。这就是为什么会有那么多次 <code>makeSlice</code> 调用的原因，继续估算一下 <code>makeSlice</code> 的消耗：</p>

<blockquote>
<p><code>1536 Bytes * 2 * 10000 = 30.72 MB</code></p>
</blockquote>

<p>把 2 次消耗的内存加起来, 2w 次 <code>ioutil.ReadAll()</code> 的时间内存消耗为 <code>10.24 MB +<br/>
30.72 MB = 40.96 MB</code>。( 倒吸一口凉气</p>

<p>实际线上发生的情况，并没有这么多的请求，但是每次请求却比较大。就是说每个请求会包含上万个参数，等价的调用了上万次的 <code>ioutil.ReadAll</code> , 消耗了大量的内存。但是仅仅这样，只是程序执行时消耗了比较多的内存，并未有内存泄露的情况，那服务器又是如何内存不足的呢？这就不得不扯到 Golang 的 GC 机制。</p>

<h2 id="toc_2">Golang 的 GC</h2>

<p>Golang 的 GC 采用经典的 <a href="http://www.brpreiss.com/books/opus5/html/page424.html">Mark-and-Sweep</a>, <a href="http://www.brpreiss.com/books/opus5/html/page424.html">Mark-and-Sweep</a> 分为 2 个阶段：</p>

<ul>
<li>第一阶段：从几个跟变量开始，扫描所有的 objects , 然后标记 <code>isMarked = true</code></li>
<li>第二阶段：扫描 heap ，对所有为标记的变量进行回收</li>
</ul>

<p>这个 GC 的过程会终止程序的运行直到回收完毕，不过从 Golang 1.3 开始，在第一阶段结束后，程序就会恢复运行，然后单独起一个 goroutine 来进行 Sweep 的操作。忽略一些细节，Golang 的 GC 大致遵循以下几个原则：</p>

<ul>
<li>已分配的 Heap 到达某个阈值，会触发 GC, 该阈值由上一次 GC 时的 HeapAlloc 和 GCPercent 共同决定</li>
<li>每 2 分钟会触发一次强制的 GC，将未 mark 的对象释放，但并不还给 OS</li>
<li>每 5 分钟会扫描一个 Heap, 对于一直没有被访问的 Heap，归还给 OS</li>
</ul>

<p>然后回头看下上面的场景，一个大请求会多次调用 <code>ioutil.ReadAll</code> 分配很多内存空间，并发的时候，会在很短时间内占用大量的系统内存，然后将 GC 阈值增加到一个很高的值，这个时候要 GC 就只有等 2 分钟一次的强制 GC。2 分钟内，无法阻止内存继续上涨，Golang 的 Heap 会吃光 OS 的内存，并且一直霸占着不归还给 OS， 造成 OS 整体性能下滑，程序的正常运行会变得很慢（包括负责 GC 的 goroutine），于是整台服务器就像多米诺骨牌一样倒下了。</p>

<h2 id="toc_3">怎么解决？</h2>

<p>问题的根本原因还是，系统在合理的负载内，浪费了大量的内存空间，加上 GC 的延时，最终导致系统的崩溃。所以还是要解决内存浪费的问题，最终还是要避免不正确的使用 <code>ioutil.ReadAll</code>，稍微修改下上面的代码，我们用比较底层的 <code>bytes.Buffer.Read</code> 来读取固定的字节：</p>

<pre><code class="language-go">func readArgument(r *bufio.Reader) ([]byte, error) {
    line, err := r.ReadString(&#39;\n&#39;)
    var argSize int
    _, err = fmt.Sscanf(line, &quot;$%d\r&quot;, &amp;argSize)
    if err != nil {
        return nil, err
    }
    // look at here !!!!!!!!!!
    data := make([]byte, argSize)
    _, err = r.Read(data)
    if err != nil {
        return nil, err
    }
    // check data size
    if len(data) != argSize {
        return nil, fmt.Errorf(&quot;error length of data.&quot;)
    }
    // check \r
    if b, err := r.ReadByte(); err != nil || b != &#39;\r&#39; {
        fmt.Printf(&quot;%s\n&quot;, string(b))
        return nil, fmt.Errorf(&quot;line should end with \\r\\n&quot;)
    }
    // check \n
    if b, err := r.ReadByte(); err != nil || b != &#39;\n&#39; {
        return nil, fmt.Errorf(&quot;line should end with \\r\\n&quot;)
    }
    return data, nil

}
</code></pre>

<p>编译后再来看看 Heap 的分配情况，仍然使用 pporf 工具:</p>

<pre><code>$ go tool pprof --alloc_space readBytes bytes.mprof
Adjusting heap profiles for 1-in-524288 sampling rate
Welcome to pprof!  For help, type &#39;help&#39;.
(pprof) top
Total: 6.5 MB
     1.5  23.1%  23.1%      1.5  23.1% fmt.newScanState
     1.0  15.4%  38.5%      1.0  15.4% reflect.unsafe_New
     0.5   7.7%  46.2%      0.5   7.7% allocg
     0.5   7.7%  53.8%      0.5   7.7% bufio.(*Reader).ReadString
     0.5   7.7%  61.5%      0.5   7.7% bufio.NewReaderSize
     0.5   7.7%  69.2%      5.5  84.6% main.main
     0.5   7.7%  76.9%      3.0  46.2% main.parse
     0.5   7.7%  84.6%      1.5  23.1% main.readArgument
     0.5   7.7%  92.3%      0.5   7.7% runtime.convT2E
     0.5   7.7% 100.0%      0.5   7.7% runtime.mallocinit
</code></pre>

<p>如何？Golang 分配的内存显著减少了，问题至此就解决了，但是我还是想吐槽这种神坑.....</p>

<p>给 <a href="https://github.com/docker/go-redis-server">docker/go-redis-server</a> 提了 Issue，至今没人回复, 呵呵，该睡觉了。</p>

]]></content>
  </entry>
  
</feed>
